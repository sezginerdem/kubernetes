# Kubernetes Yonetim Komponentleri

## CONTROL PLANE-MASTER NODE

* ### Kube-apiserver: Resepsiyon ornegi
kubernetes ile alaki um komponentlerin ve dis dunyadan kubernetes platformu ile iletisim kuran tum servislerin ortak giris noktasidir. Api server tum komponent ve node bilesenlerinin direk iletisim kurabildigi tek komponenttir. Tum iletisim api server uzerinden gerceklestirilir. Kube-apiserver k8s de kaynak olusturma islemlerinde api dogrulamasindan sorumludur. Kullanicilar kube ctl komut satiri istemcisi veya rest-api cagrilari araciligiyla api server ile iletisim kurabilirler. Kisacasi disaridan istek gerceklestirmek icin apiserver a ulasir authantication ve authorization islemlerini gercektlestirir ve kubernetese ulasirsiniz. Diger tum komponenetlerde isteklerini api server uzerinden gerceklestirir.

* ### Etcd: 
tum cluster verisi metadata bilgileri ve k8s de olusturulan tum objelerin bilgilerinin tutuldugu anahtar-deger “key-value” veri deposudur. Clusterin mevcut durumu ile ilgili bilgiyi uzerinden barindirir. Kisacasi k8s in calismasi icin ihtiyac duyulan tum bilgiler etcd uzerinde tutulur. Apiserver haric baska hicbir komponent etcd ile dogrudan haberlesemezler. Etcd ile iletisim kurmak istediklerinde bunu api-vserver uzerinden gerceklestirirler. 

* ### Kube-scheduler: ornekte uretim planlama. 
Yaratilmak istenen podlarin gereksinimlerini kontrol ederek o podun en uygun calisacagi worker node un hangisi oldugunu belirler ve o podun burada olusturulmasini saglar. Podun ozel gereksinimerini isteklerini, cpu gibi cesitli parametreleri elinde bulundurarak degerlendirir ve pod icin en uygun node un hangisi olduguna karar verir. 

* ### Kube-controller-manager: ornekte vardiya amiri
Tek bir binary olarak bulunsa da icinden birden fazla kontroller bulundurur. K8s istenilen durumla mevcut durum arasindaki karsilastirmada uygulama yonetimi saglar. Kube-controller altindaki controller managerlar k8s den istenilen durumla mevcut durum arasinda fark olup olmadigi gozlerler. Kube-api araciligiyla etcd de saklanan cluster durumunu inceler ve eger mevcut durum ile istenilen durum arasinda fark varsa iste bu farki olusturan kaynaklari gerektigi gibi olustururak, guncelleyerek veya silerek bu durumu esitler. Ornegin siz k8s e uygulamanizin 3 pod olarak calismanizi bildirdiniz. K8s de bunu gerceklestirdi ve uygulamanizin kostugu 3 pod calismaya basladi ama sonra bir tanesi silindi iste controller-manager bu podun yeniden olusturulmasini saglar. 

Control-plane yani yonetim kismi master node uzerinde calisir. Bu componentlerin hepsi tek bir linux isletim sistemine kurulabilecegi gibi birden fazla sisteme de kurulabilir. etcd tamamen bu yapidan ayri bir sekilde konuslandirilarak daha yuksek bir erisim saglanabilir iken ama genellikle etcd de api-server in kostugu yerde durur. ornegin 3 sunucu uzerinde 3 api-server kurulup diger componentler bu sistemlere dagitilabilir. Bu uzerinde control plane componentlerinin kostugu sistemlere cluster altinda master node deriz. Master nodelar sadece bu sistemleri barindirmak icin bulunur ve production ortaminda herhangi bir is yukumuzu bu sistemler uzerinde calistirmayiz. Bunlarda sadece cluster in yonetim altyapisini kostururuz. is yuklerimiz ise worker nodelar uzerinde calistirilir. Bunlar uzerinde bir container run time barindiran ve clustera dahil edilmis sistemledir. 

## WORKER NODE
***
her worker node da 3 temel component bulunur. ilk ve en onemli component containerlarin calismasini saglayacak bir container run time dir. Default olarak bu docker dir ancak k8s docker container run time destegini birakarak bazen containerd ye gecmistir.

* ### kubelet: ornekte ustabasi
her worker node da bulunur. Kubelet api-server araciligiyla etcd yi kontrole eder. Scheduler tarafindan bulundugu node uzerinde calismasi gereken bir pod belirtildi ise kubelet bu podu o sistemde yaratir. Conatinerd ye haber gonderir ve belirlenen ozelliklerde bir container in o sistemde calismasini saglar.

* ### kube-proxy: ornekte lojistik elemani
Olusturulan podlarin tcp, udp ve etcdp trafik akislarini yonetir. ag kurallarini yonetir. kisaca network proxy gorevi gorur.

Bunlarin disinda dns ya da gui hizmeti saglayan cesitli servisler entegre edilebilen servislerdir. fakat bunlar core componentler olarak adlandirilmaz

# Kubectl Kurulumu
***
Kubernetes e komut vermek icin bubu api-server uzerinden yapiyoruz. 
Bunun 3 yontemi var. 

* #### Rest cagrilari
Uygulamalarin ya da scriptlerin icinde kullanilir
* #### Gui istemcileri
Grafiksel arayuz ile iletisim kurmak. Resmi gui araci dashboard, oktant, land en bilinen araclardir. Gui istemciler en temel araclar degildir. 
* #### kubectl
Shell uzerinden apiserver a komutlar doderdigimiz k8s in resmi cli aracidir. k8s cluster olusturulmadan once ilk olarak kubectl yuklenmesi gerekir.

## kubectl config dosyasi
* kubectl araci baglanacagi kubernetes cluster bilgilerine config dosyalari araciligiyla erisir
* config dosyasinin icerisinde kubernetes cluster baglanti bilgilerini ve oraya baglanirken kullanmak istedigimiz kullanicilari belirtiriz
* Daha sonra bu baglanti bilgileri ve kullanicilari ve ek olarak namespace bilgileirni de olusturarak contextler yaratiriz
* kubectl varsayilan olarak $HOME/.kube/ altindaki config isimli dosyaya bakar
* kubectl varsayilan olarak $HOME/.kube/ altindaki config dosyasina bakar ama bunu KUBECONFIG environment variable degerini degistirerek guncelleyerebilirsiniz

# pod
***
* kubernetes icinde yaratilan en temel obje pod dur.
* kubernetesde olusturabileceginiz en kucuk birimlerdir
* podlar bir yada daha fazla container barindirabilirler ancak cogu duurmda pod tek bir container barindirir
* her bir pod un essiz bir idsi "uid" bulunur
* her pod essiz bir ip adresine sahiptir.
* ayni pod icersindeki containerlar ayni node ustunde calistirilir ve bu containerlar birbirleriyle localhost ustunden haberlesebilirler.
* kubectl ile kubea-api ile haberleserek k8s uzerinde pod yaratma islemi gerceklestirilir. api server bu poda bizim tanimladigimiz bilgileri atar ve bir pod yaratir ve etcd veri tabanina kaydedir. kube-scheduler componenti surekli burayi gozler ve herhangi bir worker node atamasi yapilmamis pod tanimi yapilmamissa o podun calismasi icin uygun bir worker node secer ve bu bilgiyi pod tanimina ekler. Sonrasinda worker node uzerinde calisan kubelet servisi de bu etcd yi surekli gozledigi icin bu pod tanimini gorur ve bu tanimda belirtilen container o worker node uzerinde olusturulur ve boylece pod olusturulma asamalari tamamlanir.

## Pod Yasam Dongusu
#### pending
pod yaratmak icin gonderilen yaml dosyasi kube-api tarafindan alinir ve varsayilan ayarlari da ekleyerek poda bir unique id atar ve tum bunlari etcd ye kaydeder. Bu asamadan sonra pod olusturulmustur ve bu asamadan sonra pending asamasina gecer. Eger bir podun statusu pending durumunda ise bunun anlami sudur: birisi bir pod olusturdu podla ilgili tanimlar yapildi ve veri tabanina kaydedildi ama pod herhangi bir node uzerinde olusturulmadi. 

#### creating
kube-scheduler api-server araciligiyla surekli etcd yi gozler. eger burada yeni yaratilmis ve herhangi bir node atamasi yapilmamis bir pod gorurse calismaya baslar. Algoritmasi ve secme kriterlerine gore podun calismasinin en uygun olacagi node u secer ve veri tabanindaki pod objesine node bilgisini ekler. Bu noktadan itibaren pod yasam dongusunde creating asamasina gecer. eger bir pod creating asamasina gecemiyorsa bu kube-scheduler in uygun bir node bulamadigi anlamina gelir. bunun pek cok nedeni olabilir: cluster da bu podun gereksinimlerini karsilayabilecek node bulunmuyor olabilir. Nodelar uzerinde cpu ve memory gibi kaynaklar tukenmis olabilir. 

#### ImagePullBackOff
Nodelar uzerinde kubelet adli bir servis calisir. bu servis ayni kube-scheduler gibi surekli etcd veri tabanini gozler ve bulundugu node a atanmis nodelari tespit eder ve hemen islemlere baslar. Ilk olarak pod taniminda olusturulacak containerlara bakar ve bu containerlarda olusturulacak image lari sisteme indirmeye baslar. Eger bir sekilde image indirilemezse pod erimagepull ve ardindan da imagepullbackoff asamasina gecer. Eger podun statusunda imagepullbackoff gorurseniz bu nodeun image i repository den cekemedigi ve bunu tekrar tekrar devam ettigi anlamina gelir. Bunun birkac nedeni olabilir: en sik karsilasilan neden pod tanimlanmasinda image isminin yanlis yazilmasidir. Image ismi yanlis yazildigi icin image cekilemeyecektir ya da image i cekmek icin sisteme kaydolmak gerektigi ve bu authentication islemlerinde hata yapilmasidir. Bu nedenle image cekilemeyecektir. Bu gibi bir hata olmamasi durumda ise

#### Runnig
Islemlerin sorunsuz bir sekilde ilerlemesi halinde kubelet node da bulunan container engine ile haberlesir ve ilgili containerlarin olusturulmasini saglar ve containerler running durumuna gecer. Bu noktadan itibaren artik pod olusturulmus olur.

* Containerlarla ilgili temel kural: container icindeki uygulama calistigi surece container calisir. Uygulama calismayi birakirsa container durdurulur. Uygulamanin da durdurulmasi 3 sekilde olur: 1) isi bittigi icin hata vermeden otomatik olarak kapanir. 2) Kullanicinin istegi uzerine komut gonderilir ve hata vermeden kapanir. 3) hata verir, sikinti cikar, hata kodu olusturarak kapanir. Ornegin nginx image uzerinden bir container yarattigimizi dusunelim. Bu uygulama bir deamon bir servis yani siz onu durdurana ya da hata cikip cokene kadar calismaya devam eder. O calistigi surece de container alismaya devam eder. Fakat diyelim ki siz bir image yarattiniz ve bu image in varsayiln uygulamasi da bir script ya da basit bir komut. container baslayinca bu script calisiyor isini yapiyor ve isi bitince kapaniyor yani container da kapaniyor yani illa ki hata vermeden kapanmasina gerek yok.

Bu gibi sorunlar nedeni ile containerlar icin restart policy tanimi yapilir. Restart policy 3 adet deger alabilir:
* Always: Default degerdir. Pod un icerindeki container her durumda durdurulursa durdurulsun her sartta yeniden baslat anlamina gelir. 
* On-failure: Sadece hata alip kapanirsa yeniden baslatilir.
* Never: Pod hicbir zaman yeniden baslatilmaz.

#### Succeed
Podun altindaki containerlar calismaya devam ettikce status running olarak devam eder. Eger containerlarin hepsi hata vermeden dogal olarak kapanirsa ve restart policy never veya onfailure olarak set edildi ise podun statusu succeed olarak statusune gecer ve pod yasam dongusunu tamamlar. Yani bir diger degisle completed, basarili olarak pod un yasam dongusu tamamlanir. 

#### Failed
Never ya da Onfailure olarak policy tanimlandi ve containerlardan biri hata verip kapandi ise bu sefer pod un statusu failed olarak isaretlenir ve yasam dongusunu boyle tamamlar. 

#### CrashLoopBackOff
fakat restart policy always olarak set edildi ise pod hicbir zaman succeed ya da failed durumuna gecmez. Bunun yerien podun icerisindeki container yeniden baslatilir ve running state de devam eder. Fakat kubernetes bu yeniden baslatma islemini belirli bir siklikla yapiyorsa bazi seylerin ters gittigine kanaat getirilir ve pod u CrashLoopBackOff adini verdigimiz bir state e sokar. Bunun anlami sen bir pod olusturdun ama bu pod un icerisindeki container ikide bir kapaniyor ama gene kapaniyor buna bir bak. Bu pod da sunlar olur: k8s container in icerisinde birseylerin ters gittigini anlar ve pod un statusunu crashloopbackoff a cevirir. Surekli restart etme yerine restart eder 10 sn bekler eger 10 sn icinde yeniden cokerse 20 sn bekler yeniden cokerse 40 sn bekler sonra 80 sn bekler ve bu durum 5dk lik araliga cikana kadar boyle devam eder ve ondan sonra her 5dk da bir tekrar eder. Bu arada container cokmeyi birakir ve 10dk sure ile sorunsuz bir sekilde calismaya devam ederse. Kubelet container i crashloopbackoff dan cikarir ve running e dondurur. eger bu olmaz ve siz mudahale etmezseniz bu durum sonsuza kadar boyle devam eder. Ozetle crashloopbackoff mudahaleyi gerektirir ve bakilmasi gerekmektedir. 

## Multicontainer pod
Bir frontedn ve bir de backend den olusan two tier yani cift katmanli bir uygulama yazdik. Cok populer olan orgenin wordpress uygulamasi. WordPress uygulamasini deploy etmek istedigimizi varsayalim. Bildigimiz uzere wordpress php tabanli bir frontend ve bu uygulamanin verilerinin tutuldugu bir mysql veri tabanina sahip. Bu uygualamyi container haline getirmek istiyorum. Bu uygulamayi icinde mysql ve word press halinde calisan tek bir container haline getirebilir miyim? Teknik olarak evet. Fakat bunu yapmiyoruz. Her iki uygulama icin de ayri iki container kullaniyoruz. BBunun iki nedeni var 1) container in teme mantigi izolasyon bu iki uygulamayi da ayni container icine koymak bu izolasyondan mahrum olmak demek. 2) Iki uygulamayi scale etmek istemeniz durumunda yatay buyume yapamiyorsunuz. Bunu soyle dusunun ben 2 sunucudan olusan bir ortam kurdum. Hem wordpress hem de mysql uygulmasini ayni container icine kurdum ve tek bir uygulama olarak calistirmaya basladim. Sayfama yogun bir istek oldugunda wordpress in frondend katmani bu isteklere cevaop verememeye basladi ben de yeniden bir container daha olusturarak load balancer arkasina almak ve kaynaklarimi cogaltmak istedim ki bu problemi cozebileyim. Ama bunu yaptigim zaamn ortamda 2 frontend 2 tane de mysql veri tabani olacak. Ancak benim veri tabanimda veri sikintisi yoktu. Ikisi de ayni container da oldugu icin bunu coklamam gerektiginde ikisini birden cokladim. Hatta 2. containeri deploy ettikten sonra mysql baglanti ayarlarini degistirdim ve 1. container icindeki mysql e baglanmasi icin ayar yaptm cunku simdiye kadar olusturdum her sey o veri tabaninda. Bunun 100 container a kadar scale edilen bir ortam oldugunu dusunun. Sirf 2 uygulamayi da ayni container icine gomdugum icin sikintilar yasadim. Bunun yerine bir mysql ve bir de wordpress olmak uzere 2 ayri imahge yaratsa idim bu sefer 1 tane mysql container yaratilirdi benim de istedigim kadar wordpress scale edebilme imkanim olusurdu. Bu nedenlerden dolayi en onemli best practise bir container icine 1 tane uygulama koymak. 
Kubernets icinde ise word press icin bir pod mysql icin bir pod olusturulmali. Fakat biz istersek bir pod icinde birden fazla container da koyabilirim ama bu da bir container icine iki uygulama koymak ile ayni sey cunku k8s de scale ettigimiz sey pod. Her container da tek bir uyguama her pod da bir container. Peki neden kubernetes ayni pod icinde birden fazla container calismasina izin veriyor? Diyelim ki ben wordpress php uygulamasinin uygulama ici performans degerlerinin merkezi bir yerde toplayarak analiz etmek istiyorum. Bunu yapabilecek bir uygulamyi deploy etmek istiyoruz. Bunu k8s de nasil yapabiliriz? Oncelikle mysql podumuzu ayaga kaldirdik sonrasinda wordpress uygulamamiz agaya kalkti son olarak da yeni uygulamanin ayaga kalkacagi podu yarattim. Son uygulamanin tek bir amaci var wordpress e baglanacak ve uygulamayi analiz edecek veriyi toplayacak yani bu son uygulama wordpress e bagimli. Bu uygulama wordpress e bagimli. Bu uygulama wordpress hangi worker node da olusturuluyorsa orda olustrulmali. Wordpress calismaya basladigi zaman calismali kapandigi zaman kapanmali yani tamamen ona bagimli. yani ben 2. bir wordpres uygulamayi yaratirken 2. bir uygulama daha deploy etmem gerekecek.3. bir wordpress uygulamasi icin 3. log kodunu olusturuacgim. Wordpress i silerken bu uygulamadan bilgi toplatan podu da silmmem gerekecek. Bu cok zaman alan bir islem surekli iki is yapmam gerekiyor. Diger bir sikinti ise diyelim ki benim metric toplayan uygulamam word press uygulamasi ile stirage seviyesinde haberlesmesi ortak dosyalari yazmasi gerekiyor ancak 2 podun ayni local volume e baglanabilmesi icin 2 pod un ayni worker node uzerinde calismasi gerekiyor. diyelim ki ben wordpress podunun olusturdugumda kubescheduler bunu o an en uygulan olan node1 da olusturdu. sonrasinda analiz verisi toplayan uygulama olusturmak istedim ve kubescheduler bunu node2 uzerinde olsuturdu. Bu uygulamalar stroge seviyesinde birbirlerine ulasamayacaklar. Bu da ayri bir sorun. K8s bu sorunlari ortadan kaldirmak icin ayni pod icerisinde 2 container calistirma imkani sagliyor. Birlikte scale edilmesi gereken, birbirleriyle network ve storage sebiyesinde haberlesmesi gereken uygulamalari ayni pod icerisinde ayri ayri containerlar olarak calistirabiliyoruz. Buna terminolojide sightcar container denmektedir. 

## init container
init container da bir pod icerisinde birde fazla pod yaratilmasina imkan verir. Fakat app containerlardan farkli olarak init containerlar pod un yasam dongusu boyunca calismaz. Siz bir pod tanimina init container tanimi koydugunuz zaman pod olusturuldugu zaman bu init container calistirilir. Init container calisir icindeki uygulama ne yapacaksa onu yapar ve ardindan kapanir bu init containerlar islerini tamamlayip kapanana kadar da app containerlar calismaya baslamaz. Peki neden boyle bir seye ihtiyac duyariz ve kullanim alani nedir? Init containerlar esas uygulamamiz calismadan once tamamlamamiz gereken seyler var ve bunlari tamamlamadan esas uygulamayi baslatmak mantikli degilse kullanmak gerekir. Mesela uygulamamizin bagimli oldugu baska bir uygulama ya da servis var. Eger bu ayakta ve hazir degilken uygulamayi baslatirsak uygulamada sikinti cikiyor. Bu durumda pod tanimina init container tanimi ekler ve bu init containerin bu servisi gozlemesi icin ayar yapariz. Init container icinde bir uygulama calistirilir ve bu servisten okey alana kadar calisacak okey aldiktan sonra da kapanacak sekilde ayar yapariz. Bu sayede pod olusturuldugu zaman ilk olarak init container calisir. Diger servisi beklemeye baslar. Diger servis hazir oldugu anda uygulama kapanir ve init container da kapatilir. Init container kapatildigi anda da esas uygulamamizin calistigi container ayaga kalkar ve boylece servisimiz hazir olana kadar uygulama beklemis olur. Soyle bir senaryo dusunun ana uygulamamizin ihtiyaci olan bazi config dosyalarinin guncel halinin sisteme cekilmesi gerekiyor iste bu cekme islemini de init container ile halleder ve ana uygulama baslamadan bunlari sisteme indirebiliriz. 

## label and selector
label yani tagler k8s de her turlu objeye atayabildiginiz anahtar deger eslenikleridir. Etiketler sayesinde olusturdugumuz objelere bizlerin anlayacagi ve gruplara yaparken kullanabilecegimiz bilgiler eklemis oluruz. Bu sayede k8s tarafindan bizlere core ozellik olarak sunulmayan objelere belirli bir aidiyet atanmasi islemini gerceklestirebiliriz. ornegin k8s de 5 ekip tarafindan olusturulan 100 den fazla pod calistiriyoruz. Hangi pod un hangi ekibe ait oldugunuz belirtmek istersek k8s in bize belirtmis oldugu bir hiyerarsi mekanizmasi yok. ama labels sayesinda bunu saglayabiliyoruz. ornegin her ekip kendi olusturdugu objeye team:teamname seklinde etiket atarsa bizler bu etiketler sayesinde hangi ekibin hangi objelere sahip oldugunu listeleme imkanina kavusuruz. Ya da uygulamamizin frontend katmanini olusturan objelere tier:front-end backend katmanini olusturan objelere ise tier:backend etiketini atayarak frontend ve backend katmanlarini ayri ayri listeleme imkanina kavusuruz. 
Etiketler obje olusturulurken atanacagi gibi olustuktan sonra da atanip silinebilir. Etiketler anahtar veri degerleri seklinde olusturulur. Etiketler bir dns subdomanin seklinde atanan opsiyonel bir on ek kismi bulundurabilir example.com/tier:frontened gibi. ama bu kisim zorunlu degildir. Bu kisimdan sonra ana kisim yazilir bu iki kisim anahtar ve deger kismindan olusur. Bu anahtar ve deger kismi en fazla 63 karekter olabilir. Alfanumerik bir karakterle baslamali ve bitmelidir. Tire alt cizgi noktolar ve arasinda numerik degerler icerebilir ancak bunlarla baslayamazlar.
Etiketlerin bir onemli gorevi de k8s de objeler arasindaki baglanti da labelslar araciligiyla kurulur. Servisler ve deployment objeleri hangi podlar ile iliski kuracagini labellar sayesinde belirlerler. ornegin bir servis yaratir ve ona git tier:frontend labelli podu bul ve servisi bu poda yonlendir.
yaml dosyalarinda selector araciligiyla deploymenta senin yonetecgein podlar app:frontend label ina sahip olacaklar. Bunu gorursen anla ki bu podlar senin podlarin. yaml dosyasi icindeki template alindaki labels kismian da bu app:frontend i ekliyorum ki deploymen hangi pod u sececegini anlayabilsin. 
Labels ve selector kismi 3 acidan onemli
* Ilk olarak bu bir zorunlu alan. Yani deployment da en az bir selector tanimi olmali ve ayni labellar template kisminda da bulunmali.
* egerki birden fazla deployment objesi olusturacaksaniz ki production ortaminda bu olacak siz her deployment objesinde farkli label ve selector kullanilmali. ayni labellar kullanilmasi sikinti cikarir.
* ayni labellari kullandiginiz baska objeleri yaratirsaniz misal sibgleton bir pod yaratirsaniz bu da sikinti cikarir. 


## Annotation
Aynen labellar gibi anahtar veri opsiyonu ekleyebilecegimiz 2. secenek ise annotations. Labellar k8s icin cok onemli oldugu icin label eklemek veya cikarmak clusterda birseyleri tetikleyebilir. dolayisiyla her bilgiyi label olarak metadataya ekleyemeyebiliriz. Ancak bu bilgiler gerekli olabilir. Mesela podun ne zaman ve kim tarafindan olusturuldugu bilgisini eklemek istiyorum. Bu bilginin label olarak eklenmesi dogru degil, yani bu bilgiyi label olarak secme istegim yok veya bu bilgiyi herhangi bir secme istegi olarak kullanmak da istemiyorum. Dolayisiyla bunu annotationa eklemek daha dogru olacaktir. Ayrica annotation k8s in ana componenti olmayan fakat k8s ile baglantisi olan yazilimlar tarafindan ihtiyac duyulan bilgilerin de yazildigi yerdir. Ornegin firmamizin destek ekipleri tarafindan kullanilan bir cagri kabul uygulamasi var. Bu yaziimi k8s den bilgi cekebilecek hale getirdik ve soyle bir sistem kurmak istiyoruz: herhangi bir k8s objesinde bir sikinti ciktigi zaman bu cagri yazilimi bunu tespit etsin ve o objenin destek sorumlusu olarak belirlenen insana mail gondersin. Bu durumda o mail adresi bilgisini objenin metadatasina annotation olarak ekleyebiliriz ve destek yazilimi bu bilgiyi cekebilir. bu sayede de bu insana mail gonderebilir. Annotation bu ve benzeri durumlar icin kullanilir. Olusturulma kurallari labellarla hemen hemen aynidir. DNS subdomain olarak eklenebilecek ve zorunlu olmayan bir prefix ile baslayabilen, sonrasindala labellar gibi 63 karakteri gecmeyecek, nokta tire ve alt tire icerebilen ama bunlarla baslayamayan alfanumerik degerler icerebilir. value kisminda ise bu kurallar gecerli degildir. Alfanumerik olmayan karakter de alabilir. 

## Namespace
Soyle bir senaryo kurun bi formada calsiyoruz ve tum calisanlarin ortak calistiklari dosyalari barindirabilecekleri bir yapi tasarliyoruz. 10 ayri ekibimiz var ve bu 10 ayi ekibin de erisip dosya barindirabilecegi bir latyapi kurmak istiyoruz. Bu sistemi nasil kurariz: bir file server yaratirim ve bunun altinda tum ekibin erisebilecegi bir paylasim alani yaratirim. Sonra gider tek tek tum kullanicilarin bilgisayarlarinin buraya erismesini saglayabilirim. Baslangicta sorunsuz olabilir ancak birden fazla ekibin olmasi durumunda soru olabilir. Bu dosya altinda 100lerce dosyanin olmasi durumunda bu belli bir zaman sonra yonetilemez hal alabilir. Bir diger sikinti da su olabilir diyelim ki ben a.txt adinda bir dosya olusturdum. baska bir arkadasim da a.txt adinda bir dosya olusturup ayni yere atamaz. Bir baska sikinti da su olabilir. diyelim ki sadece hr calisanlarinin gormesi gereken dosyalari da burada tutuyorum. bu dosyalarin sadece hr tarafindan gorulmesini saglamak icin dosya bazinda ayarlamalar yapmam gerekir. her dosya upload edildigide de bu islemleri tekrarlamam gerekir. Son olarak da su sikintiyi yasariz. Herkes tek bir alana dosya kaydettigi icin ekip bazli kota ayarlamasi yapmam zor olur. yani hr kismina 20gb alan ayirayim it kismina 100gb alan ayirayim gibi ayarlamalar yapamam. Cozum belirli bir zaman sonra yeterli olmamaya basladi.
Her ekip icin klasor altinda ayri bir alan yaratirsak tum bu problemleri ortadan kaldiririz. Her ekibe ozel bir klasor olusturulur. Her ekip kendi dosyalarini kendi klasorlerindde tutabilirler. Ayrica it ekibi gibi ekiplere ozel proje bazli klasorler de yaratiriz. Ornegin yeni bir urun gelistirmesi yaratiriz ve bu urune ait gelistirme dosyalarini bu klasorde tutariz bu sayede tum guvenlik ayarlarini klasor bazda halledebiliriz. hr klasorune sadece hr calisanlari erissin it kalsorune sadece it calisanlari erissin, proje klasorunde proje yoneticisinin yazma izni olsun ama stayjer sadece dosyalari okuyabilsin diyebiliriz. Boylece guvenlik ve gruplama ile pek cok sorunun ustesinden geldik. Isim cakismasini engelledik. son olarak da klasor bazinda kota ayarlamasi yaparak kaynak kisitlamasi saglabiliyoruz. Namespace tam anlami ile bu ise yarar. 
Kubernetes cluster i bu ornekteki fileserver olarak dusunursek namespace ler de burada yarattigimiz klasorlerdir. k8s de objeleri sanal klasorler altinda olusturabilir ve bu sanal klasorler altinda olusturabilir ve sonrasinda bu sanal klasor bazinda erisim izni ve kota ayarlamasi yapabiliriz.
Her k8s clusterinda varsayilan olarak 4 namespace olusturulur.
* kube-sysem: k8s tarafindan olusturulan objelerin tutuldugu namespace dir. 
* kube-public: kimligi dogulanmamis olanlar da dahil tum kullanicilar tarafindan erisilmesine ihtiyac duyulan objelerin olusturulabilecegi yerdir. 
* kube-node-lease: node hard disk islemleri icin olusturulan ozel bir namespacedir.
Kisacasi kube ile baslayan tum namespaceler kubernetes tarafindan olusturulur ve clusterin isleyisi ile alakali objelerin tutuldugu yerlerdir. Bunlarin disinda default namespace adinda baska bir namespace daha olusturulur ve adindan da anlasilacagi uzere bizler aksini belirtmedigimiz surece objeler burada olusturulur.
Bizler herhangi bir namespace tanimi yapmadan objelerimizi bir tek namespace altinda toplamayiz. Tek bir ekip tarafindan yonetilen kucuk bir kubernetes clusterimiz var ise boyle de devam ederiz. Fakat ne zaman buyur ve birden fazla ekip tarafindan yonetilen ve birden fazla ekip tarafindan deploy edildigi bir k8s cluster a evrilirsek iste o zaman namespaceler bize kota ayarlamasi ve namespace bazinda kullanici erisimi verebilme gibi ozellikleri sayesinde yardimci olacaktir. 

## deployment
En temel obje poddur. ama biz genelde tekil yonetilmeyen podlar yaratmayiz. podlari yoneten ust seviye objeler yaratiriz ve podlar bu objeler tarafindan yaratilir ve yonetilir. 
neden tekil podlar yaratmiyoruz?
Deployment buan nasil cozum bulur?
Bir uygulamamizi production ortaminda deploy ettigimizi varsayalim. Uygulamamizi container image i haine getirdik ve bunu kubernetes de deploy etmek icin bir pod tanimi yarattik kubectl apply ile bunu api sever a gonderdik. 3 worker node lu bir yapimiz var ve kube-scheduler uygun bir worker node secerek podu o node uzerinde olusturdu. fakat bir sure sonra bu podun olusturuldugu worker node bozuldu ve erisilemiyor. dolayisiyla pod a da erisemiyoruz. bu durumda pod terminating durumuna gecer ve oylece kalir ve tabii ki de pod calismadigi icin uygulamamiz da calismamaya baslar. Siz bir pod yarattiginiz zaman kube-scheduler bu poda uygun bir node bulur ve onun uzerinde bu pod u calistirir. eger container seviyesinde bir sikinti cikarsa ve restart policy onfailure ya da always olarak secilirse container restart edilir ve sorun cosulur. ama pod un schedule edildigi node da bir sikinti cikar ya da kaynak sikintisi nedeni ile o pod durdurulursa kube-scheduler devree girip aa su pod calismiyormus deyip baska bir node uzerinde onu schedule edeyim demez. Bu nedenle worker node umuz gittigi icin de pod umuz terminate state e duser ve oylece kalir. Buna cozum olarak da 3 ayri podun 3 ayri node uzerinde calismasini node selector tanimlari ile sagladik. Kisacasi her bir worker node da bir tane olmak uzere 3 tane pod yarattik, onlerine de bir load balancer koyduk. Boylece bir tanesine bir sey olursa diger 2 tanesine erisilebilmesini sagladik. Ancak diyelim ki ben bu uygulamayi gelistirmeye devam ettim, yeni bir versiyon cikardim, yeniden bir container image i yarattim. Simdi bu 3 podu bu yeni image la yeniden olusturmak istiyorum. Bunu nasil yapmam gerekir tek tek butum yaml dosyalarindaki imgae kismini degistirmem gerekiyor. tekrar kubectl apply yaptim ve 3 podu olusturum isler boyle zahmetli oluyor. eskisine donmek istedigimde veya label olusturmak istedigimde isler kontrolden cikiyor.
Deloyment bir veya birden fazla pod u bizim belirledigimiz desire state e gore olusturan ve sonrasinda bu desire state i yani istenilen durumu mevcut durumla surekli karsilastirip gerekli duzeltmeleri yapan bir obje turudur. Bizler bir deployment objesi olusturmak icin bir tanim yapar ve bu tanim icinde olusturmak istedigimiz pod un hangi ozellliklere sahip olacagini ve kac adet olusturmak istedigimizi belirtiriz. Bu deployment objesi olusturuldugu zaman bu tanim ve adette pod olusturulur. Ornegin nginx image indan 3 tane pod olusturan bir deployment yaratiriz. Deployment bunu desire state olarak alir ve bu 3 pod olusturulur ve deployment controller devreye girer. Ornegin bu podlardan birini sildim. Deployment kontroller desire state ile current state i karsilastirir ve sonrasinda esitler. bunun yaninda deployment objelesi bize kurallar sayesinde podlarda guncelleme yapmamiza imkan tanir. Onceki ornektei gibi uygulamanin yeni versiyonunu yazip bundan yeni bir image olusturmustuk. Ama bunu deploy etmek istedigimiz zaman tek tek pod taminlarinda bu isleri manuel yapmistik. Deployment da ise sadece desire state tanimimizla image kismini guncelleyerek bu isi halledebiliriz. Deployment desired state tanimini alir ve tek tek buna gore podlari olusturmaya baslar. Hatta bunu kontrollu bir sekilde yapmasi icin ek parametlereler belirleme sansini da verir. Yani yeni image bu podlari guncelle ama birden butun podlari silip yenilerini olusturmaya calisma. Bir tane sil sonra yenisini olustur 30 sn bekle ikincisini sil seklinde rollout u kontrollu bir sekilde yapmamizi saglayabilir. Boylelikle uygulamanin yeni versiyonu deploy ederken kesintisiz gecis imkani saglar. Bunun yaninda sikinti cikan durumlarda eskiye donmeyi de kolay bir sekilde yapabilmemize imkan saglar. Yani gunceller ama birseylerin yanlis gittigini de gorursek eski haline dondurmeyi tek bir komutla halledebiliriz. Bizler aslinda k8s uzerinde her ne kadar pod olustursak da bu podlari yalnizca pod olarak olusturmayiz. olusturulmak istenen objeleri daha ust seviyede objeler ile olustururuz ve bu sayede uzun vadede yapacagimiz islemleri otomatize etmis oluruz. Deploymentlar bunlarin icinde en sik kullanilandir hatta is yuklerimizin tamami deployment objeleri halinde deploy edilir. 
Best practice olarak yek bir pod bile yaratacak olsaniz deployment ile yaratmak gerekir.

## replicaset
Bir replicasetin amaci herhangi bir zamanda calisan kararli bir replika pod setini surdurmektir. bu nedenle, genellikle belirli sayida ozdes pod un kullanilabilirligini garanti etmek icin kullanilir. Deployment bizim istedigimiz ozelliklerde pod olusturmaz. Replicaset bizim istedigimiz ozelliklerde replicaset objesi olusturur ve podlar bu replicaset objeis tarafindaj olsturulur. 
K8s ilk ciktiginda replicaset-controller adinda bir objesi vardi halen var ancak kullanilmiyor. Replication controller biden fazla ayni tipte pod olusturmak icin kullanilirdu fakat deploy ettigi podlarla ilgili degisiklik yapmak istedigimiz zaman bazi sikintilar cikariyorudu. Bu sikintilari cozmek icin de soyle bir yola gidildi. Bu replication controller in sagladigi ozellikler deployment ve replicaset adinda 2 objeye bolundu. replicaset objesinin temel gorevi su oldu: belirledigimiz ozelliklere gore belirledigimiz sayida pod olusturmak ve bunun desired state de kalmasini saglamak. deployment ise bunun bir ust sebiye objesi olarak dizayn edildi ve pod taniminda bir guncelleme yaparsak bu guncellemenin belirledigimiz kurallara ve sirayla uygulanmasini saglamak oldu. ozetle biz bir deployment objesi olusturdugumuz zaman bu kendi yonettigi bir deploymet objesi olusturur ve bu replicaset objesi de podlari yaratir ve yonetir. Bir deployment taniminda bir degisiklik yaparsak ornegin kullanilan image i guncellersek deployment bu yeni tanimla yeni bir replicaset objesi daha yaratir. Ilk yaratilan replicaset objesi yavas yavas kendi olusturdugu podlari silmeye baslar ve yeni replicaset de yeni podlari yaratir. Burada silme ve yaratma isleminin neye gore olacagini bizler belirleyebiliriz. bu bize herhangi bir kesinti olmadan uygulama guncelleme ve yeni versiyon gecisi yapma imkani saglar. 