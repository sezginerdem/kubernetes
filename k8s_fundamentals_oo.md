# Kubernetes Yonetim Komponentleri

## CONTROL PLANE-MASTER NODE

* ### Kube-apiserver: Resepsiyon ornegi
kubernetes ile alaki um komponentlerin ve dis dunyadan kubernetes platformu ile iletisim kuran tum servislerin ortak giris noktasidir. Api server tum komponent ve node bilesenlerinin direk iletisim kurabildigi tek komponenttir. Tum iletisim api server uzerinden gerceklestirilir. Kube-apiserver k8s de kaynak olusturma islemlerinde api dogrulamasindan sorumludur. Kullanicilar kube ctl komut satiri istemcisi veya rest-api cagrilari araciligiyla api server ile iletisim kurabilirler. Kisacasi disaridan istek gerceklestirmek icin apiserver a ulasir authantication ve authorization islemlerini gercektlestirir ve kubernetese ulasirsiniz. Diger tum komponenetlerde isteklerini api server uzerinden gerceklestirir.

* ### Etcd: 
tum cluster verisi metadata bilgileri ve k8s de olusturulan tum objelerin bilgilerinin tutuldugu anahtar-deger “key-value” veri deposudur. Clusterin mevcut durumu ile ilgili bilgiyi uzerinden barindirir. Kisacasi k8s in calismasi icin ihtiyac duyulan tum bilgiler etcd uzerinde tutulur. Apiserver haric baska hicbir komponent etcd ile dogrudan haberlesemezler. Etcd ile iletisim kurmak istediklerinde bunu api-vserver uzerinden gerceklestirirler. 

* ### Kube-scheduler: ornekte uretim planlama. 
Yaratilmak istenen podlarin gereksinimlerini kontrol ederek o podun en uygun calisacagi worker node un hangisi oldugunu belirler ve o podun burada olusturulmasini saglar. Podun ozel gereksinimerini isteklerini, cpu gibi cesitli parametreleri elinde bulundurarak degerlendirir ve pod icin en uygun node un hangisi olduguna karar verir. 

* ### Kube-controller-manager: ornekte vardiya amiri
Tek bir binary olarak bulunsa da icinden birden fazla kontroller bulundurur. K8s istenilen durumla mevcut durum arasindaki karsilastirmada uygulama yonetimi saglar. Kube-controller altindaki controller managerlar k8s den istenilen durumla mevcut durum arasinda fark olup olmadigi gozlerler. Kube-api araciligiyla etcd de saklanan cluster durumunu inceler ve eger mevcut durum ile istenilen durum arasinda fark varsa iste bu farki olusturan kaynaklari gerektigi gibi olustururak, guncelleyerek veya silerek bu durumu esitler. Ornegin siz k8s e uygulamanizin 3 pod olarak calismanizi bildirdiniz. K8s de bunu gerceklestirdi ve uygulamanizin kostugu 3 pod calismaya basladi ama sonra bir tanesi silindi iste controller-manager bu podun yeniden olusturulmasini saglar. 

Control-plane yani yonetim kismi master node uzerinde calisir. Bu componentlerin hepsi tek bir linux isletim sistemine kurulabilecegi gibi birden fazla sisteme de kurulabilir. etcd tamamen bu yapidan ayri bir sekilde konuslandirilarak daha yuksek bir erisim saglanabilir iken ama genellikle etcd de api-server in kostugu yerde durur. ornegin 3 sunucu uzerinde 3 api-server kurulup diger componentler bu sistemlere dagitilabilir. Bu uzerinde control plane componentlerinin kostugu sistemlere cluster altinda master node deriz. Master nodelar sadece bu sistemleri barindirmak icin bulunur ve production ortaminda herhangi bir is yukumuzu bu sistemler uzerinde calistirmayiz. Bunlarda sadece cluster in yonetim altyapisini kostururuz. is yuklerimiz ise worker nodelar uzerinde calistirilir. Bunlar uzerinde bir container run time barindiran ve clustera dahil edilmis sistemledir. 

## WORKER NODE
***
her worker node da 3 temel component bulunur. ilk ve en onemli component containerlarin calismasini saglayacak bir container run time dir. Default olarak bu docker dir ancak k8s docker container run time destegini birakarak bazen containerd ye gecmistir.

* ### kubelet: ornekte ustabasi
her worker node da bulunur. Kubelet api-server araciligiyla etcd yi kontrole eder. Scheduler tarafindan bulundugu node uzerinde calismasi gereken bir pod belirtildi ise kubelet bu podu o sistemde yaratir. Conatinerd ye haber gonderir ve belirlenen ozelliklerde bir container in o sistemde calismasini saglar.

* ### kube-proxy: ornekte lojistik elemani
Olusturulan podlarin tcp, udp ve etcdp trafik akislarini yonetir. ag kurallarini yonetir. kisaca network proxy gorevi gorur.

Bunlarin disinda dns ya da gui hizmeti saglayan cesitli servisler entegre edilebilen servislerdir. fakat bunlar core componentler olarak adlandirilmaz

# Kubectl Kurulumu
***
Kubernetes e komut vermek icin bubu api-server uzerinden yapiyoruz. 
Bunun 3 yontemi var. 

* #### Rest cagrilari
Uygulamalarin ya da scriptlerin icinde kullanilir
* #### Gui istemcileri
Grafiksel arayuz ile iletisim kurmak. Resmi gui araci dashboard, oktant, land en bilinen araclardir. Gui istemciler en temel araclar degildir. 
* #### kubectl
Shell uzerinden apiserver a komutlar doderdigimiz k8s in resmi cli aracidir. k8s cluster olusturulmadan once ilk olarak kubectl yuklenmesi gerekir.

## kubectl config dosyasi
* kubectl araci baglanacagi kubernetes cluster bilgilerine config dosyalari araciligiyla erisir
* config dosyasinin icerisinde kubernetes cluster baglanti bilgilerini ve oraya baglanirken kullanmak istedigimiz kullanicilari belirtiriz
* Daha sonra bu baglanti bilgileri ve kullanicilari ve ek olarak namespace bilgileirni de olusturarak contextler yaratiriz
* kubectl varsayilan olarak $HOME/.kube/ altindaki config isimli dosyaya bakar
* kubectl varsayilan olarak $HOME/.kube/ altindaki config dosyasina bakar ama bunu KUBECONFIG environment variable degerini degistirerek guncelleyerebilirsiniz

# pod
***
* kubernetes icinde yaratilan en temel obje pod dur.
* kubernetesde olusturabileceginiz en kucuk birimlerdir
* podlar bir yada daha fazla container barindirabilirler ancak cogu duurmda pod tek bir container barindirir
* her bir pod un essiz bir idsi "uid" bulunur
* her pod essiz bir ip adresine sahiptir.
* ayni pod icersindeki containerlar ayni node ustunde calistirilir ve bu containerlar birbirleriyle localhost ustunden haberlesebilirler.
* kubectl ile kubea-api ile haberleserek k8s uzerinde pod yaratma islemi gerceklestirilir. api server bu poda bizim tanimladigimiz bilgileri atar ve bir pod yaratir ve etcd veri tabanina kaydedir. kube-scheduler componenti surekli burayi gozler ve herhangi bir worker node atamasi yapilmamis pod tanimi yapilmamissa o podun calismasi icin uygun bir worker node secer ve bu bilgiyi pod tanimina ekler. Sonrasinda worker node uzerinde calisan kubelet servisi de bu etcd yi surekli gozledigi icin bu pod tanimini gorur ve bu tanimda belirtilen container o worker node uzerinde olusturulur ve boylece pod olusturulma asamalari tamamlanir.

## Pod Yasam Dongusu
#### pending
pod yaratmak icin gonderilen yaml dosyasi kube-api tarafindan alinir ve varsayilan ayarlari da ekleyerek poda bir unique id atar ve tum bunlari etcd ye kaydeder. Bu asamadan sonra pod olusturulmustur ve bu asamadan sonra pending asamasina gecer. Eger bir podun statusu pending durumunda ise bunun anlami sudur: birisi bir pod olusturdu podla ilgili tanimlar yapildi ve veri tabanina kaydedildi ama pod herhangi bir node uzerinde olusturulmadi. 

#### creating
kube-scheduler api-server araciligiyla surekli etcd yi gozler. eger burada yeni yaratilmis ve herhangi bir node atamasi yapilmamis bir pod gorurse calismaya baslar. Algoritmasi ve secme kriterlerine gore podun calismasinin en uygun olacagi node u secer ve veri tabanindaki pod objesine node bilgisini ekler. Bu noktadan itibaren pod yasam dongusunde creating asamasina gecer. eger bir pod creating asamasina gecemiyorsa bu kube-scheduler in uygun bir node bulamadigi anlamina gelir. bunun pek cok nedeni olabilir: cluster da bu podun gereksinimlerini karsilayabilecek node bulunmuyor olabilir. Nodelar uzerinde cpu ve memory gibi kaynaklar tukenmis olabilir. 

#### ImagePullBackOff
Nodelar uzerinde kubelet adli bir servis calisir. bu servis ayni kube-scheduler gibi surekli etcd veri tabanini gozler ve bulundugu node a atanmis nodelari tespit eder ve hemen islemlere baslar. Ilk olarak pod taniminda olusturulacak containerlara bakar ve bu containerlarda olusturulacak image lari sisteme indirmeye baslar. Eger bir sekilde image indirilemezse pod erimagepull ve ardindan da imagepullbackoff asamasina gecer. Eger podun statusunda imagepullbackoff gorurseniz bu nodeun image i repository den cekemedigi ve bunu tekrar tekrar devam ettigi anlamina gelir. Bunun birkac nedeni olabilir: en sik karsilasilan neden pod tanimlanmasinda image isminin yanlis yazilmasidir. Image ismi yanlis yazildigi icin image cekilemeyecektir ya da image i cekmek icin sisteme kaydolmak gerektigi ve bu authentication islemlerinde hata yapilmasidir. Bu nedenle image cekilemeyecektir. Bu gibi bir hata olmamasi durumda ise

#### Runnig
Islemlerin sorunsuz bir sekilde ilerlemesi halinde kubelet node da bulunan container engine ile haberlesir ve ilgili containerlarin olusturulmasini saglar ve containerler running durumuna gecer. Bu noktadan itibaren artik pod olusturulmus olur.

* Containerlarla ilgili temel kural: container icindeki uygulama calistigi surece container calisir. Uygulama calismayi birakirsa container durdurulur. Uygulamanin da durdurulmasi 3 sekilde olur: 1) isi bittigi icin hata vermeden otomatik olarak kapanir. 2) Kullanicinin istegi uzerine komut gonderilir ve hata vermeden kapanir. 3) hata verir, sikinti cikar, hata kodu olusturarak kapanir. Ornegin nginx image uzerinden bir container yarattigimizi dusunelim. Bu uygulama bir deamon bir servis yani siz onu durdurana ya da hata cikip cokene kadar calismaya devam eder. O calistigi surece de container alismaya devam eder. Fakat diyelim ki siz bir image yarattiniz ve bu image in varsayiln uygulamasi da bir script ya da basit bir komut. container baslayinca bu script calisiyor isini yapiyor ve isi bitince kapaniyor yani container da kapaniyor yani illa ki hata vermeden kapanmasina gerek yok.

Bu gibi sorunlar nedeni ile containerlar icin restart policy tanimi yapilir. Restart policy 3 adet deger alabilir:
* Always: Default degerdir. Pod un icerindeki container her durumda durdurulursa durdurulsun her sartta yeniden baslat anlamina gelir. 
* On-failure: Sadece hata alip kapanirsa yeniden baslatilir.
* Never: Pod hicbir zaman yeniden baslatilmaz.

#### Succeed
Podun altindaki containerlar calismaya devam ettikce status running olarak devam eder. Eger containerlarin hepsi hata vermeden dogal olarak kapanirsa ve restart policy never veya onfailure olarak set edildi ise podun statusu succeed olarak statusune gecer ve pod yasam dongusunu tamamlar. Yani bir diger degisle completed, basarili olarak pod un yasam dongusu tamamlanir. 

#### Failed
Never ya da Onfailure olarak policy tanimlandi ve containerlardan biri hata verip kapandi ise bu sefer pod un statusu failed olarak isaretlenir ve yasam dongusunu boyle tamamlar. 

#### CrashLoopBackOff
fakat restart policy always olarak set edildi ise pod hicbir zaman succeed ya da failed durumuna gecmez. Bunun yerien podun icerisindeki container yeniden baslatilir ve running state de devam eder. Fakat kubernetes bu yeniden baslatma islemini belirli bir siklikla yapiyorsa bazi seylerin ters gittigine kanaat getirilir ve pod u CrashLoopBackOff adini verdigimiz bir state e sokar. Bunun anlami sen bir pod olusturdun ama bu pod un icerisindeki container ikide bir kapaniyor ama gene kapaniyor buna bir bak. Bu pod da sunlar olur: k8s container in icerisinde birseylerin ters gittigini anlar ve pod un statusunu crashloopbackoff a cevirir. Surekli restart etme yerine restart eder 10 sn bekler eger 10 sn icinde yeniden cokerse 20 sn bekler yeniden cokerse 40 sn bekler sonra 80 sn bekler ve bu durum 5dk lik araliga cikana kadar boyle devam eder ve ondan sonra her 5dk da bir tekrar eder. Bu arada container cokmeyi birakir ve 10dk sure ile sorunsuz bir sekilde calismaya devam ederse. Kubelet container i crashloopbackoff dan cikarir ve running e dondurur. eger bu olmaz ve siz mudahale etmezseniz bu durum sonsuza kadar boyle devam eder. Ozetle crashloopbackoff mudahaleyi gerektirir ve bakilmasi gerekmektedir. 

## Multicontainer pod
Bir frontedn ve bir de backend den olusan two tier yani cift katmanli bir uygulama yazdik. Cok populer olan orgenin wordpress uygulamasi. WordPress uygulamasini deploy etmek istedigimizi varsayalim. Bildigimiz uzere wordpress php tabanli bir frontend ve bu uygulamanin verilerinin tutuldugu bir mysql veri tabanina sahip. Bu uygualamyi container haline getirmek istiyorum. Bu uygulamayi icinde mysql ve word press halinde calisan tek bir container haline getirebilir miyim? Teknik olarak evet. Fakat bunu yapmiyoruz. Her iki uygulama icin de ayri iki container kullaniyoruz. BBunun iki nedeni var 1) container in teme mantigi izolasyon bu iki uygulamayi da ayni container icine koymak bu izolasyondan mahrum olmak demek. 2) Iki uygulamayi scale etmek istemeniz durumunda yatay buyume yapamiyorsunuz. Bunu soyle dusunun ben 2 sunucudan olusan bir ortam kurdum. Hem wordpress hem de mysql uygulmasini ayni container icine kurdum ve tek bir uygulama olarak calistirmaya basladim. Sayfama yogun bir istek oldugunda wordpress in frondend katmani bu isteklere cevaop verememeye basladi ben de yeniden bir container daha olusturarak load balancer arkasina almak ve kaynaklarimi cogaltmak istedim ki bu problemi cozebileyim. Ama bunu yaptigim zaamn ortamda 2 frontend 2 tane de mysql veri tabani olacak. Ancak benim veri tabanimda veri sikintisi yoktu. Ikisi de ayni container da oldugu icin bunu coklamam gerektiginde ikisini birden cokladim. Hatta 2. containeri deploy ettikten sonra mysql baglanti ayarlarini degistirdim ve 1. container icindeki mysql e baglanmasi icin ayar yaptm cunku simdiye kadar olusturdum her sey o veri tabaninda. Bunun 100 container a kadar scale edilen bir ortam oldugunu dusunun. Sirf 2 uygulamayi da ayni container icine gomdugum icin sikintilar yasadim. Bunun yerine bir mysql ve bir de wordpress olmak uzere 2 ayri imahge yaratsa idim bu sefer 1 tane mysql container yaratilirdi benim de istedigim kadar wordpress scale edebilme imkanim olusurdu. Bu nedenlerden dolayi en onemli best practise bir container icine 1 tane uygulama koymak. 
Kubernets icinde ise word press icin bir pod mysql icin bir pod olusturulmali. Fakat biz istersek bir pod icinde birden fazla container da koyabilirim ama bu da bir container icine iki uygulama koymak ile ayni sey cunku k8s de scale ettigimiz sey pod. Her container da tek bir uyguama her pod da bir container. Peki neden kubernetes ayni pod icinde birden fazla container calismasina izin veriyor? Diyelim ki ben wordpress php uygulamasinin uygulama ici performans degerlerinin merkezi bir yerde toplayarak analiz etmek istiyorum. Bunu yapabilecek bir uygulamyi deploy etmek istiyoruz. Bunu k8s de nasil yapabiliriz? Oncelikle mysql podumuzu ayaga kaldirdik sonrasinda wordpress uygulamamiz agaya kalkti son olarak da yeni uygulamanin ayaga kalkacagi podu yarattim. Son uygulamanin tek bir amaci var wordpress e baglanacak ve uygulamayi analiz edecek veriyi toplayacak yani bu son uygulama wordpress e bagimli. Bu uygulama wordpress e bagimli. Bu uygulama wordpress hangi worker node da olusturuluyorsa orda olustrulmali. Wordpress calismaya basladigi zaman calismali kapandigi zaman kapanmali yani tamamen ona bagimli. yani ben 2. bir wordpres uygulamayi yaratirken 2. bir uygulama daha deploy etmem gerekecek.3. bir wordpress uygulamasi icin 3. log kodunu olusturuacgim. Wordpress i silerken bu uygulamadan bilgi toplatan podu da silmmem gerekecek. Bu cok zaman alan bir islem surekli iki is yapmam gerekiyor. Diger bir sikinti ise diyelim ki benim metric toplayan uygulamam word press uygulamasi ile stirage seviyesinde haberlesmesi ortak dosyalari yazmasi gerekiyor ancak 2 podun ayni local volume e baglanabilmesi icin 2 pod un ayni worker node uzerinde calismasi gerekiyor. diyelim ki ben wordpress podunun olusturdugumda kubescheduler bunu o an en uygulan olan node1 da olusturdu. sonrasinda analiz verisi toplayan uygulama olusturmak istedim ve kubescheduler bunu node2 uzerinde olsuturdu. Bu uygulamalar stroge seviyesinde birbirlerine ulasamayacaklar. Bu da ayri bir sorun. K8s bu sorunlari ortadan kaldirmak icin ayni pod icerisinde 2 container calistirma imkani sagliyor. Birlikte scale edilmesi gereken, birbirleriyle network ve storage sebiyesinde haberlesmesi gereken uygulamalari ayni pod icerisinde ayri ayri containerlar olarak calistirabiliyoruz. Buna terminolojide sightcar container denmektedir. 

## init container
init container da bir pod icerisinde birde fazla pod yaratilmasina imkan verir. Fakat app containerlardan farkli olarak init containerlar pod un yasam dongusu boyunca calismaz. Siz bir pod tanimina init container tanimi koydugunuz zaman pod olusturuldugu zaman bu init container calistirilir. Init container calisir icindeki uygulama ne yapacaksa onu yapar ve ardindan kapanir bu init containerlar islerini tamamlayip kapanana kadar da app containerlar calismaya baslamaz. Peki neden boyle bir seye ihtiyac duyariz ve kullanim alani nedir? Init containerlar esas uygulamamiz calismadan once tamamlamamiz gereken seyler var ve bunlari tamamlamadan esas uygulamayi baslatmak mantikli degilse kullanmak gerekir. Mesela uygulamamizin bagimli oldugu baska bir uygulama ya da servis var. Eger bu ayakta ve hazir degilken uygulamayi baslatirsak uygulamada sikinti cikiyor. Bu durumda pod tanimina init container tanimi ekler ve bu init containerin bu servisi gozlemesi icin ayar yapariz. Init container icinde bir uygulama calistirilir ve bu servisten okey alana kadar calisacak okey aldiktan sonra da kapanacak sekilde ayar yapariz. Bu sayede pod olusturuldugu zaman ilk olarak init container calisir. Diger servisi beklemeye baslar. Diger servis hazir oldugu anda uygulama kapanir ve init container da kapatilir. Init container kapatildigi anda da esas uygulamamizin calistigi container ayaga kalkar ve boylece servisimiz hazir olana kadar uygulama beklemis olur. Soyle bir senaryo dusunun ana uygulamamizin ihtiyaci olan bazi config dosyalarinin guncel halinin sisteme cekilmesi gerekiyor iste bu cekme islemini de init container ile halleder ve ana uygulama baslamadan bunlari sisteme indirebiliriz. 

## label and selector
label yani tagler k8s de her turlu objeye atayabildiginiz anahtar deger eslenikleridir. Etiketler sayesinde olusturdugumuz objelere bizlerin anlayacagi ve gruplara yaparken kullanabilecegimiz bilgiler eklemis oluruz. Bu sayede k8s tarafindan bizlere core ozellik olarak sunulmayan objelere belirli bir aidiyet atanmasi islemini gerceklestirebiliriz. ornegin k8s de 5 ekip tarafindan olusturulan 100 den fazla pod calistiriyoruz. Hangi pod un hangi ekibe ait oldugunuz belirtmek istersek k8s in bize belirtmis oldugu bir hiyerarsi mekanizmasi yok. ama labels sayesinda bunu saglayabiliyoruz. ornegin her ekip kendi olusturdugu objeye team:teamname seklinde etiket atarsa bizler bu etiketler sayesinde hangi ekibin hangi objelere sahip oldugunu listeleme imkanina kavusuruz. Ya da uygulamamizin frontend katmanini olusturan objelere tier:front-end backend katmanini olusturan objelere ise tier:backend etiketini atayarak frontend ve backend katmanlarini ayri ayri listeleme imkanina kavusuruz. 
Etiketler obje olusturulurken atanacagi gibi olustuktan sonra da atanip silinebilir. Etiketler anahtar veri degerleri seklinde olusturulur. Etiketler bir dns subdomanin seklinde atanan opsiyonel bir on ek kismi bulundurabilir example.com/tier:frontened gibi. ama bu kisim zorunlu degildir. Bu kisimdan sonra ana kisim yazilir bu iki kisim anahtar ve deger kismindan olusur. Bu anahtar ve deger kismi en fazla 63 karekter olabilir. Alfanumerik bir karakterle baslamali ve bitmelidir. Tire alt cizgi noktolar ve arasinda numerik degerler icerebilir ancak bunlarla baslayamazlar.
Etiketlerin bir onemli gorevi de k8s de objeler arasindaki baglanti da labelslar araciligiyla kurulur. Servisler ve deployment objeleri hangi podlar ile iliski kuracagini labellar sayesinde belirlerler. ornegin bir servis yaratir ve ona git tier:frontend labelli podu bul ve servisi bu poda yonlendir.
yaml dosyalarinda selector araciligiyla deploymenta senin yonetecgein podlar app:frontend label ina sahip olacaklar. Bunu gorursen anla ki bu podlar senin podlarin. yaml dosyasi icindeki template alindaki labels kismian da bu app:frontend i ekliyorum ki deploymen hangi pod u sececegini anlayabilsin. 
Labels ve selector kismi 3 acidan onemli
* Ilk olarak bu bir zorunlu alan. Yani deployment da en az bir selector tanimi olmali ve ayni labellar template kisminda da bulunmali.
* egerki birden fazla deployment objesi olusturacaksaniz ki production ortaminda bu olacak siz her deployment objesinde farkli label ve selector kullanilmali. ayni labellar kullanilmasi sikinti cikarir.
* ayni labellari kullandiginiz baska objeleri yaratirsaniz misal sibgleton bir pod yaratirsaniz bu da sikinti cikarir. 


## Annotation
Aynen labellar gibi anahtar veri opsiyonu ekleyebilecegimiz 2. secenek ise annotations. Labellar k8s icin cok onemli oldugu icin label eklemek veya cikarmak clusterda birseyleri tetikleyebilir. dolayisiyla her bilgiyi label olarak metadataya ekleyemeyebiliriz. Ancak bu bilgiler gerekli olabilir. Mesela podun ne zaman ve kim tarafindan olusturuldugu bilgisini eklemek istiyorum. Bu bilginin label olarak eklenmesi dogru degil, yani bu bilgiyi label olarak secme istegim yok veya bu bilgiyi herhangi bir secme istegi olarak kullanmak da istemiyorum. Dolayisiyla bunu annotationa eklemek daha dogru olacaktir. Ayrica annotation k8s in ana componenti olmayan fakat k8s ile baglantisi olan yazilimlar tarafindan ihtiyac duyulan bilgilerin de yazildigi yerdir. Ornegin firmamizin destek ekipleri tarafindan kullanilan bir cagri kabul uygulamasi var. Bu yaziimi k8s den bilgi cekebilecek hale getirdik ve soyle bir sistem kurmak istiyoruz: herhangi bir k8s objesinde bir sikinti ciktigi zaman bu cagri yazilimi bunu tespit etsin ve o objenin destek sorumlusu olarak belirlenen insana mail gondersin. Bu durumda o mail adresi bilgisini objenin metadatasina annotation olarak ekleyebiliriz ve destek yazilimi bu bilgiyi cekebilir. bu sayede de bu insana mail gonderebilir. Annotation bu ve benzeri durumlar icin kullanilir. Olusturulma kurallari labellarla hemen hemen aynidir. DNS subdomain olarak eklenebilecek ve zorunlu olmayan bir prefix ile baslayabilen, sonrasindala labellar gibi 63 karakteri gecmeyecek, nokta tire ve alt tire icerebilen ama bunlarla baslayamayan alfanumerik degerler icerebilir. value kisminda ise bu kurallar gecerli degildir. Alfanumerik olmayan karakter de alabilir. 

## Namespace
Soyle bir senaryo kurun bi formada calsiyoruz ve tum calisanlarin ortak calistiklari dosyalari barindirabilecekleri bir yapi tasarliyoruz. 10 ayri ekibimiz var ve bu 10 ayi ekibin de erisip dosya barindirabilecegi bir latyapi kurmak istiyoruz. Bu sistemi nasil kurariz: bir file server yaratirim ve bunun altinda tum ekibin erisebilecegi bir paylasim alani yaratirim. Sonra gider tek tek tum kullanicilarin bilgisayarlarinin buraya erismesini saglayabilirim. Baslangicta sorunsuz olabilir ancak birden fazla ekibin olmasi durumunda soru olabilir. Bu dosya altinda 100lerce dosyanin olmasi durumunda bu belli bir zaman sonra yonetilemez hal alabilir. Bir diger sikinti da su olabilir diyelim ki ben a.txt adinda bir dosya olusturdum. baska bir arkadasim da a.txt adinda bir dosya olusturup ayni yere atamaz. Bir baska sikinti da su olabilir. diyelim ki sadece hr calisanlarinin gormesi gereken dosyalari da burada tutuyorum. bu dosyalarin sadece hr tarafindan gorulmesini saglamak icin dosya bazinda ayarlamalar yapmam gerekir. her dosya upload edildigide de bu islemleri tekrarlamam gerekir. Son olarak da su sikintiyi yasariz. Herkes tek bir alana dosya kaydettigi icin ekip bazli kota ayarlamasi yapmam zor olur. yani hr kismina 20gb alan ayirayim it kismina 100gb alan ayirayim gibi ayarlamalar yapamam. Cozum belirli bir zaman sonra yeterli olmamaya basladi.
Her ekip icin klasor altinda ayri bir alan yaratirsak tum bu problemleri ortadan kaldiririz. Her ekibe ozel bir klasor olusturulur. Her ekip kendi dosyalarini kendi klasorlerindde tutabilirler. Ayrica it ekibi gibi ekiplere ozel proje bazli klasorler de yaratiriz. Ornegin yeni bir urun gelistirmesi yaratiriz ve bu urune ait gelistirme dosyalarini bu klasorde tutariz bu sayede tum guvenlik ayarlarini klasor bazda halledebiliriz. hr klasorune sadece hr calisanlari erissin it kalsorune sadece it calisanlari erissin, proje klasorunde proje yoneticisinin yazma izni olsun ama stayjer sadece dosyalari okuyabilsin diyebiliriz. Boylece guvenlik ve gruplama ile pek cok sorunun ustesinden geldik. Isim cakismasini engelledik. son olarak da klasor bazinda kota ayarlamasi yaparak kaynak kisitlamasi saglabiliyoruz. Namespace tam anlami ile bu ise yarar. 
Kubernetes cluster i bu ornekteki fileserver olarak dusunursek namespace ler de burada yarattigimiz klasorlerdir. k8s de objeleri sanal klasorler altinda olusturabilir ve bu sanal klasorler altinda olusturabilir ve sonrasinda bu sanal klasor bazinda erisim izni ve kota ayarlamasi yapabiliriz.
Her k8s clusterinda varsayilan olarak 4 namespace olusturulur.
* kube-sysem: k8s tarafindan olusturulan objelerin tutuldugu namespace dir. 
* kube-public: kimligi dogulanmamis olanlar da dahil tum kullanicilar tarafindan erisilmesine ihtiyac duyulan objelerin olusturulabilecegi yerdir. 
* kube-node-lease: node hard disk islemleri icin olusturulan ozel bir namespacedir.
Kisacasi kube ile baslayan tum namespaceler kubernetes tarafindan olusturulur ve clusterin isleyisi ile alakali objelerin tutuldugu yerlerdir. Bunlarin disinda default namespace adinda baska bir namespace daha olusturulur ve adindan da anlasilacagi uzere bizler aksini belirtmedigimiz surece objeler burada olusturulur.
Bizler herhangi bir namespace tanimi yapmadan objelerimizi bir tek namespace altinda toplamayiz. Tek bir ekip tarafindan yonetilen kucuk bir kubernetes clusterimiz var ise boyle de devam ederiz. Fakat ne zaman buyur ve birden fazla ekip tarafindan yonetilen ve birden fazla ekip tarafindan deploy edildigi bir k8s cluster a evrilirsek iste o zaman namespaceler bize kota ayarlamasi ve namespace bazinda kullanici erisimi verebilme gibi ozellikleri sayesinde yardimci olacaktir. 

## deployment
En temel obje poddur. ama biz genelde tekil yonetilmeyen podlar yaratmayiz. podlari yoneten ust seviye objeler yaratiriz ve podlar bu objeler tarafindan yaratilir ve yonetilir. 
neden tekil podlar yaratmiyoruz?
Deployment buan nasil cozum bulur?
Bir uygulamamizi production ortaminda deploy ettigimizi varsayalim. Uygulamamizi container image i haine getirdik ve bunu kubernetes de deploy etmek icin bir pod tanimi yarattik kubectl apply ile bunu api sever a gonderdik. 3 worker node lu bir yapimiz var ve kube-scheduler uygun bir worker node secerek podu o node uzerinde olusturdu. fakat bir sure sonra bu podun olusturuldugu worker node bozuldu ve erisilemiyor. dolayisiyla pod a da erisemiyoruz. bu durumda pod terminating durumuna gecer ve oylece kalir ve tabii ki de pod calismadigi icin uygulamamiz da calismamaya baslar. Siz bir pod yarattiginiz zaman kube-scheduler bu poda uygun bir node bulur ve onun uzerinde bu pod u calistirir. eger container seviyesinde bir sikinti cikarsa ve restart policy onfailure ya da always olarak secilirse container restart edilir ve sorun cosulur. ama pod un schedule edildigi node da bir sikinti cikar ya da kaynak sikintisi nedeni ile o pod durdurulursa kube-scheduler devree girip aa su pod calismiyormus deyip baska bir node uzerinde onu schedule edeyim demez. Bu nedenle worker node umuz gittigi icin de pod umuz terminate state e duser ve oylece kalir. Buna cozum olarak da 3 ayri podun 3 ayri node uzerinde calismasini node selector tanimlari ile sagladik. Kisacasi her bir worker node da bir tane olmak uzere 3 tane pod yarattik, onlerine de bir load balancer koyduk. Boylece bir tanesine bir sey olursa diger 2 tanesine erisilebilmesini sagladik. Ancak diyelim ki ben bu uygulamayi gelistirmeye devam ettim, yeni bir versiyon cikardim, yeniden bir container image i yarattim. Simdi bu 3 podu bu yeni image la yeniden olusturmak istiyorum. Bunu nasil yapmam gerekir tek tek butum yaml dosyalarindaki imgae kismini degistirmem gerekiyor. tekrar kubectl apply yaptim ve 3 podu olusturum isler boyle zahmetli oluyor. eskisine donmek istedigimde veya label olusturmak istedigimde isler kontrolden cikiyor.
Deloyment bir veya birden fazla pod u bizim belirledigimiz desire state e gore olusturan ve sonrasinda bu desire state i yani istenilen durumu mevcut durumla surekli karsilastirip gerekli duzeltmeleri yapan bir obje turudur. Bizler bir deployment objesi olusturmak icin bir tanim yapar ve bu tanim icinde olusturmak istedigimiz pod un hangi ozellliklere sahip olacagini ve kac adet olusturmak istedigimizi belirtiriz. Bu deployment objesi olusturuldugu zaman bu tanim ve adette pod olusturulur. Ornegin nginx image indan 3 tane pod olusturan bir deployment yaratiriz. Deployment bunu desire state olarak alir ve bu 3 pod olusturulur ve deployment controller devreye girer. Ornegin bu podlardan birini sildim. Deployment kontroller desire state ile current state i karsilastirir ve sonrasinda esitler. bunun yaninda deployment objelesi bize kurallar sayesinde podlarda guncelleme yapmamiza imkan tanir. Onceki ornektei gibi uygulamanin yeni versiyonunu yazip bundan yeni bir image olusturmustuk. Ama bunu deploy etmek istedigimiz zaman tek tek pod taminlarinda bu isleri manuel yapmistik. Deployment da ise sadece desire state tanimimizla image kismini guncelleyerek bu isi halledebiliriz. Deployment desired state tanimini alir ve tek tek buna gore podlari olusturmaya baslar. Hatta bunu kontrollu bir sekilde yapmasi icin ek parametlereler belirleme sansini da verir. Yani yeni image bu podlari guncelle ama birden butun podlari silip yenilerini olusturmaya calisma. Bir tane sil sonra yenisini olustur 30 sn bekle ikincisini sil seklinde rollout u kontrollu bir sekilde yapmamizi saglayabilir. Boylelikle uygulamanin yeni versiyonu deploy ederken kesintisiz gecis imkani saglar. Bunun yaninda sikinti cikan durumlarda eskiye donmeyi de kolay bir sekilde yapabilmemize imkan saglar. Yani gunceller ama birseylerin yanlis gittigini de gorursek eski haline dondurmeyi tek bir komutla halledebiliriz. Bizler aslinda k8s uzerinde her ne kadar pod olustursak da bu podlari yalnizca pod olarak olusturmayiz. olusturulmak istenen objeleri daha ust seviyede objeler ile olustururuz ve bu sayede uzun vadede yapacagimiz islemleri otomatize etmis oluruz. Deploymentlar bunlarin icinde en sik kullanilandir hatta is yuklerimizin tamami deployment objeleri halinde deploy edilir. 
Best practice olarak yek bir pod bile yaratacak olsaniz deployment ile yaratmak gerekir.

## replicaset
Bir replicasetin amaci herhangi bir zamanda calisan kararli bir replika pod setini surdurmektir. bu nedenle, genellikle belirli sayida ozdes pod un kullanilabilirligini garanti etmek icin kullanilir. Deployment bizim istedigimiz ozelliklerde pod olusturmaz. Replicaset bizim istedigimiz ozelliklerde replicaset objesi olusturur ve podlar bu replicaset objeis tarafindaj olsturulur. 
K8s ilk ciktiginda replicaset-controller adinda bir objesi vardi halen var ancak kullanilmiyor. Replication controller biden fazla ayni tipte pod olusturmak icin kullanilirdu fakat deploy ettigi podlarla ilgili degisiklik yapmak istedigimiz zaman bazi sikintilar cikariyorudu. Bu sikintilari cozmek icin de soyle bir yola gidildi. Bu replication controller in sagladigi ozellikler deployment ve replicaset adinda 2 objeye bolundu. replicaset objesinin temel gorevi su oldu: belirledigimiz ozelliklere gore belirledigimiz sayida pod olusturmak ve bunun desired state de kalmasini saglamak. deployment ise bunun bir ust sebiye objesi olarak dizayn edildi ve pod taniminda bir guncelleme yaparsak bu guncellemenin belirledigimiz kurallara ve sirayla uygulanmasini saglamak oldu. ozetle biz bir deployment objesi olusturdugumuz zaman bu kendi yonettigi bir deploymet objesi olusturur ve bu replicaset objesi de podlari yaratir ve yonetir. Bir deployment taniminda bir degisiklik yaparsak ornegin kullanilan image i guncellersek deployment bu yeni tanimla yeni bir replicaset objesi daha yaratir. Ilk yaratilan replicaset objesi yavas yavas kendi olusturdugu podlari silmeye baslar ve yeni replicaset de yeni podlari yaratir. Burada silme ve yaratma isleminin neye gore olacagini bizler belirleyebiliriz. bu bize herhangi bir kesinti olmadan uygulama guncelleme ve yeni versiyon gecisi yapma imkani saglar.
Replicaste ile deployment yaml ayni arasidaki tek var kind kisminda birisinde replicaset yazarken digerinde deployment yazar. Geri kalan tum satirlar ayni kalmaktadir.

## Rollout and Rollout
Deployment yaml dosyalarinda spec altinda strategy anahtari ile bizler bu deploymenti guncelledigimiz zaman rollout islemlerinin nasil yapilacagini belirleriz. 
Kullanabilecegimiz 2 tip rollout stratejisi mevcuttur. 
* Recreate: Bu deploymentta bir degisiklik yaparsam oncelikle bu deploymet daki tum podlari sil ve bu islem tamamlandiktan sonra yeni podlari olustur. Bu stratejiyi genelde hardcore migration yaptimizda kullaniriz. Ornegin uygulamamizin yeni versiyonu ile eski versiyonunun kisa bir sure icin bile olsa bir arada calismasinin sakincali olacaksa, major degisikliklerde recreate stratejisi secilerek oncelikle tum eski verisyonlarin sistemden kaldirilmasi ve sonrasinda yeni podlarin yaratilmasi istenebilir.
* Rollingupdate: Default stratejidir. Herhnagi strateji belirtmez iseniz bu strateji uygulanir. Recrate in tam tersidir. Degisikligi asamali olarak yapar. Bu islemin de 2 farkli opsiyonu vardir. Bunlar maxUnavailable ve maxSurge dir. maxUnavailable: ben deploymentda bir degisiklik yaptigim zaman en fazla burada belirledigim sayi kadar podu sil. MaxUnavailable: 2 su demek: Yani ornegin 10 tane poddan olusturulan bir deployment da bir guncelleme yapar isem bu guncellemeye baslandigi anda en fazla 2 tanesini siliyor. Sonra yeni podlari olusturuyor onlar olusturulduktan sonra devam ediyor yani bir nevi gecis sirasinda en fazla kac pod un silinebilecegi bilgisini giriyorum. Burada sayi yerine yuzde de girilebilir. maxSurge: gecis sirasinda toplam pod sayisinin en fazla kac pod olabilecegini belirler. soyle bir sey dusunun 10 podlu bir deployment i guncelledigim zaman ne olacak kubernetes sirayla 10 tane eski podu silecek ve 10 tane de yeni pod ayaga kaldiracak ve bunu asama asama yapacak yani ayni anda sistemde hem yeni hem de eski podlar olacak. Iste bu durumda toplamda sistemde en fazla kac pod olabilir sayisini maxSurge ile belirliyoruz. Bu ornekte 2 secilmis bu su demek oluyor bizim desired state imiz 10 pod ama gecis sirasinda 12 poda kadar calisabilir. Yani olacak olan su ben bu deployment i olusturdum, sonra update ettim, k8s oncelikle yeni bir replicaset olusturacak ve yeni tanimda 2 pod ayaga kalkacak. Doalyisiyla eski 10 pod + 2 yeni pod toplam 12 pod ayakta olacak. Sonrasinda eski replicaset 2 tane podu silecek. Toplam pod sayisi tekrar 10 a dusecek. Yeni replicaset yeni 2 pod olusturacak toplam sayi 12 ye cikacak sonra eski replicaset tekrar silecek sonra yeni replicaset tekrar olusturacak tek tek toplam sayi 12 yi gecmeden ve 8 in altina da dusmeden islemler devam edecek ve sonunda eski replicaset 0 a yeni replicaset de 10 a cikacak bu sayede sistemde hicbir kesinti yapmadan gecis yapmis olacagiz.
maxUnavailable ve macSurge degerlerinin default degeri yuzde 25 dir.
Rollback ise deployment da yapilan degisikllerin geri alinmasidir.
Rollout komutunda 3 opsiyon mevcuttur:
* Rollout status: Bu opsiyonu ile deployment imizda yapilan degisiklikleri canli olarak izleme imkani elde ederiz. 
* Rollout pause: Bir rollout un ortasinda bu komut ile mevcut komut durduruluyor.
* Rollout resume: Durdurmus oldugum deployment i yeniden devam ettirmek istersem resume komutu ile devam ettiriyorum. 

## Network
Network temel kurallari
* Kubernetes kurulumunda podlara ip dagitilmasi icin bir ip adres araligi ya da kubernetes terminolojisinda bilinen adiyla --pod-network-cidr belirlenir.
* Kubernetes'de her pod bu cidr blogundan atanacak bir unique ip adresine sahip olur.
* Ayni cluster icerisindeki tum podlar varsayilan olarak birbirleriyle herhangi bir kisitlama olmadan ve NAT yani network address translation olmadan haberlesebilirler.

Diyelim ki elimde 4 tane sunucum var ve bunlardan kubernetes cluster olusturmak istiyorum. Evdeki internet baglantimi saglayan bir routerim var ve bu routerin da ic networku 192.168.1.0/24 networku.  router in ic bacagi da 192.168.1.1. ip adresine sahip. buraya bir switch bagladim ve router ve makinalari da bu switch e bagladim. Makinalara 192.168.1 networkundan ip adresi atadim ve hepsinin default gateway leri router i goruyor. Bu makinalar hepsi ayni network uzerinde ve dolayisiyla birbirleriyle sikintisiz bir sekilde haberlesebiliyorlar. Default gateway olarak da router olarak ayarli oldugu icin de dis dunyaya gidecek tum trafigi bu router a yonlendiriyorlar ve onun ustunden gitmek istedikleri yere gidebiliyorlar. Bu makinalarin hepsine linux isletim sistemlerini de yukledim. Simdi sirada k8s kurulumu var. Tum makinalarda k8s kurulumu icin gerekli on hazirliklari yaptim ve master node a gecerek k8s kurulumunu baslatacagim. oncelikle podlar icin bir ip adres araligi belirlemem gerekiyor yani --pod-network-cidr. Ben de bu network disindan bir ip adres araligi belirledim ve 10.10.0.0/16 networkunu sectim. k8s kurulumuna --pod-network-cidr=10.10.0.0/16 opsiyonunu girerek basladim, master node ayaga kalkti. Sonra gittim tek tek worker node lari da cluster a dahil ettim ve k8s kurulumum tamamlandi. Bu kurulumu tamamladigimizda worker node larda su islemler olur: ilk olarak her bir worker node uzerinde bir adet virtual bridge yaratilir. Bu virtual bridge bizi baslangicta belirledigimiz pod-cidr araliginda iletisim kuracak seklinde ayarlanir. Sonrasinda yeni bir pod yarattigimiz zaman o pod icin linux kernelinda ayri bir network namespace olusturulur ve bu pod un sanal ethernet sifir interface ini bu pod cidr adres blogundan bir ip adresi atanarak bulundugu host ustundeki bridge e balganir. 6 adet pod yarattim ve bu podlarin hepsi cidr blogundan ip adresi almis durumda. Bu asamada network trafiginin nasil isledigini dusunelim bununla ilgili 3 ayri senorya yapilacak:
1. senaryoda podlar dis dunyadaki bri adrese gitmek istediginde network akisi nasil oluyor bunu pod a uzerinden inceleyecegiz. Diyelim ki bu senaryodaki pod-a dis yunyadaki 1.1.1.1 ip adresli bir sunucu  ile iletisim kurmak istedi. Bu durumda pod-a iletisim paketlerini bridge e teslim edecek paket burdan worker node un ethernet sifir internet face ine ulasacak peket burada nat lanaack sonrasinda paket worker node un default gateway i olan rooter a gidecek. Rooter da yeniden natlacak oradan da 1.1.1.1 ip adresli sunucuya ulasacak. Bu sunucudan gelecek paket de yeniden natlacak ve pod-a ya ulasacak. Dolayisiyla pod-a uzerinde calistigi worker node un ulasabildigi her destination a ulasabilecek. 
2. senaryoda yine pod-a yi ele alalim ve bu sefer pod-a nin ayni worker node da bulunan pod-b ile iletisim kurmak istedigini varsayalim. Gordugunuz gibi her iki pod da ayni worker node uzerinde bulunan ayni bridge e bagli ve ayni ip adres blogundan adrese sahipler. Dolayisiyla pod-a paketi bridge e teslim edecek bridge den de pod-b ye gidecek. pod-b nin cevabi da bbu yolun aynisi olacak. Ayni network deki 2 makina gibi sorunsuz ve natsiz birbirleriyle haberlesebilecekler. 
3. Bu senaryoda pod-a baska bir worker node da deploy edilmis bir pod ile iletisim kurmak istiyor. Ornegin worker2 node uzerindeki pod-c ile. Kurallarda ne demistik podlar birbirleriyle nat (network adress translation) a ihtiyac duymadan birbirleriyle haberlesebilmeli. Peki kural buysa pod-a pod-c ile nat olmadan nasil haberlesecek? Pod-a pod-c ye ulasmak istiyor paketi bridge e teslim etti. ancak bridge de pod-c nin ip adresi tanimli degil. Mecburen trafigi ethernet sifir a gonderecek. Ethernet sifir bakacak destination 10.10.20.2 bilmedigi bir adres. Kendi ip adresi blogu disinda, routing tanimli degil. napacak mecburen natlayip router a teslim edecek. Orada da paket kaybi olacak. Cunku router da peketi nereye gonderecegi ile ilgili bir bilgi yok. Bu senaryoda ayri worker nodelarda kosan podlar birbirleriyle haberlesemiyorlar. Bunu cozecek birkac teknoloji var. Mesele vxlan teknolojisini kullanarak 10.10.0.0/16 network unu node larin bagli oldugu 192.168.1.0 network u uzerinde kosacak bir overlay network u olarak tanimlayarak nat a ihtiyac duymadan trafik akisi saglayabiliriz. Ya da podlara 198.162.1.0 networkundan ip verecek sekilde tanim yapip worker node un ethernet sifirlarina bu ip adreslerini de tanimlayip trafigi nat siz route edebiliriz. Bu ve benzeri bizim network altyapimiza uygun olacak bir cozum bulabiliriz. Ancak tum bu cozumleri k8s altinda saglamak neredeyse imkansiz. Bu nedenle k8s kendi icinde bir network cozumu ile gelmez bunun yerine contaner run timelar veya kubernetes gibi orchestratorlar ortaya ciktiklarinda hepsi bu ve benzeri network sorunlarini adreslemek icin cesitli yontemler denediler ve uyguladilar fakat bir sure sonra bu ayri ayri yontemler belirli bir sure sonra karmasa yaratti. Bu karmasayi cozmek adina Cloud Native Computing Foundation (CNCF) tarafindan tum bu linux container alt yapisinda ip dagitimi bridge tanimlari vxlan tanimlari ve diger ayarlarin standartlarini belirleyebilecek container Container Netork Interface (CNI) denilen bir proje baslatildi. CNI bu container netork altyapisinin naisl olmasi gerektigi ile ilgili standartlar yayinladi ve developerlarin bu standarda uygun networking cozumleri olusturabilmesine imkan sagladi. Bu sayede bu standartlara uygun bir sekilde network plugin leri yazilma imkani dogdu. K8s de bu projenin bir tarafi olarak sunu dedi bakin ben k8s de bu pod networkunu kendim cozmeyecegim CNI standardini benimsedim. Siz kubernetes clusterinizda network islerinizi bu standarda uygun olarak yazilmis ve sizin network cozumunuze uygun olacak bir plugin secin. Yani k8s  bu isleri kendi uzerinden atti ve sizin k8s i deploy ettiginiz ortama uygun CNI network plugini secerek tum bu altyapiyi bunun tanimlamasina izin verdi.
K8s kurulumunda ortamimiza uygun bir CNI plugin i secmemiz sarttir. Kullanabilecegimiz pek cok open source plugi vardir ve hepsinin degisik ozellikleri vardir. Mesela Azure uzerinden yonetilen Aks kullansa idik Azure kendi yazdigi aks plugin ini kullanarak her pod un vnetten direk bir ip adresi almasini saglar ve bu iletisimi bu sekilde cozer. Ya da cisco router lardan olusan bir network altyapisi kullaniyor olsa idik cisco nun aci plugin ini kullanip bunun bu isi halletmesini saglayabilirdi. Tum bu cni plugin dunyasi ortama uygun olarak secilmelidir. Bunlarin 2 tanesi on plana cikti. Birincisi flaner ve calico dur. k8s kurulumunu tamamladiktan sonra cni plugin ini de yukleyerek k8s in network yonetimini bu plug ine devretmesini saglayabiliyoruz. Ornegin biz bu ortamda calico cni plugin yuklersek podlarin ip adresinin atanmasi ip table kurallarinin duzenlenmesi ve node lar arasinda bu cidr blogunun nasil calismasi icin overlay tanimlarinin yapilarak vxlan interfacelarinin olusturulmasi dahil tum gorevler calico tarafindan hallediliyor. k8s de bir pod olusturdugunuz zaman scheduler bunun calisacagi node u secer ve kubelet burada devreye girerek podu olusturur. Pod icin tanacak ip adresinin belirlenmesi bunun poda atanmasi ve altyapida ayarlanmasi gereken tum islemlerin halledilmesi ise calico tarafindan saglanir. Boylece pod lar farkli node larda olsalar bile birbirleriyle nat olmadan haberlesebilirler. k8s de ingress trafigi bu sekilde gerceklesiyor.

## service
Bir dizi pod yzerinde calisan bir uygulamayi ag hizmeti olarak gostermenin soyut bir yolu.
Bir web sitesi olusturacagimizi hayal edelim. Insanlar bu web sitesine baglanacak ve resim yukleyecek. Yukledikleri resimlere bir filtre ekleyecegiz ve gero dondurecegiz. Biz bu uygulamayi 2 ayri servis olarak tasarladik. Birinci servis front end adinda olacak ve kullanicilar buraya baglanacak. Bu kismi java script ile yazdik. 2. kisim ise bu front end in baglanarak resimleri gonderdigi ve b resimlere filtre ekleyen yazilim olacak. Bunu da .net de yazdik. Her iki kismi da container image lari haline getirdik, artik k8s uzerinde bunu deploy edebiliriz. 2 tane deployment yaml dosyasi olusturduk ve bu frontend ve backend katmanlarini 3'er pod olacak seklinde k8s uzerinde deploy ettik, senaryomuz bu. simdi bu senaryonun erisim kismini inceleyelim. Ilk olarak bu 3 frontend poduna benim bir sekilde dis dunyadan baglanilmasini saglamam gerekiyor. Cozmemiz gereken ilk sorun bu. Benim bu podlara dis dunyadan bir sekilde erisim saglamam lazim. Bu sorunu  bir sekilde cozuk diyelim. Kullanicilar bu podlardan birine baglandi ve resmi yukledi. Benim bu forntedn uygulamamin resmi islemesi icin backend podlardan birisi ile iletisim kurmasi ve resmi ona gondermesi gerekiyor. Bu kisim sanki daha kolay gibi. Sonucta k8s uzerinden her podun essiz bir ip adresi var ve her pod birbiri ile sikintisiz haberlesebiliyor. Fakat simdi soyle bir sikinti var. 3 frontend pod u deploy edecek seklide bir deployment objesi olusturdum ve o da 3 pod olusturdum. ayni sekilde 3 backend podu olusturacak deployment objesi de olusturdum. Benim frontend podlarin backend podlarim ile network uzerinden iletisim kurmalari gerekiyor. Frontend e bir resim yuklendigi zaman bu frontend herhangi bir backend pod ile iletisim kurarak ona resmi yollayacak o da isleyip geri yollayacak. Bunun olmasi icin frontendlerin backend podlara nasil ulasabilecegini bilmeleri gerekir. Kisacasi onlarin ip adreslerini bilmeleri lazim. Ben bu sorunu cozmek adina oncelikle tum backend podlarinin ip adreslerini listeledim sonra frontend podlarima tek tek baglandim ve iletisim kumalari gereken ip adreslerini yazilima ekledim ve bu sayade frontendler backendlere erisebildi sorunu simdilik cozdum. Ama simdi soyle bir sikinti var. Bildiginiz gibi k8s cesitli durumlarda bu podlari bu sistemlerden kaldirip yeniden olusturabilir. Ornegin node lardan biri bir sikinti cikardi ve node devre disi kaldi. Backend deployment objesi bu durumu tespit etti ve devre disi kalan node lar uzerinde kalan podlari saglikli node lar uzerinde yeniden olusturdu. Yeniden olusan podlar da yeni ip adresleri aldi. Simdi benim tekrardan frontendlere baglanarak yazilim icerisinde bu ip adreslerini guncellemem gerekecek ve isin kotu tarafi su ki bu durum k8s uzerinde surekli olabilir. Surekli pod silinerek yeniden yaratilabilir. Ya da diyelim ben bu backend deploymenti scale ettim. Artik 4 pod calisiyor bunu da eklemem gerekebilir. Goruldugu gibi benim bu senaryoda cok islem yapmam gerekiyor. Neyse ki bu mekanizmayi cozebilecek bir mekanizma k8s de mevcut.
k8s servisler bu ve benzeri sikintilari ortadan kaldirmaya yarayan objelerdir. 4 tip servis objesi yaratabiliriz. 
* Cluster IP: Bizler bir cluster ip tipi servis objesi yaratip, bunu label selector ler araciligiyla podlarla iliskilendirebiliriz. Bu objeyi yarattigimiz zaman bu obje cluster icerisindeki tum podlarin ve kaynaklarin cozebilecegi unique bir dns hizmetine sahip olur. Bunun yaninda bizler her k8s hizmeti kurulumunda servis cluster range ip anahtari ile bir sanal ip blogu belirleriz. Ornegin 10.10.0.0/16. Siz bir cluster ip servis objesi yarattiginiz zaman bu objeye bu ip blogundan bir sanal ip adresi atanir ve ismi ile bu ip adresi cluster in dns mekanizmasinda kaydedilir. Bu ip adresi virtual bir ip adresidir herhangi bir interface e map edilmez. Kube-proxy servisi tarafindan tum nodelardaki ip table lari eklenir ve buraya ulasan trafik servisin iliskilendirdigi podlara randomic bir siralama ile yonlendirilir. Yani cluster icerisinden bu ip adresine gonderilen paketler bu servisle iliskilendirilmis podlardan bir tanesine yonlendirilir. Bu bizim iki sikintimizi cozer. Az onbceki ornekteki gibi ben bu backend podlarla iliski kuracak bir backend isimli servis yaratir ve frontendlere de backend podlara ulasmak istediginde gotmek gereken adres backend isimli adrestir dersem su olur. Cluster icerisindeki herhangi bir pod beckedn ismini cozmek istediginde ona cevap olarak bu backend servisinin ip adresi doner. Pod o ip adresine ulasmak istediginde de trafik backend podlarindan bir tanesine yonlendirilir dolayisiyla ben tek tek frontendlere baglanarak her seferinde yeni eklenen podlarin ip adresi bu, eski podlarin ip adresleri bu diye guncelleme yapmam gerekmez. Ona sadece gitmen gereken yer backend derim o trafigi bu ip adresie yollar. gerisini cluster halleder. Yeni eklenen podlar otomatik olarak bu servise eklenir. Sistemden cikarilan podlar da otomatik olarak bu servisten cikarilir. yani cluster ip servisleri k8s altinda basit de olsa service discovery ve load balancing hizmetleri kuran obje tipidir. Simdi cluster ip sayesinde forntend backend baglanti sorununu cozduk. 
* Nodeport: Ilk sorunumuz neydi. podlarimiz web sitemizi barindiriyor dolayisiyla bunlarin cluster disindan erisilebiliyor olmasin lazim. Bu sorun da nodeport tipi servis objeleri ile cozeriz. Aynen cluster ip gibi node port da bir servis obje tipidir. Siz bir nodeport servisi yaratip bunu label selectorler araciligiyla podlarla eslestirebilirsiniz. Bu obje yaratildigi zaman su olur 30000-32767 araliginda bir port secilir ve tum worker node larda bu port sizin servisinizde proxylenir. Kisacasi worker node un ip adresininin bu portuna gelen tum paketler iliskilendirdiginiz podlarin public ettiginiz podlarina yonlendirilir. Sizler de bu worker nodelarin onune bir worker load balancer ya da reverse proxy sunucu koyar ve dis dunyadan erisilmesini saglarsaniz buraya ulasacak paketler worker node lara worker node lara ulasan paketler de podlarin birine ulasarak kullanicilarinizin uygulamaniza erismesini saglarsiniz. 
* Lod balancer: Bu tip objeleri sadece azure k8s servis gibi ya da google k8s engine gibi cluster saglayicilari manage k8s servislerinde kullanabiliriz. Bu obje olusturuldugunda cloud servis saglayici dis dunyadan erisilebilecek public bir ip adresine sahip bir load blancer olusturur. sonrasinda bu load balancer i sizin service objenizle iliskilendirir. Dolayisiyla u public adresine ulasan paketler iceriden podlara yonlendirilir. Nodeport icin dedim ya worker node larin onune bir load balancer ya da reverse proxy koyar oradan kullanicilara eristirirsiniz. Iste cloud servis saglayicilar bunu otomatik olarak yapiyorlar. Buna da load balancr tipi servis objesi diyoruz. 