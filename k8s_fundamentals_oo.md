# Kubernetes Yonetim Komponentleri

## CONTROL PLANE-MASTER NODE

* ### Kube-apiserver: Resepsiyon ornegi
kubernetes ile alaki um komponentlerin ve dis dunyadan kubernetes platformu ile iletisim kuran tum servislerin ortak giris noktasidir. Api server tum komponent ve node bilesenlerinin direk iletisim kurabildigi tek komponenttir. Tum iletisim api server uzerinden gerceklestirilir. Kube-apiserver k8s de kaynak olusturma islemlerinde api dogrulamasindan sorumludur. Kullanicilar kube ctl komut satiri istemcisi veya rest-api cagrilari araciligiyla api server ile iletisim kurabilirler. Kisacasi disaridan istek gerceklestirmek icin apiserver a ulasir authantication ve authorization islemlerini gercektlestirir ve kubernetese ulasirsiniz. Diger tum komponenetlerde isteklerini api server uzerinden gerceklestirir.

* ### Etcd: 
tum cluster verisi metadata bilgileri ve k8s de olusturulan tum objelerin bilgilerinin tutuldugu anahtar-deger “key-value” veri deposudur. Clusterin mevcut durumu ile ilgili bilgiyi uzerinden barindirir. Kisacasi k8s in calismasi icin ihtiyac duyulan tum bilgiler etcd uzerinde tutulur. Apiserver haric baska hicbir komponent etcd ile dogrudan haberlesemezler. Etcd ile iletisim kurmak istediklerinde bunu api-vserver uzerinden gerceklestirirler. 

* ### Kube-scheduler: ornekte uretim planlama. 
Yaratilmak istenen podlarin gereksinimlerini kontrol ederek o podun en uygun calisacagi worker node un hangisi oldugunu belirler ve o podun burada olusturulmasini saglar. Podun ozel gereksinimerini isteklerini, cpu gibi cesitli parametreleri elinde bulundurarak degerlendirir ve pod icin en uygun node un hangisi olduguna karar verir. 

* ### Kube-controller-manager: ornekte vardiya amiri
Tek bir binary olarak bulunsa da icinden birden fazla kontroller bulundurur. K8s istenilen durumla mevcut durum arasindaki karsilastirmada uygulama yonetimi saglar. Kube-controller altindaki controller managerlar k8s den istenilen durumla mevcut durum arasinda fark olup olmadigi gozlerler. Kube-api araciligiyla etcd de saklanan cluster durumunu inceler ve eger mevcut durum ile istenilen durum arasinda fark varsa iste bu farki olusturan kaynaklari gerektigi gibi olustururak, guncelleyerek veya silerek bu durumu esitler. Ornegin siz k8s e uygulamanizin 3 pod olarak calismanizi bildirdiniz. K8s de bunu gerceklestirdi ve uygulamanizin kostugu 3 pod calismaya basladi ama sonra bir tanesi silindi iste controller-manager bu podun yeniden olusturulmasini saglar. 

Control-plane yani yonetim kismi master node uzerinde calisir. Bu componentlerin hepsi tek bir linux isletim sistemine kurulabilecegi gibi birden fazla sisteme de kurulabilir. etcd tamamen bu yapidan ayri bir sekilde konuslandirilarak daha yuksek bir erisim saglanabilir iken ama genellikle etcd de api-server in kostugu yerde durur. ornegin 3 sunucu uzerinde 3 api-server kurulup diger componentler bu sistemlere dagitilabilir. Bu uzerinde control plane componentlerinin kostugu sistemlere cluster altinda master node deriz. Master nodelar sadece bu sistemleri barindirmak icin bulunur ve production ortaminda herhangi bir is yukumuzu bu sistemler uzerinde calistirmayiz. Bunlarda sadece cluster in yonetim altyapisini kostururuz. is yuklerimiz ise worker nodelar uzerinde calistirilir. Bunlar uzerinde bir container run time barindiran ve clustera dahil edilmis sistemledir. 

## WORKER NODE
***
her worker node da 3 temel component bulunur. ilk ve en onemli component containerlarin calismasini saglayacak bir container run time dir. Default olarak bu docker dir ancak k8s docker container run time destegini birakarak bazen containerd ye gecmistir.

* ### kubelet: ornekte ustabasi
her worker node da bulunur. Kubelet api-server araciligiyla etcd yi kontrole eder. Scheduler tarafindan bulundugu node uzerinde calismasi gereken bir pod belirtildi ise kubelet bu podu o sistemde yaratir. Conatinerd ye haber gonderir ve belirlenen ozelliklerde bir container in o sistemde calismasini saglar.

* ### kube-proxy: ornekte lojistik elemani
Olusturulan podlarin tcp, udp ve etcdp trafik akislarini yonetir. ag kurallarini yonetir. kisaca network proxy gorevi gorur.

Bunlarin disinda dns ya da gui hizmeti saglayan cesitli servisler entegre edilebilen servislerdir. fakat bunlar core componentler olarak adlandirilmaz

# Kubectl Kurulumu
***
Kubernetes e komut vermek icin bubu api-server uzerinden yapiyoruz. 
Bunun 3 yontemi var. 

* #### Rest cagrilari
Uygulamalarin ya da scriptlerin icinde kullanilir
* #### Gui istemcileri
Grafiksel arayuz ile iletisim kurmak. Resmi gui araci dashboard, oktant, land en bilinen araclardir. Gui istemciler en temel araclar degildir. 
* #### kubectl
Shell uzerinden apiserver a komutlar doderdigimiz k8s in resmi cli aracidir. k8s cluster olusturulmadan once ilk olarak kubectl yuklenmesi gerekir.

## kubectl config dosyasi
* kubectl araci baglanacagi kubernetes cluster bilgilerine config dosyalari araciligiyla erisir
* config dosyasinin icerisinde kubernetes cluster baglanti bilgilerini ve oraya baglanirken kullanmak istedigimiz kullanicilari belirtiriz
* Daha sonra bu baglanti bilgileri ve kullanicilari ve ek olarak namespace bilgileirni de olusturarak contextler yaratiriz
* kubectl varsayilan olarak $HOME/.kube/ altindaki config isimli dosyaya bakar
* kubectl varsayilan olarak $HOME/.kube/ altindaki config dosyasina bakar ama bunu KUBECONFIG environment variable degerini degistirerek guncelleyerebilirsiniz

# pod
***
* kubernetes icinde yaratilan en temel obje pod dur.
* kubernetesde olusturabileceginiz en kucuk birimlerdir
* podlar bir yada daha fazla container barindirabilirler ancak cogu duurmda pod tek bir container barindirir
* her bir pod un essiz bir idsi "uid" bulunur
* her pod essiz bir ip adresine sahiptir.
* ayni pod icersindeki containerlar ayni node ustunde calistirilir ve bu containerlar birbirleriyle localhost ustunden haberlesebilirler.
* kubectl ile kubea-api ile haberleserek k8s uzerinde pod yaratma islemi gerceklestirilir. api server bu poda bizim tanimladigimiz bilgileri atar ve bir pod yaratir ve etcd veri tabanina kaydedir. kube-scheduler componenti surekli burayi gozler ve herhangi bir worker node atamasi yapilmamis pod tanimi yapilmamissa o podun calismasi icin uygun bir worker node secer ve bu bilgiyi pod tanimina ekler. Sonrasinda worker node uzerinde calisan kubelet servisi de bu etcd yi surekli gozledigi icin bu pod tanimini gorur ve bu tanimda belirtilen container o worker node uzerinde olusturulur ve boylece pod olusturulma asamalari tamamlanir.

## Pod Yasam Dongusu
#### pending
pod yaratmak icin gonderilen yaml dosyasi kube-api tarafindan alinir ve varsayilan ayarlari da ekleyerek poda bir unique id atar ve tum bunlari etcd ye kaydeder. Bu asamadan sonra pod olusturulmustur ve bu asamadan sonra pending asamasina gecer. Eger bir podun statusu pending durumunda ise bunun anlami sudur: birisi bir pod olusturdu podla ilgili tanimlar yapildi ve veri tabanina kaydedildi ama pod herhangi bir node uzerinde olusturulmadi. 

#### creating
kube-scheduler api-server araciligiyla surekli etcd yi gozler. eger burada yeni yaratilmis ve herhangi bir node atamasi yapilmamis bir pod gorurse calismaya baslar. Algoritmasi ve secme kriterlerine gore podun calismasinin en uygun olacagi node u secer ve veri tabanindaki pod objesine node bilgisini ekler. Bu noktadan itibaren pod yasam dongusunde creating asamasina gecer. eger bir pod creating asamasina gecemiyorsa bu kube-scheduler in uygun bir node bulamadigi anlamina gelir. bunun pek cok nedeni olabilir: cluster da bu podun gereksinimlerini karsilayabilecek node bulunmuyor olabilir. Nodelar uzerinde cpu ve memory gibi kaynaklar tukenmis olabilir. 

#### ImagePullBackOff
Nodelar uzerinde kubelet adli bir servis calisir. bu servis ayni kube-scheduler gibi surekli etcd veri tabanini gozler ve bulundugu node a atanmis nodelari tespit eder ve hemen islemlere baslar. Ilk olarak pod taniminda olusturulacak containerlara bakar ve bu containerlarda olusturulacak image lari sisteme indirmeye baslar. Eger bir sekilde image indirilemezse pod erimagepull ve ardindan da imagepullbackoff asamasina gecer. Eger podun statusunda imagepullbackoff gorurseniz bu nodeun image i repository den cekemedigi ve bunu tekrar tekrar devam ettigi anlamina gelir. Bunun birkac nedeni olabilir: en sik karsilasilan neden pod tanimlanmasinda image isminin yanlis yazilmasidir. Image ismi yanlis yazildigi icin image cekilemeyecektir ya da image i cekmek icin sisteme kaydolmak gerektigi ve bu authentication islemlerinde hata yapilmasidir. Bu nedenle image cekilemeyecektir. Bu gibi bir hata olmamasi durumda ise

#### Runnig
Islemlerin sorunsuz bir sekilde ilerlemesi halinde kubelet node da bulunan container engine ile haberlesir ve ilgili containerlarin olusturulmasini saglar ve containerler running durumuna gecer. Bu noktadan itibaren artik pod olusturulmus olur.

* Containerlarla ilgili temel kural: container icindeki uygulama calistigi surece container calisir. Uygulama calismayi birakirsa container durdurulur. Uygulamanin da durdurulmasi 3 sekilde olur: 1) isi bittigi icin hata vermeden otomatik olarak kapanir. 2) Kullanicinin istegi uzerine komut gonderilir ve hata vermeden kapanir. 3) hata verir, sikinti cikar, hata kodu olusturarak kapanir. Ornegin nginx image uzerinden bir container yarattigimizi dusunelim. Bu uygulama bir deamon bir servis yani siz onu durdurana ya da hata cikip cokene kadar calismaya devam eder. O calistigi surece de container alismaya devam eder. Fakat diyelim ki siz bir image yarattiniz ve bu image in varsayiln uygulamasi da bir script ya da basit bir komut. container baslayinca bu script calisiyor isini yapiyor ve isi bitince kapaniyor yani container da kapaniyor yani illa ki hata vermeden kapanmasina gerek yok.

Bu gibi sorunlar nedeni ile containerlar icin restart policy tanimi yapilir. Restart policy 3 adet deger alabilir:
* Always: Default degerdir. Pod un icerindeki container her durumda durdurulursa durdurulsun her sartta yeniden baslat anlamina gelir. 
* On-failure: Sadece hata alip kapanirsa yeniden baslatilir.
* Never: Pod hicbir zaman yeniden baslatilmaz.

#### Succeed
Podun altindaki containerlar calismaya devam ettikce status running olarak devam eder. Eger containerlarin hepsi hata vermeden dogal olarak kapanirsa ve restart policy never veya onfailure olarak set edildi ise podun statusu succeed olarak statusune gecer ve pod yasam dongusunu tamamlar. Yani bir diger degisle completed, basarili olarak pod un yasam dongusu tamamlanir. 

#### Failed
Never ya da Onfailure olarak policy tanimlandi ve containerlardan biri hata verip kapandi ise bu sefer pod un statusu failed olarak isaretlenir ve yasam dongusunu boyle tamamlar. 

#### CrashLoopBackOff
fakat restart policy always olarak set edildi ise pod hicbir zaman succeed ya da failed durumuna gecmez. Bunun yerien podun icerisindeki container yeniden baslatilir ve running state de devam eder. Fakat kubernetes bu yeniden baslatma islemini belirli bir siklikla yapiyorsa bazi seylerin ters gittigine kanaat getirilir ve pod u CrashLoopBackOff adini verdigimiz bir state e sokar. Bunun anlami sen bir pod olusturdun ama bu pod un icerisindeki container ikide bir kapaniyor ama gene kapaniyor buna bir bak. Bu pod da sunlar olur: k8s container in icerisinde birseylerin ters gittigini anlar ve pod un statusunu crashloopbackoff a cevirir. Surekli restart etme yerine restart eder 10 sn bekler eger 10 sn icinde yeniden cokerse 20 sn bekler yeniden cokerse 40 sn bekler sonra 80 sn bekler ve bu durum 5dk lik araliga cikana kadar boyle devam eder ve ondan sonra her 5dk da bir tekrar eder. Bu arada container cokmeyi birakir ve 10dk sure ile sorunsuz bir sekilde calismaya devam ederse. Kubelet container i crashloopbackoff dan cikarir ve running e dondurur. eger bu olmaz ve siz mudahale etmezseniz bu durum sonsuza kadar boyle devam eder. Ozetle crashloopbackoff mudahaleyi gerektirir ve bakilmasi gerekmektedir. 

## Multicontainer pod
Bir frontedn ve bir de backend den olusan two tier yani cift katmanli bir uygulama yazdik. Cok populer olan orgenin wordpress uygulamasi. WordPress uygulamasini deploy etmek istedigimizi varsayalim. Bildigimiz uzere wordpress php tabanli bir frontend ve bu uygulamanin verilerinin tutuldugu bir mysql veri tabanina sahip. Bu uygualamyi container haline getirmek istiyorum. Bu uygulamayi icinde mysql ve word press halinde calisan tek bir container haline getirebilir miyim? Teknik olarak evet. Fakat bunu yapmiyoruz. Her iki uygulama icin de ayri iki container kullaniyoruz. BBunun iki nedeni var 1) container in teme mantigi izolasyon bu iki uygulamayi da ayni container icine koymak bu izolasyondan mahrum olmak demek. 2) Iki uygulamayi scale etmek istemeniz durumunda yatay buyume yapamiyorsunuz. Bunu soyle dusunun ben 2 sunucudan olusan bir ortam kurdum. Hem wordpress hem de mysql uygulmasini ayni container icine kurdum ve tek bir uygulama olarak calistirmaya basladim. Sayfama yogun bir istek oldugunda wordpress in frondend katmani bu isteklere cevaop verememeye basladi ben de yeniden bir container daha olusturarak load balancer arkasina almak ve kaynaklarimi cogaltmak istedim ki bu problemi cozebileyim. Ama bunu yaptigim zaamn ortamda 2 frontend 2 tane de mysql veri tabani olacak. Ancak benim veri tabanimda veri sikintisi yoktu. Ikisi de ayni container da oldugu icin bunu coklamam gerektiginde ikisini birden cokladim. Hatta 2. containeri deploy ettikten sonra mysql baglanti ayarlarini degistirdim ve 1. container icindeki mysql e baglanmasi icin ayar yaptm cunku simdiye kadar olusturdum her sey o veri tabaninda. Bunun 100 container a kadar scale edilen bir ortam oldugunu dusunun. Sirf 2 uygulamayi da ayni container icine gomdugum icin sikintilar yasadim. Bunun yerine bir mysql ve bir de wordpress olmak uzere 2 ayri imahge yaratsa idim bu sefer 1 tane mysql container yaratilirdi benim de istedigim kadar wordpress scale edebilme imkanim olusurdu. Bu nedenlerden dolayi en onemli best practise bir container icine 1 tane uygulama koymak. 
Kubernets icinde ise word press icin bir pod mysql icin bir pod olusturulmali. Fakat biz istersek bir pod icinde birden fazla container da koyabilirim ama bu da bir container icine iki uygulama koymak ile ayni sey cunku k8s de scale ettigimiz sey pod. Her container da tek bir uyguama her pod da bir container. Peki neden kubernetes ayni pod icinde birden fazla container calismasina izin veriyor? Diyelim ki ben wordpress php uygulamasinin uygulama ici performans degerlerinin merkezi bir yerde toplayarak analiz etmek istiyorum. Bunu yapabilecek bir uygulamyi deploy etmek istiyoruz. Bunu k8s de nasil yapabiliriz? Oncelikle mysql podumuzu ayaga kaldirdik sonrasinda wordpress uygulamamiz agaya kalkti son olarak da yeni uygulamanin ayaga kalkacagi podu yarattim. Son uygulamanin tek bir amaci var wordpress e baglanacak ve uygulamayi analiz edecek veriyi toplayacak yani bu son uygulama wordpress e bagimli. Bu uygulama wordpress e bagimli. Bu uygulama wordpress hangi worker node da olusturuluyorsa orda olustrulmali. Wordpress calismaya basladigi zaman calismali kapandigi zaman kapanmali yani tamamen ona bagimli. yani ben 2. bir wordpres uygulamayi yaratirken 2. bir uygulama daha deploy etmem gerekecek.3. bir wordpress uygulamasi icin 3. log kodunu olusturuacgim. Wordpress i silerken bu uygulamadan bilgi toplatan podu da silmmem gerekecek. Bu cok zaman alan bir islem surekli iki is yapmam gerekiyor. Diger bir sikinti ise diyelim ki benim metric toplayan uygulamam word press uygulamasi ile stirage seviyesinde haberlesmesi ortak dosyalari yazmasi gerekiyor ancak 2 podun ayni local volume e baglanabilmesi icin 2 pod un ayni worker node uzerinde calismasi gerekiyor. diyelim ki ben wordpress podunun olusturdugumda kubescheduler bunu o an en uygulan olan node1 da olusturdu. sonrasinda analiz verisi toplayan uygulama olusturmak istedim ve kubescheduler bunu node2 uzerinde olsuturdu. Bu uygulamalar stroge seviyesinde birbirlerine ulasamayacaklar. Bu da ayri bir sorun. K8s bu sorunlari ortadan kaldirmak icin ayni pod icerisinde 2 container calistirma imkani sagliyor. Birlikte scale edilmesi gereken, birbirleriyle network ve storage sebiyesinde haberlesmesi gereken uygulamalari ayni pod icerisinde ayri ayri containerlar olarak calistirabiliyoruz. Buna terminolojide sightcar container denmektedir. 

## init container
init container da bir pod icerisinde birde fazla pod yaratilmasina imkan verir. Fakat app containerlardan farkli olarak init containerlar pod un yasam dongusu boyunca calismaz. Siz bir pod tanimina init container tanimi koydugunuz zaman pod olusturuldugu zaman bu init container calistirilir. Init container calisir icindeki uygulama ne yapacaksa onu yapar ve ardindan kapanir bu init containerlar islerini tamamlayip kapanana kadar da app containerlar calismaya baslamaz. Peki neden boyle bir seye ihtiyac duyariz ve kullanim alani nedir? Init containerlar esas uygulamamiz calismadan once tamamlamamiz gereken seyler var ve bunlari tamamlamadan esas uygulamayi baslatmak mantikli degilse kullanmak gerekir. Mesela uygulamamizin bagimli oldugu baska bir uygulama ya da servis var. Eger bu ayakta ve hazir degilken uygulamayi baslatirsak uygulamada sikinti cikiyor. Bu durumda pod tanimina init container tanimi ekler ve bu init containerin bu servisi gozlemesi icin ayar yapariz. Init container icinde bir uygulama calistirilir ve bu servisten okey alana kadar calisacak okey aldiktan sonra da kapanacak sekilde ayar yapariz. Bu sayede pod olusturuldugu zaman ilk olarak init container calisir. Diger servisi beklemeye baslar. Diger servis hazir oldugu anda uygulama kapanir ve init container da kapatilir. Init container kapatildigi anda da esas uygulamamizin calistigi container ayaga kalkar ve boylece servisimiz hazir olana kadar uygulama beklemis olur. Soyle bir senaryo dusunun ana uygulamamizin ihtiyaci olan bazi config dosyalarinin guncel halinin sisteme cekilmesi gerekiyor iste bu cekme islemini de init container ile halleder ve ana uygulama baslamadan bunlari sisteme indirebiliriz. 

## label and selector
label yani tagler k8s de her turlu objeye atayabildiginiz anahtar deger eslenikleridir. Etiketler sayesinde olusturdugumuz objelere bizlerin anlayacagi ve gruplara yaparken kullanabilecegimiz bilgiler eklemis oluruz. Bu sayede k8s tarafindan bizlere core ozellik olarak sunulmayan objelere belirli bir aidiyet atanmasi islemini gerceklestirebiliriz. ornegin k8s de 5 ekip tarafindan olusturulan 100 den fazla pod calistiriyoruz. Hangi pod un hangi ekibe ait oldugunuz belirtmek istersek k8s in bize belirtmis oldugu bir hiyerarsi mekanizmasi yok. ama labels sayesinda bunu saglayabiliyoruz. ornegin her ekip kendi olusturdugu objeye team:teamname seklinde etiket atarsa bizler bu etiketler sayesinde hangi ekibin hangi objelere sahip oldugunu listeleme imkanina kavusuruz. Ya da uygulamamizin frontend katmanini olusturan objelere tier:front-end backend katmanini olusturan objelere ise tier:backend etiketini atayarak frontend ve backend katmanlarini ayri ayri listeleme imkanina kavusuruz. 
Etiketler obje olusturulurken atanacagi gibi olustuktan sonra da atanip silinebilir. Etiketler anahtar veri degerleri seklinde olusturulur. Etiketler bir dns subdomanin seklinde atanan opsiyonel bir on ek kismi bulundurabilir example.com/tier:frontened gibi. ama bu kisim zorunlu degildir. Bu kisimdan sonra ana kisim yazilir bu iki kisim anahtar ve deger kismindan olusur. Bu anahtar ve deger kismi en fazla 63 karekter olabilir. Alfanumerik bir karakterle baslamali ve bitmelidir. Tire alt cizgi noktolar ve arasinda numerik degerler icerebilir ancak bunlarla baslayamazlar.
Etiketlerin bir onemli gorevi de k8s de objeler arasindaki baglanti da labelslar araciligiyla kurulur. Servisler ve deployment objeleri hangi podlar ile iliski kuracagini labellar sayesinde belirlerler. ornegin bir servis yaratir ve ona git tier:frontend labelli podu bul ve servisi bu poda yonlendir.
yaml dosyalarinda selector araciligiyla deploymenta senin yonetecgein podlar app:frontend label ina sahip olacaklar. Bunu gorursen anla ki bu podlar senin podlarin. yaml dosyasi icindeki template alindaki labels kismian da bu app:frontend i ekliyorum ki deploymen hangi pod u sececegini anlayabilsin. 
Labels ve selector kismi 3 acidan onemli
* Ilk olarak bu bir zorunlu alan. Yani deployment da en az bir selector tanimi olmali ve ayni labellar template kisminda da bulunmali.
* egerki birden fazla deployment objesi olusturacaksaniz ki production ortaminda bu olacak siz her deployment objesinde farkli label ve selector kullanilmali. ayni labellar kullanilmasi sikinti cikarir.
* ayni labellari kullandiginiz baska objeleri yaratirsaniz misal sibgleton bir pod yaratirsaniz bu da sikinti cikarir. 


## Annotation
Aynen labellar gibi anahtar veri opsiyonu ekleyebilecegimiz 2. secenek ise annotations. Labellar k8s icin cok onemli oldugu icin label eklemek veya cikarmak clusterda birseyleri tetikleyebilir. dolayisiyla her bilgiyi label olarak metadataya ekleyemeyebiliriz. Ancak bu bilgiler gerekli olabilir. Mesela podun ne zaman ve kim tarafindan olusturuldugu bilgisini eklemek istiyorum. Bu bilginin label olarak eklenmesi dogru degil, yani bu bilgiyi label olarak secme istegim yok veya bu bilgiyi herhangi bir secme istegi olarak kullanmak da istemiyorum. Dolayisiyla bunu annotationa eklemek daha dogru olacaktir. Ayrica annotation k8s in ana componenti olmayan fakat k8s ile baglantisi olan yazilimlar tarafindan ihtiyac duyulan bilgilerin de yazildigi yerdir. Ornegin firmamizin destek ekipleri tarafindan kullanilan bir cagri kabul uygulamasi var. Bu yaziimi k8s den bilgi cekebilecek hale getirdik ve soyle bir sistem kurmak istiyoruz: herhangi bir k8s objesinde bir sikinti ciktigi zaman bu cagri yazilimi bunu tespit etsin ve o objenin destek sorumlusu olarak belirlenen insana mail gondersin. Bu durumda o mail adresi bilgisini objenin metadatasina annotation olarak ekleyebiliriz ve destek yazilimi bu bilgiyi cekebilir. bu sayede de bu insana mail gonderebilir. Annotation bu ve benzeri durumlar icin kullanilir. Olusturulma kurallari labellarla hemen hemen aynidir. DNS subdomain olarak eklenebilecek ve zorunlu olmayan bir prefix ile baslayabilen, sonrasindala labellar gibi 63 karakteri gecmeyecek, nokta tire ve alt tire icerebilen ama bunlarla baslayamayan alfanumerik degerler icerebilir. value kisminda ise bu kurallar gecerli degildir. Alfanumerik olmayan karakter de alabilir. 

## Namespace
Soyle bir senaryo kurun bi formada calsiyoruz ve tum calisanlarin ortak calistiklari dosyalari barindirabilecekleri bir yapi tasarliyoruz. 10 ayri ekibimiz var ve bu 10 ayi ekibin de erisip dosya barindirabilecegi bir latyapi kurmak istiyoruz. Bu sistemi nasil kurariz: bir file server yaratirim ve bunun altinda tum ekibin erisebilecegi bir paylasim alani yaratirim. Sonra gider tek tek tum kullanicilarin bilgisayarlarinin buraya erismesini saglayabilirim. Baslangicta sorunsuz olabilir ancak birden fazla ekibin olmasi durumunda soru olabilir. Bu dosya altinda 100lerce dosyanin olmasi durumunda bu belli bir zaman sonra yonetilemez hal alabilir. Bir diger sikinti da su olabilir diyelim ki ben a.txt adinda bir dosya olusturdum. baska bir arkadasim da a.txt adinda bir dosya olusturup ayni yere atamaz. Bir baska sikinti da su olabilir. diyelim ki sadece hr calisanlarinin gormesi gereken dosyalari da burada tutuyorum. bu dosyalarin sadece hr tarafindan gorulmesini saglamak icin dosya bazinda ayarlamalar yapmam gerekir. her dosya upload edildigide de bu islemleri tekrarlamam gerekir. Son olarak da su sikintiyi yasariz. Herkes tek bir alana dosya kaydettigi icin ekip bazli kota ayarlamasi yapmam zor olur. yani hr kismina 20gb alan ayirayim it kismina 100gb alan ayirayim gibi ayarlamalar yapamam. Cozum belirli bir zaman sonra yeterli olmamaya basladi.
Her ekip icin klasor altinda ayri bir alan yaratirsak tum bu problemleri ortadan kaldiririz. Her ekibe ozel bir klasor olusturulur. Her ekip kendi dosyalarini kendi klasorlerindde tutabilirler. Ayrica it ekibi gibi ekiplere ozel proje bazli klasorler de yaratiriz. Ornegin yeni bir urun gelistirmesi yaratiriz ve bu urune ait gelistirme dosyalarini bu klasorde tutariz bu sayede tum guvenlik ayarlarini klasor bazda halledebiliriz. hr klasorune sadece hr calisanlari erissin it kalsorune sadece it calisanlari erissin, proje klasorunde proje yoneticisinin yazma izni olsun ama stayjer sadece dosyalari okuyabilsin diyebiliriz. Boylece guvenlik ve gruplama ile pek cok sorunun ustesinden geldik. Isim cakismasini engelledik. son olarak da klasor bazinda kota ayarlamasi yaparak kaynak kisitlamasi saglabiliyoruz. Namespace tam anlami ile bu ise yarar. 
Kubernetes cluster i bu ornekteki fileserver olarak dusunursek namespace ler de burada yarattigimiz klasorlerdir. k8s de objeleri sanal klasorler altinda olusturabilir ve bu sanal klasorler altinda olusturabilir ve sonrasinda bu sanal klasor bazinda erisim izni ve kota ayarlamasi yapabiliriz.
Her k8s clusterinda varsayilan olarak 4 namespace olusturulur.
* kube-sysem: k8s tarafindan olusturulan objelerin tutuldugu namespace dir. 
* kube-public: kimligi dogulanmamis olanlar da dahil tum kullanicilar tarafindan erisilmesine ihtiyac duyulan objelerin olusturulabilecegi yerdir. 
* kube-node-lease: node hard disk islemleri icin olusturulan ozel bir namespacedir.
Kisacasi kube ile baslayan tum namespaceler kubernetes tarafindan olusturulur ve clusterin isleyisi ile alakali objelerin tutuldugu yerlerdir. Bunlarin disinda default namespace adinda baska bir namespace daha olusturulur ve adindan da anlasilacagi uzere bizler aksini belirtmedigimiz surece objeler burada olusturulur.
Bizler herhangi bir namespace tanimi yapmadan objelerimizi bir tek namespace altinda toplamayiz. Tek bir ekip tarafindan yonetilen kucuk bir kubernetes clusterimiz var ise boyle de devam ederiz. Fakat ne zaman buyur ve birden fazla ekip tarafindan yonetilen ve birden fazla ekip tarafindan deploy edildigi bir k8s cluster a evrilirsek iste o zaman namespaceler bize kota ayarlamasi ve namespace bazinda kullanici erisimi verebilme gibi ozellikleri sayesinde yardimci olacaktir. 

## deployment
En temel obje poddur. ama biz genelde tekil yonetilmeyen podlar yaratmayiz. podlari yoneten ust seviye objeler yaratiriz ve podlar bu objeler tarafindan yaratilir ve yonetilir. 
neden tekil podlar yaratmiyoruz?
Deployment buan nasil cozum bulur?
Bir uygulamamizi production ortaminda deploy ettigimizi varsayalim. Uygulamamizi container image i haine getirdik ve bunu kubernetes de deploy etmek icin bir pod tanimi yarattik kubectl apply ile bunu api sever a gonderdik. 3 worker node lu bir yapimiz var ve kube-scheduler uygun bir worker node secerek podu o node uzerinde olusturdu. fakat bir sure sonra bu podun olusturuldugu worker node bozuldu ve erisilemiyor. dolayisiyla pod a da erisemiyoruz. bu durumda pod terminating durumuna gecer ve oylece kalir ve tabii ki de pod calismadigi icin uygulamamiz da calismamaya baslar. Siz bir pod yarattiginiz zaman kube-scheduler bu poda uygun bir node bulur ve onun uzerinde bu pod u calistirir. eger container seviyesinde bir sikinti cikarsa ve restart policy onfailure ya da always olarak secilirse container restart edilir ve sorun cosulur. ama pod un schedule edildigi node da bir sikinti cikar ya da kaynak sikintisi nedeni ile o pod durdurulursa kube-scheduler devree girip aa su pod calismiyormus deyip baska bir node uzerinde onu schedule edeyim demez. Bu nedenle worker node umuz gittigi icin de pod umuz terminate state e duser ve oylece kalir. Buna cozum olarak da 3 ayri podun 3 ayri node uzerinde calismasini node selector tanimlari ile sagladik. Kisacasi her bir worker node da bir tane olmak uzere 3 tane pod yarattik, onlerine de bir load balancer koyduk. Boylece bir tanesine bir sey olursa diger 2 tanesine erisilebilmesini sagladik. Ancak diyelim ki ben bu uygulamayi gelistirmeye devam ettim, yeni bir versiyon cikardim, yeniden bir container image i yarattim. Simdi bu 3 podu bu yeni image la yeniden olusturmak istiyorum. Bunu nasil yapmam gerekir tek tek butum yaml dosyalarindaki imgae kismini degistirmem gerekiyor. tekrar kubectl apply yaptim ve 3 podu olusturum isler boyle zahmetli oluyor. eskisine donmek istedigimde veya label olusturmak istedigimde isler kontrolden cikiyor.
Deloyment bir veya birden fazla pod u bizim belirledigimiz desire state e gore olusturan ve sonrasinda bu desire state i yani istenilen durumu mevcut durumla surekli karsilastirip gerekli duzeltmeleri yapan bir obje turudur. Bizler bir deployment objesi olusturmak icin bir tanim yapar ve bu tanim icinde olusturmak istedigimiz pod un hangi ozellliklere sahip olacagini ve kac adet olusturmak istedigimizi belirtiriz. Bu deployment objesi olusturuldugu zaman bu tanim ve adette pod olusturulur. Ornegin nginx image indan 3 tane pod olusturan bir deployment yaratiriz. Deployment bunu desire state olarak alir ve bu 3 pod olusturulur ve deployment controller devreye girer. Ornegin bu podlardan birini sildim. Deployment kontroller desire state ile current state i karsilastirir ve sonrasinda esitler. bunun yaninda deployment objelesi bize kurallar sayesinde podlarda guncelleme yapmamiza imkan tanir. Onceki ornektei gibi uygulamanin yeni versiyonunu yazip bundan yeni bir image olusturmustuk. Ama bunu deploy etmek istedigimiz zaman tek tek pod taminlarinda bu isleri manuel yapmistik. Deployment da ise sadece desire state tanimimizla image kismini guncelleyerek bu isi halledebiliriz. Deployment desired state tanimini alir ve tek tek buna gore podlari olusturmaya baslar. Hatta bunu kontrollu bir sekilde yapmasi icin ek parametlereler belirleme sansini da verir. Yani yeni image bu podlari guncelle ama birden butun podlari silip yenilerini olusturmaya calisma. Bir tane sil sonra yenisini olustur 30 sn bekle ikincisini sil seklinde rollout u kontrollu bir sekilde yapmamizi saglayabilir. Boylelikle uygulamanin yeni versiyonu deploy ederken kesintisiz gecis imkani saglar. Bunun yaninda sikinti cikan durumlarda eskiye donmeyi de kolay bir sekilde yapabilmemize imkan saglar. Yani gunceller ama birseylerin yanlis gittigini de gorursek eski haline dondurmeyi tek bir komutla halledebiliriz. Bizler aslinda k8s uzerinde her ne kadar pod olustursak da bu podlari yalnizca pod olarak olusturmayiz. olusturulmak istenen objeleri daha ust seviyede objeler ile olustururuz ve bu sayede uzun vadede yapacagimiz islemleri otomatize etmis oluruz. Deploymentlar bunlarin icinde en sik kullanilandir hatta is yuklerimizin tamami deployment objeleri halinde deploy edilir. 
Best practice olarak yek bir pod bile yaratacak olsaniz deployment ile yaratmak gerekir.

## replicaset
Bir replicasetin amaci herhangi bir zamanda calisan kararli bir replika pod setini surdurmektir. bu nedenle, genellikle belirli sayida ozdes pod un kullanilabilirligini garanti etmek icin kullanilir. Deployment bizim istedigimiz ozelliklerde pod olusturmaz. Replicaset bizim istedigimiz ozelliklerde replicaset objesi olusturur ve podlar bu replicaset objeis tarafindaj olsturulur. 
K8s ilk ciktiginda replicaset-controller adinda bir objesi vardi halen var ancak kullanilmiyor. Replication controller biden fazla ayni tipte pod olusturmak icin kullanilirdu fakat deploy ettigi podlarla ilgili degisiklik yapmak istedigimiz zaman bazi sikintilar cikariyorudu. Bu sikintilari cozmek icin de soyle bir yola gidildi. Bu replication controller in sagladigi ozellikler deployment ve replicaset adinda 2 objeye bolundu. replicaset objesinin temel gorevi su oldu: belirledigimiz ozelliklere gore belirledigimiz sayida pod olusturmak ve bunun desired state de kalmasini saglamak. deployment ise bunun bir ust sebiye objesi olarak dizayn edildi ve pod taniminda bir guncelleme yaparsak bu guncellemenin belirledigimiz kurallara ve sirayla uygulanmasini saglamak oldu. ozetle biz bir deployment objesi olusturdugumuz zaman bu kendi yonettigi bir deploymet objesi olusturur ve bu replicaset objesi de podlari yaratir ve yonetir. Bir deployment taniminda bir degisiklik yaparsak ornegin kullanilan image i guncellersek deployment bu yeni tanimla yeni bir replicaset objesi daha yaratir. Ilk yaratilan replicaset objesi yavas yavas kendi olusturdugu podlari silmeye baslar ve yeni replicaset de yeni podlari yaratir. Burada silme ve yaratma isleminin neye gore olacagini bizler belirleyebiliriz. bu bize herhangi bir kesinti olmadan uygulama guncelleme ve yeni versiyon gecisi yapma imkani saglar.
Replicaste ile deployment yaml ayni arasidaki tek var kind kisminda birisinde replicaset yazarken digerinde deployment yazar. Geri kalan tum satirlar ayni kalmaktadir.

## Rollout and Rollout
Deployment yaml dosyalarinda spec altinda strategy anahtari ile bizler bu deploymenti guncelledigimiz zaman rollout islemlerinin nasil yapilacagini belirleriz. 
Kullanabilecegimiz 2 tip rollout stratejisi mevcuttur. 
* Recreate: Bu deploymentta bir degisiklik yaparsam oncelikle bu deploymet daki tum podlari sil ve bu islem tamamlandiktan sonra yeni podlari olustur. Bu stratejiyi genelde hardcore migration yaptimizda kullaniriz. Ornegin uygulamamizin yeni versiyonu ile eski versiyonunun kisa bir sure icin bile olsa bir arada calismasinin sakincali olacaksa, major degisikliklerde recreate stratejisi secilerek oncelikle tum eski verisyonlarin sistemden kaldirilmasi ve sonrasinda yeni podlarin yaratilmasi istenebilir.
* Rollingupdate: Default stratejidir. Herhnagi strateji belirtmez iseniz bu strateji uygulanir. Recrate in tam tersidir. Degisikligi asamali olarak yapar. Bu islemin de 2 farkli opsiyonu vardir. Bunlar maxUnavailable ve maxSurge dir. maxUnavailable: ben deploymentda bir degisiklik yaptigim zaman en fazla burada belirledigim sayi kadar podu sil. MaxUnavailable: 2 su demek: Yani ornegin 10 tane poddan olusturulan bir deployment da bir guncelleme yapar isem bu guncellemeye baslandigi anda en fazla 2 tanesini siliyor. Sonra yeni podlari olusturuyor onlar olusturulduktan sonra devam ediyor yani bir nevi gecis sirasinda en fazla kac pod un silinebilecegi bilgisini giriyorum. Burada sayi yerine yuzde de girilebilir. maxSurge: gecis sirasinda toplam pod sayisinin en fazla kac pod olabilecegini belirler. soyle bir sey dusunun 10 podlu bir deployment i guncelledigim zaman ne olacak kubernetes sirayla 10 tane eski podu silecek ve 10 tane de yeni pod ayaga kaldiracak ve bunu asama asama yapacak yani ayni anda sistemde hem yeni hem de eski podlar olacak. Iste bu durumda toplamda sistemde en fazla kac pod olabilir sayisini maxSurge ile belirliyoruz. Bu ornekte 2 secilmis bu su demek oluyor bizim desired state imiz 10 pod ama gecis sirasinda 12 poda kadar calisabilir. Yani olacak olan su ben bu deployment i olusturdum, sonra update ettim, k8s oncelikle yeni bir replicaset olusturacak ve yeni tanimda 2 pod ayaga kalkacak. Doalyisiyla eski 10 pod + 2 yeni pod toplam 12 pod ayakta olacak. Sonrasinda eski replicaset 2 tane podu silecek. Toplam pod sayisi tekrar 10 a dusecek. Yeni replicaset yeni 2 pod olusturacak toplam sayi 12 ye cikacak sonra eski replicaset tekrar silecek sonra yeni replicaset tekrar olusturacak tek tek toplam sayi 12 yi gecmeden ve 8 in altina da dusmeden islemler devam edecek ve sonunda eski replicaset 0 a yeni replicaset de 10 a cikacak bu sayede sistemde hicbir kesinti yapmadan gecis yapmis olacagiz.
maxUnavailable ve macSurge degerlerinin default degeri yuzde 25 dir.
Rollback ise deployment da yapilan degisikllerin geri alinmasidir.
Rollout komutunda 3 opsiyon mevcuttur:
* Rollout status: Bu opsiyonu ile deployment imizda yapilan degisiklikleri canli olarak izleme imkani elde ederiz. 
* Rollout pause: Bir rollout un ortasinda bu komut ile mevcut komut durduruluyor.
* Rollout resume: Durdurmus oldugum deployment i yeniden devam ettirmek istersem resume komutu ile devam ettiriyorum. 

## Network
Network temel kurallari
* Kubernetes kurulumunda podlara ip dagitilmasi icin bir ip adres araligi ya da kubernetes terminolojisinda bilinen adiyla --pod-network-cidr belirlenir.
* Kubernetes'de her pod bu cidr blogundan atanacak bir unique ip adresine sahip olur.
* Ayni cluster icerisindeki tum podlar varsayilan olarak birbirleriyle herhangi bir kisitlama olmadan ve NAT yani network address translation olmadan haberlesebilirler.

Diyelim ki elimde 4 tane sunucum var ve bunlardan kubernetes cluster olusturmak istiyorum. Evdeki internet baglantimi saglayan bir routerim var ve bu routerin da ic networku 192.168.1.0/24 networku.  router in ic bacagi da 192.168.1.1. ip adresine sahip. buraya bir switch bagladim ve router ve makinalari da bu switch e bagladim. Makinalara 192.168.1 networkundan ip adresi atadim ve hepsinin default gateway leri router i goruyor. Bu makinalar hepsi ayni network uzerinde ve dolayisiyla birbirleriyle sikintisiz bir sekilde haberlesebiliyorlar. Default gateway olarak da router olarak ayarli oldugu icin de dis dunyaya gidecek tum trafigi bu router a yonlendiriyorlar ve onun ustunden gitmek istedikleri yere gidebiliyorlar. Bu makinalarin hepsine linux isletim sistemlerini de yukledim. Simdi sirada k8s kurulumu var. Tum makinalarda k8s kurulumu icin gerekli on hazirliklari yaptim ve master node a gecerek k8s kurulumunu baslatacagim. oncelikle podlar icin bir ip adres araligi belirlemem gerekiyor yani --pod-network-cidr. Ben de bu network disindan bir ip adres araligi belirledim ve 10.10.0.0/16 networkunu sectim. k8s kurulumuna --pod-network-cidr=10.10.0.0/16 opsiyonunu girerek basladim, master node ayaga kalkti. Sonra gittim tek tek worker node lari da cluster a dahil ettim ve k8s kurulumum tamamlandi. Bu kurulumu tamamladigimizda worker node larda su islemler olur: ilk olarak her bir worker node uzerinde bir adet virtual bridge yaratilir. Bu virtual bridge bizi baslangicta belirledigimiz pod-cidr araliginda iletisim kuracak seklinde ayarlanir. Sonrasinda yeni bir pod yarattigimiz zaman o pod icin linux kernelinda ayri bir network namespace olusturulur ve bu pod un sanal ethernet sifir interface ini bu pod cidr adres blogundan bir ip adresi atanarak bulundugu host ustundeki bridge e balganir. 6 adet pod yarattim ve bu podlarin hepsi cidr blogundan ip adresi almis durumda. Bu asamada network trafiginin nasil isledigini dusunelim bununla ilgili 3 ayri senorya yapilacak:
1. senaryoda podlar dis dunyadaki bri adrese gitmek istediginde network akisi nasil oluyor bunu pod a uzerinden inceleyecegiz. Diyelim ki bu senaryodaki pod-a dis yunyadaki 1.1.1.1 ip adresli bir sunucu  ile iletisim kurmak istedi. Bu durumda pod-a iletisim paketlerini bridge e teslim edecek paket burdan worker node un ethernet sifir internet face ine ulasacak peket burada nat lanaack sonrasinda paket worker node un default gateway i olan rooter a gidecek. Rooter da yeniden natlacak oradan da 1.1.1.1 ip adresli sunucuya ulasacak. Bu sunucudan gelecek paket de yeniden natlacak ve pod-a ya ulasacak. Dolayisiyla pod-a uzerinde calistigi worker node un ulasabildigi her destination a ulasabilecek. 
2. senaryoda yine pod-a yi ele alalim ve bu sefer pod-a nin ayni worker node da bulunan pod-b ile iletisim kurmak istedigini varsayalim. Gordugunuz gibi her iki pod da ayni worker node uzerinde bulunan ayni bridge e bagli ve ayni ip adres blogundan adrese sahipler. Dolayisiyla pod-a paketi bridge e teslim edecek bridge den de pod-b ye gidecek. pod-b nin cevabi da bbu yolun aynisi olacak. Ayni network deki 2 makina gibi sorunsuz ve natsiz birbirleriyle haberlesebilecekler. 
3. Bu senaryoda pod-a baska bir worker node da deploy edilmis bir pod ile iletisim kurmak istiyor. Ornegin worker2 node uzerindeki pod-c ile. Kurallarda ne demistik podlar birbirleriyle nat (network adress translation) a ihtiyac duymadan birbirleriyle haberlesebilmeli. Peki kural buysa pod-a pod-c ile nat olmadan nasil haberlesecek? Pod-a pod-c ye ulasmak istiyor paketi bridge e teslim etti. ancak bridge de pod-c nin ip adresi tanimli degil. Mecburen trafigi ethernet sifir a gonderecek. Ethernet sifir bakacak destination 10.10.20.2 bilmedigi bir adres. Kendi ip adresi blogu disinda, routing tanimli degil. napacak mecburen natlayip router a teslim edecek. Orada da paket kaybi olacak. Cunku router da peketi nereye gonderecegi ile ilgili bir bilgi yok. Bu senaryoda ayri worker nodelarda kosan podlar birbirleriyle haberlesemiyorlar. Bunu cozecek birkac teknoloji var. Mesele vxlan teknolojisini kullanarak 10.10.0.0/16 network unu node larin bagli oldugu 192.168.1.0 network u uzerinde kosacak bir overlay network u olarak tanimlayarak nat a ihtiyac duymadan trafik akisi saglayabiliriz. Ya da podlara 198.162.1.0 networkundan ip verecek sekilde tanim yapip worker node un ethernet sifirlarina bu ip adreslerini de tanimlayip trafigi nat siz route edebiliriz. Bu ve benzeri bizim network altyapimiza uygun olacak bir cozum bulabiliriz. Ancak tum bu cozumleri k8s altinda saglamak neredeyse imkansiz. Bu nedenle k8s kendi icinde bir network cozumu ile gelmez bunun yerine contaner run timelar veya kubernetes gibi orchestratorlar ortaya ciktiklarinda hepsi bu ve benzeri network sorunlarini adreslemek icin cesitli yontemler denediler ve uyguladilar fakat bir sure sonra bu ayri ayri yontemler belirli bir sure sonra karmasa yaratti. Bu karmasayi cozmek adina Cloud Native Computing Foundation (CNCF) tarafindan tum bu linux container alt yapisinda ip dagitimi bridge tanimlari vxlan tanimlari ve diger ayarlarin standartlarini belirleyebilecek container Container Netork Interface (CNI) denilen bir proje baslatildi. CNI bu container netork altyapisinin naisl olmasi gerektigi ile ilgili standartlar yayinladi ve developerlarin bu standarda uygun networking cozumleri olusturabilmesine imkan sagladi. Bu sayede bu standartlara uygun bir sekilde network plugin leri yazilma imkani dogdu. K8s de bu projenin bir tarafi olarak sunu dedi bakin ben k8s de bu pod networkunu kendim cozmeyecegim CNI standardini benimsedim. Siz kubernetes clusterinizda network islerinizi bu standarda uygun olarak yazilmis ve sizin network cozumunuze uygun olacak bir plugin secin. Yani k8s  bu isleri kendi uzerinden atti ve sizin k8s i deploy ettiginiz ortama uygun CNI network plugini secerek tum bu altyapiyi bunun tanimlamasina izin verdi.
K8s kurulumunda ortamimiza uygun bir CNI plugin i secmemiz sarttir. Kullanabilecegimiz pek cok open source plugi vardir ve hepsinin degisik ozellikleri vardir. Mesela Azure uzerinden yonetilen Aks kullansa idik Azure kendi yazdigi aks plugin ini kullanarak her pod un vnetten direk bir ip adresi almasini saglar ve bu iletisimi bu sekilde cozer. Ya da cisco router lardan olusan bir network altyapisi kullaniyor olsa idik cisco nun aci plugin ini kullanip bunun bu isi halletmesini saglayabilirdi. Tum bu cni plugin dunyasi ortama uygun olarak secilmelidir. Bunlarin 2 tanesi on plana cikti. Birincisi flaner ve calico dur. k8s kurulumunu tamamladiktan sonra cni plugin ini de yukleyerek k8s in network yonetimini bu plug ine devretmesini saglayabiliyoruz. Ornegin biz bu ortamda calico cni plugin yuklersek podlarin ip adresinin atanmasi ip table kurallarinin duzenlenmesi ve node lar arasinda bu cidr blogunun nasil calismasi icin overlay tanimlarinin yapilarak vxlan interfacelarinin olusturulmasi dahil tum gorevler calico tarafindan hallediliyor. k8s de bir pod olusturdugunuz zaman scheduler bunun calisacagi node u secer ve kubelet burada devreye girerek podu olusturur. Pod icin tanacak ip adresinin belirlenmesi bunun poda atanmasi ve altyapida ayarlanmasi gereken tum islemlerin halledilmesi ise calico tarafindan saglanir. Boylece pod lar farkli node larda olsalar bile birbirleriyle nat olmadan haberlesebilirler. k8s de ingress trafigi bu sekilde gerceklesiyor.

## service
Bir dizi pod yzerinde calisan bir uygulamayi ag hizmeti olarak gostermenin soyut bir yolu.
Bir web sitesi olusturacagimizi hayal edelim. Insanlar bu web sitesine baglanacak ve resim yukleyecek. Yukledikleri resimlere bir filtre ekleyecegiz ve gero dondurecegiz. Biz bu uygulamayi 2 ayri servis olarak tasarladik. Birinci servis front end adinda olacak ve kullanicilar buraya baglanacak. Bu kismi java script ile yazdik. 2. kisim ise bu front end in baglanarak resimleri gonderdigi ve b resimlere filtre ekleyen yazilim olacak. Bunu da .net de yazdik. Her iki kismi da container image lari haline getirdik, artik k8s uzerinde bunu deploy edebiliriz. 2 tane deployment yaml dosyasi olusturduk ve bu frontend ve backend katmanlarini 3'er pod olacak seklinde k8s uzerinde deploy ettik, senaryomuz bu. simdi bu senaryonun erisim kismini inceleyelim. Ilk olarak bu 3 frontend poduna benim bir sekilde dis dunyadan baglanilmasini saglamam gerekiyor. Cozmemiz gereken ilk sorun bu. Benim bu podlara dis dunyadan bir sekilde erisim saglamam lazim. Bu sorunu  bir sekilde cozuk diyelim. Kullanicilar bu podlardan birine baglandi ve resmi yukledi. Benim bu forntedn uygulamamin resmi islemesi icin backend podlardan birisi ile iletisim kurmasi ve resmi ona gondermesi gerekiyor. Bu kisim sanki daha kolay gibi. Sonucta k8s uzerinden her podun essiz bir ip adresi var ve her pod birbiri ile sikintisiz haberlesebiliyor. Fakat simdi soyle bir sikinti var. 3 frontend pod u deploy edecek seklide bir deployment objesi olusturdum ve o da 3 pod olusturdum. ayni sekilde 3 backend podu olusturacak deployment objesi de olusturdum. Benim frontend podlarin backend podlarim ile network uzerinden iletisim kurmalari gerekiyor. Frontend e bir resim yuklendigi zaman bu frontend herhangi bir backend pod ile iletisim kurarak ona resmi yollayacak o da isleyip geri yollayacak. Bunun olmasi icin frontendlerin backend podlara nasil ulasabilecegini bilmeleri gerekir. Kisacasi onlarin ip adreslerini bilmeleri lazim. Ben bu sorunu cozmek adina oncelikle tum backend podlarinin ip adreslerini listeledim sonra frontend podlarima tek tek baglandim ve iletisim kumalari gereken ip adreslerini yazilima ekledim ve bu sayade frontendler backendlere erisebildi sorunu simdilik cozdum. Ama simdi soyle bir sikinti var. Bildiginiz gibi k8s cesitli durumlarda bu podlari bu sistemlerden kaldirip yeniden olusturabilir. Ornegin node lardan biri bir sikinti cikardi ve node devre disi kaldi. Backend deployment objesi bu durumu tespit etti ve devre disi kalan node lar uzerinde kalan podlari saglikli node lar uzerinde yeniden olusturdu. Yeniden olusan podlar da yeni ip adresleri aldi. Simdi benim tekrardan frontendlere baglanarak yazilim icerisinde bu ip adreslerini guncellemem gerekecek ve isin kotu tarafi su ki bu durum k8s uzerinde surekli olabilir. Surekli pod silinerek yeniden yaratilabilir. Ya da diyelim ben bu backend deploymenti scale ettim. Artik 4 pod calisiyor bunu da eklemem gerekebilir. Goruldugu gibi benim bu senaryoda cok islem yapmam gerekiyor. Neyse ki bu mekanizmayi cozebilecek bir mekanizma k8s de mevcut.
k8s servisler bu ve benzeri sikintilari ortadan kaldirmaya yarayan objelerdir. 4 tip servis objesi yaratabiliriz. 
* Cluster IP: Bizler bir cluster ip tipi servis objesi yaratip, bunu label selector ler araciligiyla podlarla iliskilendirebiliriz. Bu objeyi yarattigimiz zaman bu obje cluster icerisindeki tum podlarin ve kaynaklarin cozebilecegi unique bir dns hizmetine sahip olur. Bunun yaninda bizler her k8s hizmeti kurulumunda servis cluster range ip anahtari ile bir sanal ip blogu belirleriz. Ornegin 10.10.0.0/16. Siz bir cluster ip servis objesi yarattiginiz zaman bu objeye bu ip blogundan bir sanal ip adresi atanir ve ismi ile bu ip adresi cluster in dns mekanizmasinda kaydedilir. Bu ip adresi virtual bir ip adresidir herhangi bir interface e map edilmez. Kube-proxy servisi tarafindan tum nodelardaki ip table lari eklenir ve buraya ulasan trafik servisin iliskilendirdigi podlara randomic bir siralama ile yonlendirilir. Yani cluster icerisinden bu ip adresine gonderilen paketler bu servisle iliskilendirilmis podlardan bir tanesine yonlendirilir. Bu bizim iki sikintimizi cozer. Az onbceki ornekteki gibi ben bu backend podlarla iliski kuracak bir backend isimli servis yaratir ve frontendlere de backend podlara ulasmak istediginde gotmek gereken adres backend isimli adrestir dersem su olur. Cluster icerisindeki herhangi bir pod beckedn ismini cozmek istediginde ona cevap olarak bu backend servisinin ip adresi doner. Pod o ip adresine ulasmak istediginde de trafik backend podlarindan bir tanesine yonlendirilir dolayisiyla ben tek tek frontendlere baglanarak her seferinde yeni eklenen podlarin ip adresi bu, eski podlarin ip adresleri bu diye guncelleme yapmam gerekmez. Ona sadece gitmen gereken yer backend derim o trafigi bu ip adresie yollar. gerisini cluster halleder. Yeni eklenen podlar otomatik olarak bu servise eklenir. Sistemden cikarilan podlar da otomatik olarak bu servisten cikarilir. yani cluster ip servisleri k8s altinda basit de olsa service discovery ve load balancing hizmetleri kuran obje tipidir. Simdi cluster ip sayesinde forntend backend baglanti sorununu cozduk.
olusturula servise atanan bir virtual ip adresidir ve yalnizca cluster icinde gecerlidir. Cluster icinde herhangi bir kaynak bu ip adresinin 5000 portuna istek gonderdigi anda bu istek bu servisin selector ile sectigi podlardan birinin ip adresine ayni port uzerinden yonlendirilecek. ayni cluster icindeki podlar birbirleriyle isimleri uzerinden haberlesebilirler ancak namespace farklilastigi zaman acik ismini belirtmek gerekir. podun acik ismi de su sekilde yazilmaktadir: isim.namespace_ismi.svc.cluster_domain. bir poddan digerine istek yapildiginda her seferinde olmasa da bu yanit farki podlardan gelir cunku load balancing yapilmaktadir. Burada round robin algoritmasi kullanilacak istekler farkli podlara dagitilmaktadir.
* Nodeport: Ilk sorunumuz neydi. podlarimiz web sitemizi barindiriyor dolayisiyla bunlarin cluster disindan erisilebiliyor olmasin lazim. Bu sorun da nodeport tipi servis objeleri ile cozeriz. Aynen cluster ip gibi node port da bir servis obje tipidir. Siz bir nodeport servisi yaratip bunu label selectorler araciligiyla podlarla eslestirebilirsiniz. Bu obje yaratildigi zaman su olur 30000-32767 araliginda bir port secilir ve tum worker node larda bu port sizin servisinizde proxylenir. Kisacasi worker node un ip adresininin bu portuna gelen tum paketler iliskilendirdiginiz podlarin public ettiginiz podlarina yonlendirilir. Sizler de bu worker nodelarin onune bir worker load balancer ya da reverse proxy sunucu koyar ve dis dunyadan erisilmesini saglarsaniz buraya ulasacak paketler worker node lara worker node lara ulasan paketler de podlarin birine ulasarak kullanicilarinizin uygulamaniza erismesini saglarsiniz. 
Olusturdugum nodeport service cluster IP den cok farkli degil. Cluster icindeki haberlesmeye imkna taniyor ancak bunun yanindan ek bir sey daha yapiyor tum worker node larin dis bacaginda 30447 portunu dinlemeye basliyor. Biz herhangi bir worker node un bu portuna ulasir isek trafik frontend portlarindan birine yonlenecek. 
* Lod balancer: Bu tip objeleri sadece azure k8s servis gibi ya da google k8s engine gibi cluster saglayicilari manage k8s servislerinde kullanabiliriz. Bu obje olusturuldugunda cloud servis saglayici dis dunyadan erisilebilecek public bir ip adresine sahip bir load blancer olusturur. sonrasinda bu load balancer i sizin service objenizle iliskilendirir. Dolayisiyla u public adresine ulasan paketler iceriden podlara yonlendirilir. Nodeport icin dedim ya worker node larin onune bir load balancer ya da reverse proxy koyar oradan kullanicilara eristirirsiniz. Iste cloud servis saglayicilar bunu otomatik olarak yapiyorlar. Buna da load balancr tipi servis objesi diyoruz. 
service olustururkenki yaml dosyalarinda spec kisminin altindaki selector arkasina alaacgi yani yakalayacgi podlarin labelleri yazilmali. Ports tanimlamarsinda ise port bu servisin dinleyecgi port target port ise podlarin expose ettigi port olacak.
Bu servisi localde kurmak bir ise yaramayacaktir. Bu servis gcp, eks, aks gibi cloud saglayicilarin k8s ortamlarinda ise yarayip sonuc veren bir uygulamadir.

Deploymen konusunda gectigi uzere deployment lar replicasetleri replicasetler de podlari olusturur. Benzer bir durum service icin de gecerli biz bir service olusturdugumuz zaman bu service endpoint adinda bir obje olusturur ve endpoint objesi bizim belirledigimiz selector tarafindan secilen podlarin ip adresini uzerinde barindirir service trafigini nereye yonlendirecegini bu listeye gore duzenler. yaratilan service arka planda sectigi podlarin ip lerini adresler yani decribe komutu ile bu ip adreslerini liste halinde gorebiliriz. Bu listei dimanik olarak tutmaktadir. Bir pod terminate oldugunda veya yeni bir pod yaratildiginda bu liste otomatik olarak guncelleniyor. Ekleniyor ya da siliniyor. 

## livenessprobe
Kubelet, bir containerin ne zaman yeniden baslatilacagi bilmek icin liveness probe kullanir. ornegin, liveless probe, bir uygulamanin calistigi ancak ilerleme saglayamadigi bir kilitlenmeyi yakalayabilir. Boyle bir durumda bir container'i yeniden baslatmak, hatalara ragmen uygulamayi daha kullanilabilir hale getirmeye yardimci olabilir.
Livenessprobe ile container icinde bir komut calistirarak ya da bir http endpoint e request gondererek ya da bir porta tcp connection acarak uygulamamizin dogru calisip calismadigini kontrol etmeye calisiyoruz. Bu sorgulamanin sonucunda bekledigimiz sonucu alirsak container in saglikli oldigunu eger bekledigimiz sonucu alamazsak container da sikinti oldugunu tespit edebiliyoruz. 
Bazi durumlarda pod icinde uygulama calisiyor olarak gorunse de islevini yerine getirmiyor olabilir. Ornegin bir web sayfasi yayinlayan bir podumuz var. httpd temelli bir container calisiyor. Bu baslayinca container icindeki httpd deamon ayaga kalkiyor ve bizim web sitemizi sunmaya basliyor. Fakat bir zaman sonra bu servis takilmaya basliyor. Bir hatadan dolayi sayfalari sunmaya devam edemiyor. Ama httpd uygulamasi yani service ayakta service cokmedi ya da kapanmadi ama esas yapmasi gerekn is olan web sitesini yayinlamak sunma isini yapamiyor. Bu durumda ortamda soyle bir sorun olusuyor. Eger uygulama calismiyor olsa ya da uygulama kapansa kubelet bunu tespit ederek containeri yeniden olusturarak sorunu cozuyor. Fakay uygulama ayakta olmasina ragmen yapmasi gerekn isi yapmiyorsa kubelet bunu tespit edemiyor dolayisiyla container i yeniden baslatarak sorunu cozme mekanizmasi calismiyor. Bunun cozumu ise liveness probes dur. 
Livenessprobe da bir endpointe httpGet ile istek gonderiyoruz oradan 200 donerse basarili 400 donerse basarisiz olarak gorunuyor. Eger sizin uygulamanizin boyle bir healthcheck endpointi varsa onu denersiniz yoksa direk localhosta gidebilirsiniz. Ya da saglik kontrolunu yapmanin en saglikli oldugu endpoint neresi ise onu denersiniz. Bunu uygulamaya ozel kurgulamaniz gerekir. 
Livenessprobe yaml dosyasi icerisindeki initialDelaySeconds kismi pod olustuktan sonra uygulamay hemen ayaga kalkmayabilir on hazirlik yapmasi biryerlerden dosya cekmesi gerekebilir. Eger livenessprobe hemen baslatilirsa uygulama hazir olmadigindan hata alacak ve uygulama yeniden baslatilacaktir. Bunu onmemek adina container basladiktan sonra bekledikten sonra healtcheck e baslamasi gerektigini soyleriz. periodSeconds degeri de bu isin kac saniyede bir yapilacagini gosteriyor. Yani container baslatildi delaysconds 3 saniye, 3 saniye bekledi, httpget ile liveness yapmaya basladi birinci sorgusunu yapti cevap olumlu 3 sn bekleyecek ve sonra bir sorgu daha iste bu sorgular arasinda kacar saniye bekleyecegi bu periodSeconds degeridir. 
Eger bizim uygulamamiz http tabanli bir uygulama degilse bir consol uygulamasi ise bunun icin bir script yazar ve bu script ile uygulamamiza uygun sorgu ne ise misal bir dosyanin olup olmadigina bakabiliriz. Iste liveness checkout alinda httpget disinda ikinci kullanabildigimiz prob olan exec bize bu imkani verir. exec ile shellde komut ya da uygulama calistiririz. Bu uygulama bize 0 kodu donerse saglikli bunun disinda bir kod donerse sagliksiz yani sikintili oldugunu anlar ve container i restart eder. 
tcpSocker: probu ise bazen http ya da exec probu birseylerin saglikli olup olmadigina yardimci olmayabilir. Mysql veri tabani container i calistiriyoruz. Mysql process i 3306 portundan erisilebilir. Iste bizler tcp socket ile bu porta istek gonderip eger cevap alirsak uygulamnin duzgun calistigini varsayabiliriz. Bu tarz http get ve exec ile sonuc alamayacagimiz durumlar icin tcp portuna sorgulama yontemini izlemek istersek tcp socket portunu kullanabiliriz.
Uygulamniza yonelik uygun bir prob tanimi yapmalisiniz.

## Readiness
Kubelet, bir containerin ne zaman trafgi kabul etmeye hazir oldgunu bilmek icin readiness probelari kullanir. Bir pod tum containerlar hazir oldugunda hazir kabul edilir. Readiness probe sayesinde bir pod hazir olana kadar service arkasina eklenmez.
3 frontend podumu olusturacak bir deploymentimiz bunblari da dis dunyadan erisilmesini saglayacak load balancer tip bir service imiz oldugunu varsayalim. Bunlari yaml dosyalari araciligiyla olusturduk. Bu deploymet da bir guncelleme yapmak istersek diyelim ki web sitemizin yeni bir image ini olusturduk ve deploymentimizi guncelledikve rolling update stratejisi secildigi icin de podlar birer birer devreden cikartilip yeni podlar devreye alindi. Ben bu deployment da yeni bir image atadigim zaman k8s hemen bu guncel image ile yeni bir pod olusturacak ve bunu load balancer servisine dahil edecek Yani olusturup baslatildigi anda dis dunyaya bunun uzerine trafi akmaya baslayacak. Ya bu benim uygulamam ilk baslatildiginda bir yerlere baglanip bazi dosyalar cekiyor ve sonra web sitesini servis etmeye basliyor ise ya da baska bir nedenden dolayi container baslatilir baslatilmaz bu servis hizmet veremiyorsa arada belirli bir sure gecip bazi islemler yapmasi gerekyorsa eger durum buysa ben bu podu olusturdugum anda bu podlar load balancer servisinden trafik almaya basladigi icin sikinti olacak. Yani bu pod ayakta icindeki uygulama calisiyor. Ama daha henuz web sitemi sunmaya hazir degil. Dolayisiyla bu pod henuz load balancer servisine eklenmek icin hazir degil. Peki ben bunun load balancer hizmetinden trafik almaya hazir oldugunu nasil anlayacagim? Readiness ile.
Readiness probe lar aynen liveness probe lar gibi olusturulur. http, exec ya da tcp socket olarak kurgulanabilir. Siz bri readiness check tanimlarsiniz eger bu readiness check olumlu sonuc donuyorsa pod servise eklenir ve trafik alir. Eger bu readiness check fail ederse de bu pod servisten cikarilir.
Ornege donersek image i guncelledim ve pod olusturuldu pod calismaya basladi ama bu noktada hemen loadbalancer a eklenmedi. Pod icerisinde readiness check calismaya basladi, initial delay suresi kadar bekledi sonra tanimladigimiz prob ile kontrolunu yapti. Olumlu cevap aldigi anda bu pod servise eklenir ve trafik akmaya baslar. Readiness check period seconda belirlenen saniye kadar surede bir bu islemi yapmaya devam eder. Faile edene kadar bu pod servise eklenir. Eger fail ederse de pod servisin endpoint listesinden cikarilir. readiness chek budur. Pod un servis sunmaya hazir halde oldugunu belirtir. Pod olustururldu ama pod da readiness tanimi var. Dolayisiyla henuz servise eklenmedi. Initial delay second kadar bekledi ardindan ilk kontrolu yapti. Olumlu cevap aldi ve servise eklenerek trafik almaya basladi. Tam bu noktada eski pod terminate edilmeye baslanir. Bu pod terminate edildiginde dahi halen bu poda trafik gelebilir. Kullanicilar buna baglanmaya devam ederler ve tam islem yaparken pod kapanmaya baslamis olursa bu kullanicilara hata donmek zorunda kaliriz. Bunu engellemek adina bir pod terminate edilecegi zaman bu surede poda yeni trafik gelmemesi icin pod servis arkasindan alinir ve servis ile baglantisi kesilir boylece yeni trafik akmaz fakat k8s yine de bu podu kapatmaz cunku yeni trafik gelmese bile halen bu pod hala eski istekleri islemey devam ediyor ve bu islemlerin bir sure bitmesinin beklenmesi istenebilir.
Pod tanimlarinda terminationGracePeriodSeconds adinda bir anahtar bulunur ve varsayilan degeri 30sn dir. Siz ya da kubelet bir podu terminate etmek istediginiz zaman bu podun icinde calisan pid1 process e skill yani yaptigin ne varsa hemen birak ve kapan komutu gondermez bunun yerine sitterm yani su anda yaptigin islem varsa onlari duzgun sekilde tamamla ve bitirince de kendini kapat sinyali gosterir. Process bu sinyali aldiginda eger ortasinda oldugu bir is varsa onu yapmayi birakmaz. O islemleri tamamlar ve bitince tum baglantilari duzgun sekilde kapatir ve sonrasinda process de kapatilir. Iste terminattionGracePeriodSeconds degeri container a duzgun bir sekilde kendini kapatabilmesi icin atadigimiz suredir. Eger container bu sure icerisinde duzgun bir sekilde kapanirsa sikinti olmaz fakat kapanmazsa skill gonderilir ve zorla kapatilir. Dolayiisyla sizin uygulamanizin ne kadar sure ile duzgun bir sekiilde kapanabilecegini bilmeniz ve ondan daha uzun sure surecek bir terminationGracePeriodSeconds degeri atamaniz gerekmektedir. Ama 30 sn hemen hemen tum islemler icin yeterlidir. 
Yaml dosyalari icinde liveness tanimlari ile readiness tanimlamalari ayni seklide yapilmaktadir.

## Resource limits
Siz aksini belirtmediginiz surece containerlar uzerinde calistirdiginiz sistemin kaynaklarina sinirsiz bir sekilde erisirler. Eger bir pod tum kaynakari tuketiyorsa diger podlari etkileyebilir bu nedenle podlarin kullanacagi kaynaklari kisitlamak gerekir. pod tanimlarinda kullanabildigimiz request ve limit tanimlari bize bu imkani verir.
* cpu: cpu kisitlamalari su sekilde uygulanir her uygulamanin belirli cpu kaynaklari vardir. Bu kaynaklar cloud uzerinde ya da sanal olarak calisan sistemlerde vcp olarak bare metal sunucularda da hyperthread olarak hesaplanir. kisacasi core saysindan bahsediyorum. ben bir pod tanimina cpu: "1" tanimi eklersem. O podun calistigi node uzerindeki corelardan sadece bir tanesini kullanabilecegi anlamina gelir. Bunun bir baska yazim sekli daha vardir. her pod 1000 ms yani 1000 cpu luk br guc demektir. Ben eger cpu 100m tanimi yaparsam pod calistigi node uzerindeki bir core un 10 da 1 lik gucune erisebilir anlamina gelecektir. Yani cpu 1 ile cpu 1000m ayni keza 0.1 cpu ile 100m de ayni.
* memory: Clusterda mevcut durumda kullanilabilir bellek varsa, bir container belirlenen bellek istegini asabilir. Ancak bir container bellek sinirindan fazlasini kullanmasina izin verilmez. Bir container, sinirindan daha fazla bellek ayirirsa, container sonlandirma icin aday olur. Container, limitinin otesinde bellek tuketmeye devam ederse, container sonlandirilir Sonlandirilmis bir container yeniden baslatilabiliyorsa diger herhangi bir tuntime hatasi turunde oldugu gibi kubelet onu yeniden baslatir.
Container in kullanacagi max memory i byte cinsinden tanimlayabilirsiniz. Ornegin memor 64m derseniz bu container in en fazla 64mbit memor kullanmaniza izin verdiginiz anlamdina gelir. M megabayt, g gigabayt, k kilobaytin kisaltilmasi anlamina gelmektedir. Bunlarin yaninda 2 nin katlarini kullanan gosterim sekli olan  kibubayt KIB, mebibayt MIB, gibibayt GIB tanimlari da desteklenir. 
yaml dosyasi icinde resource kisitlamalarini 2 ayri bolumde tanimlayabiliyoruz. 1. request: k8s bu podu olusturmaya basladigi zaman scheduler bu podun olusturulacagi bir node sececek bu secme asamasinda tanimlanan degerlerin bulungu bir node uzerinde schedule et yani bu pod un olusturulabilmesi icin node da minimum ne kadar kaynagin olmasi gerektigini belirtiyor, scheduler bu parametleri dikkate alarak hangi node da scale edilecegini belirliyor eger buradi kaynaklarin saglandigi bir node yoksa bu pod olusturulamacak. 2. limits: bu container in en fazla kullanacagi sistem kaynagi. cpu da belirlenen limitde fazlasina erisemez ancak memory degerinde tam olarak oyle degil. memory de belirlenen sayi container in kullanabilecegi max rem degeri fakat container bu degere geldigi zaman daha fazlasini kullanamayacak diye bir durum soz konusu degil. memory allocation cpu gibi calismiyor. bu sistemden daha fazla ram talebinde bulunabiliyor ve sistem varsayilan olarak bunu engelleme sansina sahip degil. container daha fazla memory istegine cevap alamadiginda ise bu durumda OOMKilled durumuna gelip restart edilecek.

## envrionment variable
Ornegin bir web uygulamasi yaziyoruz ve bu web uygulamasi bir veri tabanina baglanaak uygulamalarini burada sakliyor. Bu uygulamanin baglandigi veri tabaninin sunucu adresini ve kullanici adi, sifre bilgilerini de bu uygulamanin icine gomduk ve bu uygulamayi container haline getirdik ve istedigimiz platformda calistiraya haziriz. Fakat bu senaryoda 2 sikintimiz var. Biz bu veritabani baglanti bilgilerini bu container icine hard-code olarak yazdik. yani bir sekilde container image i ele gecerse bi bilgiler expose olabilir. Diger bir sorun ise ben bu imajdan container olusturmaya calistigim zaman her defasinda ayni veri tabanina ayni kullanici adi ve sifre ile baglanmaya calisacak. Benim veri tabani adim test ortamimda ayri prod ortamimda ayri stage ortamimda ayri olabilir ben kullanici adi bilgilerini zaman icerisinde guncellemis olabilirim. Ya her ortam icin yeni bilgilerle image olusturacagim ya da container olusturduktan sonra bu bilgileri guncelleyecegim. her seferinde bunu manuel olarka yapmak mumkun degil. Bunu verine environment variable lar tanimlariz.
K8s de disaridan bir poda direk olarak eririm saglayamayiz ya load balancer ya da nodeport araciligiyla olur. fakat k8s soyle bir imkan sagliyor. kubetcl araciligiyla poda deployment a service e kendi bilgisayarimdan tunel acarak o objenin portuna trafigimi yonlendirebiliyorum. Iste testlerde hizli bir sekilde objelere baglanmamizi saglayan bu ozellige portforward diyoruz. 
kubectl port-forward pod/envpod 8080:80 # komutu ile local hostumu envpod unun 80 portuna yonlendirdim

# Volume
Stateless: uzerine yazilan bilginin silinmesi durumunda sorun olmayan uygulama. Mesela onune gelen istege islem yapip gonderen uzerinde veri barindirmayan bir backend uygulamasi. Bu tur uygulamalar container silinip yenisi olustugunda islemeye devam eder. 
Ornegin bir frontend ve bir web sitemiz oldugunu dusunelim. Kullanicilar bu uygulamaya baglaniyor ve bu uygulamada baska bir yerden resimlero cekerek kullanicilara gosteriyor. uygulama remi bir defa cektikten sonra bunu bir folder icinde sakliyor ve bir daha resmi cekmesine gerek kalmiyor yani bir nevi cachliyor. Bunu k8s de deply edecegiz. Bir deployment objesi olusturduk ve bu uygulama bir replica olacak sekilde deploy edildi. Podumuz olusturuldu ve calismaya basladi. Buraya erisebilecek bir service objesi de olusturduk. boylece bu uygulamaya kullanicilar baglanabildi. Hemen backend uygulamamizi ver service i de olusturduk. Kullanicilar web sitemizi kullanmaya basladilar ve diyelim ki cesitli resimleri sectiler. Bizim uygulamamiz da yapmasi gereken islemlere basladi. Diger servise baglandi bu resimleri cekti isledi ve bagli kullanicilara bunu servis etti ama ayni zamanda bu resimleri container icindeki cache isimli container a kaydetti. Bir baska kullanici daha ayni resmi istedigi zaman tekrar diger uygulamaya baglanmasine gerek kalmadan cache den cekiyor. Diyelim ki bu pod taniminda livenessprob da var bir sikinti cikti ve livenessprob fail etti. Bu durumda ne oluyordu kubelet devreye giriyordu ve containeri restart ediyordu ama aslinda containerlarda restart diye bir kavram yoktur. k8s bize bunu restart olarak gosterse de olay aslinda olan sey su: kubelet calisan containeri siler ve yeniden bir container olusturur. Ancak container silindigi zaman icindeki veriler de siliniyor. Dolayisiyla yeni container icinde cache foderi bos. Cok buykmbir sikinti degil sonucta bunlar cache dosyalari tekrar diger servise baglanilarak bunu halledebilirim ancak keske bu cache dosyalari yeni container olusturuldugunda silinmese. Keske ben podun icerisindeki tum containerlarin erisebilecegi ve podun yasam suresi boyunca ayakta kalabilecek bir mekanizmaya sahip olsam da dosyalari buraya yazsam. Boyle bir mekanizma olsa bu cache folderini buraya baglarim ve icerisine yazilan tum dosyalar buraya yazilir. Yeni bir container olustugunda da bu dosyalara erismeye devame ederim. Boylece cache den mahrum kalmam. Bu duruma cozum getiren volume ise ephemeral yani gecici volume. 
* Ephemeral volume: Verileri container disinda tutmamiza yarayan volume turleri. 2 tip ephemeral olume vardir

1. empyDir: emptyDir volume ilk olarak bir Pod bir node'a atandiginda olusturulur ve bu Pod o node'unda calistigi surece var olur. Adindan da anlasilacagi gibi emptyDir birimi baslangicta bostur.. Pod icindeki tum containerlar, emptyDir volumedeki ayni dosyalari okuyabilir ve yazabilir, ancak bu birim her kapsayicida ayni veya arkli yollara mount edilebilir. Bir pod herhangi bir nedenle bir node'dan silindiginde, emptyDir icindeki veriler de kalici olarak silinir. 
EmptyDir olusturulmasi ile ilgili gerekli tanimlamalari yaptiginiz zaman k8s pod uzerinde bos bir klasr yaratir daha sonra siz bu klasoru konteyner icerisindeki herhangi bir path e mount edebilirsiniz. Ornegin bizim senaryomuzdaki /cache klasorune. Bu containerin bu mount ettiginiz path e yazdiginiz her turlu dosya fiziksel olarak node uzerinde olusturulmus olan bu bos klasore yazilir. dolayisiyla container silinse bile bu dosyalar silinmez. yeni container olusturuldugu zaman tekra bu volume container da ayni path e mount edilir. Bu sayede yeni olusturulmus container da bu dosyalara erismeye devam eder. fakat podun yasam suresi sonuna kadar erisilebilir durumdadir. Yani pod silinirse bu volume de silinir. bu nedenle bu volume lere gecici volume ler denir. 
2.HostPath: Bir hostPath volum, worker node dosya sisteminden Pod'unuza bir dosya veya dizini baglayabilme imkani verir. Bu cogu podun ihtiyac duyacagi bir sey degildir, ancak bazi uygulamalar icin guclu bir kacis kapisi sunar. 
Temelde emptydir ile mantik olarak aynidir. Yine siz bir volume yaratilmasi icin bir pod taniminizda gerekli ayarlamalari yaparsiniz fakat bu sefer rastgele bos klasor yaratilmasini soylemek yerine podun olusturulacagi worker node uzerindeki spesifik bir klasoru veya dosyayi belirtirsiniz. Ornegin worker node uzerindeki /tmp dosyasinin volume olarak kullanmasini soyleyebilirsiniz. Daha sonra container icerisindeki bir path e mounth edebilirsiniz. 
Bu genelde podlarin calistigi worker node lar uzerinde bulunan spesifik folder ya da dosyalara erismesi gerektigi durumlarda isimize yarar. Misal ozel bir path de konuslanmis bur unique soketine container in baglanmasi gerekirse burayi hostpath ile container a mount edebiliriz.
Volume yaml dosyalarinda once volume leri yazariz daha sonra bunlarin path ini containerlara mount ederiz.

## secret
Secretlar, parolalar, OAuth token ve ssh anahtarlari gibi hassas bilgileri depolamaniza ve yonetmenize olanak tanir. Gizli bilgileri bir secret icinde saklamak onu bir pod tanimina veya bir container imajina koymaktan daha guvenli ve esnektir.
1. env variable lari yaml dosyalarinda tanimladigimizda bu doslarar erisebilen herkes bu bilgilere de kolaylikla erisebiliyor.
2. verilerin degismesi sifrenin guncellenmesi gerektigi zaman yaml dosyasi icerisinde degisikliklere gitmem gerekiyor. 
Bu hassas bilgileri bu tanimlamalardan ayirmak gerekiyor bunu da secretlar ile saglayabiliyoruz.
Verileri secret icerisinde saklar sonrasinda pod a ekleriz. secretlar da diger objeler gibi yaratabilriz.
secret ile pod ayni namespace icinde olmali.
8 degisik tipte secret yaratabiliriz. Opaque bunlardan varsayilan olan turdur. Ama nerdeyse hicbir zaman Opaque disinda bir scret tipini kullanmayiz. 
Bu secretlari pod a nasil ekleriz. 2 senecek var ya bunlari voluma araciligiyla ya da env variable araciligiyla ekleriz.
bu secretlar etcd de base64 olarak tutuyor. kendi yonettigimiz clusterlarda manuel olarak bu secretlari encrypt etmemiz gerekiyor. Cloud saglayicilar bunu bizim yerimize otomatik olarak yapiyor. 
siz bir secret olusturdugunuzda diger objelerde oldugu gubu diger kullanicilar da bu secretlara varsayilan olarak erisim hakkina sahip.

## configmap
Gizli olmayan verileri anahtar/deger eslenikleri olarak depolamak icin kullanilan bir API nesnesidir. Podlar, ConfigMap'i environment variable, komut satiri argumanlari veya bir volume olarak baglanan yapilandirma dosyalari olarak kullanabilir.
secretlar ile ayni gorevi gorurler. olusturulan configmap dosyaarindaki key value ler ile verileri tutup bunlari env var olarak ya da volme olarak podlara aktarabiliriz. Olusturma adimlari da nerede ise birebir aynidir. sadece secret yazilan yerlere configmap yazilir. Peki birebir ayni ise neden iki farkli obje tipi vardir. secret hassas verileri saklamak icin kullanilan objelerdir. olusturulan secretler base64 encode edilerek etcd de saklanir ve ayar yaparsak etcd uzerinde encryptsiz sekilde durabilir. config mapde ise verilen base64 edilmez ve encrypt etmemize de gerek yoktur. Cunku bizler config map icerisinde gizli olmayan fakat yine de pod tanimimizdan ayirmamiz gereken configurasyon tarzi bilgileri tutariz. yani uygulama disinda tutmaniz gereken veri sifre ssh anahtari gibi gizli bir veri ise secret degilse configmap uygun olacaktir. bu ikisinin etcd de encoded olarak tutulmasi disinda birebir aynidir. ilerde secretlarin guncellenmesinin otomatize edilmesi ya da ozel secret store larda ayri tutulmasi gibi ek ozellikler getirilebilir. Bu nedenle en bastan iki farkli obje tipi olusturmus k8s.
Yaml dosyasi secret gibi tek farki data kismian girmek istedigim bilgileri key value seklinde girmek ya da site.settings altinda oldugu gibi birden fazla degeri alt alta girebiliyorum. poda aktarma kismi secret ile ayni. 

## Node affinity(yakinlik benzesme)
Node affinity kavramsal olarak nodeSelector'a benzer ve nodelara atanan etiketlere gore podunuzun hangi node ustunde schedule edilmeye uygun oldugunu kisitlamaniza olanak tanir.
Podlarimizin uygun worker nodelarda olusturulmasini saglayan k8s objesidir. Node selectora cok benzer bizler pod umuzu ekler ve podumuzun schedule edilecegi node uzerinde ekledigimiz label in olmasini bekleriz. Bunun yaninda bunun olmasini sart kosar ya da tercih ettigimizi belirtiriz.
yaml dosyasinda affinity tanimi affinity altinda yapiliyor burada iki secenek var bunlardan ilki `requiredDuringSchedulingIgnoredDuringExecution` digeri ise `preferredDuringSchedulingIgnoredDuringExecution` birinci podda olan required tanimi du demek: Bu podu olustururken mutlaka altta yapmis oldugum eslesmeye uygun bir node bul ve bu podu orada olustur. Uygun node bulamazsan olusturma. bu yaml dosyasini k8s api ye gonderirsem nodelarda label i blue olan bir node arayacak buna uygun bir node bulursa podumu orada schedule edecek. Tanim required oldugu icin eger buna uygun bir workernode bulamazsa da schedule etmeyecek pod pending olacak. Buradaki bir kac secenek nodeselectorden ayiriyor. yaml dosyasinda operator kisminda in yerine not in dese idim bu sefer pod app=blue tanimlanmamis bir node uzerinde olusturulacakti. Buna da antiaffinity diyoruz. ya da operator olarak exist degerini girsem bu podu calistirmak icin uzerinde app anahtari olusturulmus herhangi bir worker node bul demek olacakti. yani degeri onemli degil, app olsun da blue da olur. DoesNotExist ise uzerinde app anahtari olmayan bir worker node bul ve orada calistir. 
Mesela podlarimin ssd li worker nodelarda calismasini isteyebilirim. Nodelara label olarak ssd atamasi yaparim ve required alanin da ssd olarak belirlerim ancak burada da sorun su ki bu worker nodelarin kapasitesi doldugu zaman ne yapacaksiniz? O zaman gene de olusturulsun derseniz prefer dir. 
Prefer secenegi su anlama gelir bak burada tanim yapiyorum buna bak bu podu bu tanima uygun bir node uzerinde olustur. Fakat bu tanima uygun bir node bulamazsan da pending de bekleme baska bir node bul. Yine orada olustur prefer taniminda gordugunuz uzere weight de girebiliyorum. 1 ile 100 arasinda bir deger girilebiliyor hangisinin oncelikli olarak degerlendirilebicegi belirleniyor. ornekte 1 ve 2 var sadece. Oncelikli olarak weighti yuksek olan yani 2 olan secenek uygulanacak yoksa 1 inci secenek uygulanacak o da yerine getirilemiyorsa uygun olan herhangi bir worker node da olusturulur. Ornegin pd calismaya basladikten sonra ben buradaki label i worker node dan sildim. Ignoredduringexecution ise label silinse de bu pod orada calismaya devam etsin anlamina gelmektedir. Bu secenegin baska bir alternatifi yok. 

## pod affinity
Pod affinity podunuzun hangi node uzerinde olusturulmaya uygun oldugunu nodelardaki etiketlere gore degil halihazirda nodeda calismakta olan podlardaki etiketlere gore sinirlamaniza olanak tanir.
Podun uzerinde calistiracagimizi workder node da calisan baska podlarin durumuna gore podun nerede orada calisip calismamasini isteyecegimiz durumlarda kullaniriz. 
Ornegin bir azure uzerinde kosan 4 farkli worker node uzerinde 2 farkli az uzerinde kosan bir cluserimiz var. ornegin frontend cache ve veri tabanindan olsaun bir uygulamamiz var. Ilk olarak veri tabanimizi deploy edecek pod tanimimizi hazirladik ve k8s api uzerine gonderdik. k8s scheduler isini yapti ve bu podun calisacagi uygun bir node buldu ve varsayalim ki bu node node1 sonra frontend i deploy ettik onu da node3 de schedule etti. son olarak da cache podumuzu schedule ettik onu da node4 de schedule etti. Buradaki sorun ise frontend ve backend podum ayri az uzerinde schedule edildiler. Benim frontend sunumum veri tabani ile surekli gorusuyor ve hemen ehemn tum cloud providerlarda 2 az arasindaki veri transferi ucrete tabi. yani ben bu iki podu da ayni az uzerinde bulunan worker nodelarda kostursa idim veri transferine apara odemeyecektim. Bunun icin nodeaffinity yazarak podlari ayni yere toplayabilirim. ancak bu sefer her seyi manuel ayarlamam gerekir. eger ben front end tanimina podu olustururken db poduna bak ve o pod hangi az de olustu ise onu da orda olustur der ve sorunu cozerdim. 
Diger bir ornek de cache ile frontend podlari keske ayni makinada olsa cunku bu iki pod arasinda ne kadaar az latecy olursa performans artar. eger ben cache pod tanimima frontend poduna bak eger o hangi node da olusturudu ise o node da olsutur diye yazsa idim sorun cozulurdu. 
nasil node affinity de bir podun o node da olusturulup olusturulmamasi label lara gore secebiliyorsak bunu da o node uzerinde calisan pod olup olmadigina gore secebiliyoruz. 
Nodelar uzerinde onceden tanimlanmis pek cok label bulunmaktadir. Her node uzewrinde kubernetes.io/arch, kubernetes.io/hostname, kubernetes.io/os isimli labeller bulunmaktadir. kubernetes.io/arch anahtarrinin degeri amd64 olabilir isletim sisteminin degerini gosterir. kubernetes.io/hostname nodeun hostname degerini alir minikube gibi. kubernetes.io/os ise linux degerini alabilir. Bunun yaninda cloud providerlarda topology.kubernetes.io/region=northeurope, topology.kubernetes.io/zone=northeurope-1 nodelarin hangi regionlada bve az lerde bulundugu bu labellarla belirtilir. Podun tanimlandigi yaml dosyasina bakildiginda. node affinity tanimlarindaki gibi hard yani required soft yani prefered olmak uzere iki tanimlama yapilmaktadir. Tek fark burada topologykey diye bir deger girilir. topologykey kisminda kubernetes.io/hostname secersem bu podlari ayni hostname e sahip nodelar uzerinde calistir demek. zone secseydim ayni worker node uzerinde olmasina gerek olmayacakti. o zaman ayni az deki herhangi bir node uzerinde calismasi gerektigi belirtilir. prefer ise nodeaffinity ile ayni yani olsa iyi olur ancak bu sart durum degil. Pod affinity nin bir diger farki ise antiaffinity taniminin ayri olarak yapilmasidir. node antiaffinity yi not in ile yapabliyorken pod antianffinity yi ozel olarak tanimlamamaiz gerekiyor. bu da affinity nin tam tersi. 
Bu tanimlamalar buyuk clusterlarda sikca kullanilan bi ozelliktir. Kucuk cluster icin cok da gerekmemektedir.

## taint ve toleration
taint(leke) anlamina gelir. Siz bir node a anahtar veri eslenigi seklinde bir tanit ve bu tanitle birlikte noschedule, prefernoschedule ya da noexecute olmak uzere bir emir eklersiniz. bu asamadan sonra bu node uzerinde sadece bu taint i yani bozuklugu tolere edebilecek nodelar calisabilir. 

Worker1 blue olarak label landi. worker2 ise green olarak label landi. worker3 ise label lanmadi. Blue uygulamamizin blue labellanan node da green uygulamamaizin green labellanan node da schedule edilmesini istiyoruz. Buna gore gerekli pod effinity ve node affinity tanimlamalarini yaptik ve uygulamalari deploy ettik. baska bir ekip yeni bir pod schedule etti ve 3 pod tum nodelarda schedule edilmeya baslandi. 
mesela sizde 10 tane node unzu var bunlardan 3 tanesi cok uyi makinalar ve bu 3 makina musteriye sunulan nodelar olsun istiyorsunuz kalan 7 node ise test ortami olarak kullanacaginiz uygulamalr olsun isiyorsunuz. yani sadece belirli tipte podlar belirli nodelar uzerinde schedule edilebilsin. 
Ornegin ben platform=production:NoSchedule isimli bi taint eklersem bu etiket eklenmemis hicbir pod bu taint uzerindeki nodelarda schedule edilemez. Ayni sekilde pod a bu labeli eklersem ve prefer noschedule yazarsam bu etiketi tolere edemeyen hicbir pod bu worker node uzerinde schedule edilemz. Ama nun icin uygun baska bir node bulunamazsa da son secenek olarak bu worker node uzerinde calistirilabilir.
Noexecute secenegi ise bu tainti tolere edemeyen hicbir pod bu node uzerinde schedule edilemeyecegi gibi mevcut durumda bu worker node uzerinde bu tainti tolere edemeyen podlar da varsa o podlar da o worker node uzerinden silinerek baska uygun nodelar uzerinde yeniden olusturulur. 
Taint eklemk icin kubectl taint node minikube platform=production:NoSchedule
taint silmek icin kubectl taint node minikube platform
Taml dosyasinda podn birebir tolere edebilmesi icin node da girilen tolerations kisminin aynisi pod daki yaml dosyasinda da olmali.
taint ve toleration node affinity nin yapmis oldugunu yapmaz. yani bu ozelliklere gore git surada schedule ol demez. yani bir pod surada olusturulsun node affinity, worker node umun uzerinde sadece su podlar calisabilsin taint and toleraion.
Calisan podlar varken taint ayarlarini degistridigim zaman calisan podlar terminate olacaktir.

## DaemonSet
DaeminSet, tum (veya bazi) nodelarin bir podun bir kopyasini calistirmasini saglar. Clustera yeni node eklendikce, onlara podlar da eklenir. Clusterdan node kaldirildiginda, bu Podlar da kalidirilir. Bir DaemonSet;in silinmesi, olusturdugu Podlari da temizleyecektir.
Deployment objelerine oldukca benzeyen bir k8s objesidir. Bir Daemonset objesi olusturdugunuz zaman Daemonet sistemde template altinda belirttiginiz pod tanimina gore bir pod olusturur. Varsayilan olarak her node uzerinde bir pod olusturulur. Fakat siz bunu degistirerek sadece belirli nodelar uzerinde de olusturulmasini saglayabilirsiniz. Storage provisioinng ve log uygulamalari gibi uygulamalarin her node de kolayca deploye edilmesini saglar. Bu uygulamalar icin DaemonSet kullanmak su avantaji saglar: 1. is basitlesir 2. olarak yeni node geldiginde uygulamayi orada da olusturur. Yeniden bir islem yapmaniza erek kalmaz. 
Ornegin 20 worker nodedan olusan bir K8s clusterimiz var. Bizim bu worker nodelarin tamaminda calismasi gerekn bir uygulamamiz mevcut. Ornegin biz bu worker nodelarda olusan loglari merkezi bir log sunucusuna gondermek istiyoruz. her bir worker ndde da bir uygulama calistiracagiz ve bu uygulama o worker node da olusan loglari toplayacak ve merkezi log sunucusuna gonderecek. Ilk olarak tum worker nodelara baglanarak bu uygulamayi kurabilirim ama bu imkansiz. Ikinci olarak bu uygulamayi container image i haline getiririm ardindan gerekli ayarlarin oldugu 20 pod tanimi yaparim. Her birine node affinity eklerim ve bu podlari ayri ayri worker nodelar uzerinde calisacak sekilde deploy ederim. Bu iki yontem de zahmetli bu durumda yeni node eklersem bu islemleri yeniden yapmak zorunda kalirim.
Deployment la benzer yaml dosyalari vardir ancak rollout ozelliklerini kullanmyoruz temel fark bu. 2 seye dikkat etmek gerekir. daemonset de de label selector tanimi vardir ve daemonset olusturacagi bu podlari bu selector a gore secer. Bu nedenle ayni label taniminin spec de de olmasi lazim. Minikube kurduumuz icin bunu gormedik ama normalde kubeadm ile kendi kurdugumuz clusterlarda ya da cloud saglayicilarin  sagladigi clusterlarda master node uzerinde pod calistirmayiz. Master node larda `node-role.kubernets.io/master:NoSchedule` adinda bir taint eklidir. Dolayisiyla bunu tolere edecek bir tanim ekli degilse pod bu bu master node uzerinde schedule edilmez. Eger biz worker node un sadece daemon setlerde pod olusturmasini istiyorsak o zaman bir sey eklememize gerek yok. Ama bizim DaemonSetimizin master nodelarda da pod olusturmasini istiyorsak o zaman bu tolerationu buraya eklemek zorndayiz. 

## Persistent Volume and Persistent Volume Claim
Pod yasam suresinden daha fazla saklamamiz gereken verileri sakladigimiz ve cluster disinda tuttugumuz volumelere persistent volume diyoruz.

3 nodelu bir cluster uzerinde mysql db var. Tek deployment olacak bri replicaset tasarladik. deployment in icerisinde mysql veri tabani dosyalarinin duracagi emptyDir volume olusturmasi icin tanim ekledik. Buna container a mount edecek tanimlari da olusturduktans sorna deployment objemizi de olusturduk. Mysql containcerdan olusan podumuz uygun bir node uzerinde olusturuldu. Bu noktada container da sikinti olusturulursa container yeniden baslatilacak ve volume ekledigimiz icin verilerimize birsey olmayacak. Ancak diyelim ki podun calistigi worker node a birsey oldu ve terminate oldu. Podumuz uygun olan baska bir worker node uzerinde yeniden yaratilacak. Bu durum ephemeral uygulama icin sikinti yaratmazken myswl uygulamasi icin sikinti olusturabilir. Cunku mysql yeniden olusturabilecegimiz degil uzun sura barindirmak zorunda oldugumuz kayitlari tutuyor. Poda ne olursa olsun bu kayitlari kaybetmemiz gerekiyor. Podu olusturduk mysql calisti ve verileri emptyDir adli volume e yazmaya basladik. Bu volume fiziksel olarak o konteyner in calistigi worker node uzerinde duruyor artik o worker node a erisilemiyor. dolayisiyla pod baska bir worker node uzerinde yeniden olusturuldugunda bu dosyalara erisilemeycek. Bunun cozumu clusterin disinda olan ancak tum worker nodelar tarafindan erisilebilen bir folder da olusturuyor olabilmemiz lazim. Bunu yaparsak ver devamliligi saglayabiliriz. 
Bunun icin ilk once bir kac ayar yapmamiz gerekiyor
1. Ilk olarak volume olarak ayarladigimiz birim ile clusterimizin konusabilmesi gerekiyor. Bunun icin de cluster uzerinde bu depolama biriminin driverlarinin yuklenmesi gerekiyor. Kubernetes nfs ve Isqz gbi universal protokollere ait driverlarin yaninda azure disk azure file Aws ebs, google persistent disk gibi pek cok storagelarin driverlarini bunyesinde barindirmaktadir. Yani K8s default olarka bunlarla konusabiliyor durumda ancak depolama cozumleri yalnizca bunlarla sinirli degil. K8s bunlarinda disindaki storagelarla konusabilecek cozumleri `Container Storage Iterface (CSI)` ile sagliyor.
CSI istege bagli blok ve dosya depolama sistemlerini K8s gibi Container Orchestration Systems (CO'ler) uzerindeki containerized is yuklerine maruz birakmak icin bir standart olarak gelistirilmistir, Container Stoarge Interface'in benimsenmesiyle K8s volume katmani gercekten genisletilebilir hale geldi. Ucuncu taraf depolama saglayicilari, CSI kullanarak cekirdek K8s koduna dokunmak zorunda kalmadan K8s te yeni depolama sistemlerini aciga cikaran eklentiler yazabilme imkanina kavustu. 
CSI k8s storage altyapsinin nasil ayarlanmasi gerektigii belirten bir starndart. depolama cozumu uretenler bu standarta uyan driverlar yazarak K8s in kendi altyapilari ile de konusabilmelerine imkan saglamaktadir. Ozetle baglanabilecegimiz depolama birimleri nfs, azure, aws, gcp gibi degilse bu sefer kullanilacak storage in csi driver ini sisteme yuklemem gerekiyor.
Ilk adim cluster ile depolama aracini birbirine bagliyoruz ve konusabilecek hale getiriyoruz. Sonra depolama birimleri uzerinde k8s de kullanmak uzere depolama birimleri yaratiyoruz. Sonra bu depolama birimlerinin k8s deki karsiliklarini k8s altinda persistent volume olarak bir obje seklinde olustururuz. 
K8S clusterimiz altinda nfs tabanli bir depolama birimi oldugunu varsayalim. k8s altinda nfs driverlari bulundugu icin ek bir driver yuklememize gerek kalmadan bu iki ortam birbiriyle gorusebilir durumda. Sirada bu storage uzerinde bizim erisebilecegimiz bir depolama birimi yaratma isi var. nfs cihazina bagalaniyor ve tmp diye bir paylasim alani yaratiyorz. Sonrasinda bunun k8s deki karsiligini yaratma isi var bunun karsiligi k8s de persistent volume dur. Burada olusturulacak yaml dosyasindaki spec kismi baglanilacak driver a gore degisiyor. Driver a gore degismeyen 3 secenek var. 1. Capacity: bizim ne kadarlik bir volume yaratmak istedigimzi belirtiyor 5Gi yani 5 gibibayt gibi. accesModes: kisminda bu volume un ayni anda birden fazla volume e baglandigi zaman ne sekilde bir davranis sergileyecegini secbiliyrouz. burada 3 senecek mevcut readWriteOce: okuma yazma -teknode, readOnlyMany: sadece okuma -birden fazla node, readWriteMany: okuma yazma - birden fazla node. Bu durum driver in yeteneklerine ve baglanilacak storage a degisebiliyor. persistentVolumeReclaimPolicy: secenegi ise podumuzun isi bitip birakildiktan sonra bu volume e nasil davranilacagini bildiriyoruz. Burada  retain: volume kullanildiktan sonra oldugu gibi kaliyor. Bizler bu dosyalari manuel olarak tasiyoruz ve kurtarma imkanina sahip oluyoruz. recycle seceneginde icindeki bilgiler siliniyor ancak voluem siliniyor. volume tekrar kullanilabiliyor. delete ise volume ile is bittiginde tamamen siliyor. 
Volume olustugunda pod a nasil bagliyoruz. direk olarka bir persisten volume bir pod ile bagalayamiyoruz. oncelikle persistent volume claim kisaca pvc bizlerin sistemde bulunan pv lerden isimize yaran bir tanesini secmemize yani bunu kullanmak adina talep yaratma objesidir. Neden bu talep etme farkli objede tanimlaniyor. Mesela ben developer olabilirim ve yonetme isi farkli bir yonetici tarafindan yapiliyor olabilir. dolayisiyla o islemleri ben bilemiyoor olabilir. 
Peki bu volumeleri pod nasil baglariz? yaml dosyasinda volume tanimlarinin altinda yeni bir volume olustur ve bunun hedefini persistentVolumeClaim olarak belirler sonrasinda bunu pod altinda gerekli path e mount ederiz. Bu pod olusturuldugu anda bu path devreye girer. Bu podun olusturulacagi worker node uzerinde bu volume ile ilgili ayarlamalar yapilir. sonucunda bu pod bu path ilgili volume baglanir ve bu path e yazilan dosyalar bu volume e yazilir.
pv yaratirken label onemli cunku pvc ler bu labellara gore istek yapilmaktadir. 
Pv lerin yaratilmasina kadar olan kisim system yoneticilerin daha sonra bu pvc nin kullanilmasi kismi ise developerlara ait olan bir kisim. 
Volume u deployment in icinde tanimliyorum:
    volumes:
    - name: mysqlvolume
      persistentVolumeClaim:
        claimName: mysqlclaim

## Storage Class
Storage class yoneticilerin sunduklari depolama "siniflarini" tanimlamalari icin bir yol saglar. Farkli siniflar, hizmet kalitesi duzeylerine veya yedekleme ilkelerine veya cluster yonetticileri tarafindan belirlenen istege bagli ilkelere eslenebilir. k8s siniflarin neyi temsil ettigi konusunda fikir sahibi degildir. Bu kavram bazen diger depolama sistemlerinde "profiller" olarak adlandirilir. 
10 ayri ekip yonetilen k8s cluser oldugunu dusunun 10larca farkli node ve 100lerce pod olusturuluyor. Uygulamalar surekli yaratiliyor siliniyor. Bu durumda her seferinde pvc ve pv olusturmamiz cok zahmetli olacaktir. Bu soruna cozum olarak storage class objesi olusturuldu. 
pv yi manule olursturmak yerine claime gore dinamik olusturma imkani getirdi. Bu ozellikle cloud servis saglayicilarin dinamik ortamlarinda her seyi otomatize edebilirken sirf volume olusturma islemini manuel olusturma sacmaligini cozdu. Storage class objelerini baglantimiz olan depolama uretimi uzerinde ne cesit volume yaratacagimizi belirten templateler olarak dusunebilrisiniz. ornegin uzerinde hem ssd hem de hdd olan storage oldugunu dusunun. Bu durumda ben yavas diskler uzerinde otomatik volume yaratan yavas isimli bir storage class ve hizli diskler uzerinde volume yaratan hizli isimli 2. bir storage class yaratabilirim. Development ekipleri de uygulamalarinda kullanmak icin volume talep etmek adina olusturduklari pvc de uygulamalarinin ihtiyacina gore bu storage class lardan birini secerler ve istedikleri boyutta bir volume storage class objesi tarafindan otomatik olarak olusturulup pvc ye bind edilir.
Sadece retain ve delete secenegi mevcut
Immediate yazyorsa hemen volume un olusturulup pod a atanmasini soyler.
WaitForCounsumer ise pod yaratilana kadar beklemesi gerektigini soyler.
Cloud ortaminda bu secenekleri bilmek gerekimiyor otamaik sagloiyor ama bu detaylari kendi clusterinizi yaratiyorsaniz olabilir. Ama size dick saglayan forma bunlari size belirtir. 
pv.yaml dosyalarindan tek farki burada label kullanmiyorum onun yerien kullanmak istedigim storafeClassName yerine statndarddisk yani diskin ozelligini yaziyorum. 

## StatefulSet
1.StatefulSet tarafindan olusturulan her pod, statefl set taniminda belirlediginiz pvc'ye gore olusturulan bir pvye sahip olur. yani her podun kendine ait bir pv si olur.
2. StatefulSet altinda podlar sirayla olusturulup sirayla silinir. Ornegin 3 podlu bir stateful set olusturdugunuz zaman oncelikle pod olusturulur. Bu pod ayaga kalkip readiness ve liveness checklerinden gecip islemlerini tamamlamadan bir sonraki pod olusturulamaz. Ne zaman ki bu pod hazir running durumuna gecer o zaman bir sonraki pod olusturulur. Ayni sekilde 3.pod da 2. pod tamamlanmadan olusturulamaz. Tam tersi durumda da bu gecerlidir. Siz 3 podlu bir statefullseti 2. poda indirirseniz k8s random bir pod secip onu silme yoluna gitmez. En son yaratilan pod hangisi ise ilk olarak o silinir.
3. StefulSet tarafindan olusturulan her poda statefulsetin adi 0-1-2-3 seklinde devam eden sabit bir isim verilir. Isimler random secilmez ve bu isimler ayni zamanda containerin hostname'i olarak da atandigi icin her uygulama bu isimle ulasilabilir.

3 pod olusturacak bir deployment var. Bu obje bir replicaset lusturuyordu ve bu replicaset de bizim belirledigimiz tanima gore de random isimler verilen podlar olusturuyordu. scale out ile scale in ile bunlarla oynayabiliyorum ama k8s bunlardan hangisi once yaratilip yaratilmadigina bakmiyor siliyor. podlar uzerinde bir state tutmadigi icin bu durum sikinti yaratmiyor. Ancak yeni bir sql veri tabani deploy etmek istiyorsunuz. Bir tane master instance olusturur ve bunun uzerinden cluster olsutururardindan bu cluster a yeni instancelar eklersiniz. Yazma islemlerini bu master uzerinden yaparken sorgulari herhangi bir isntance a gonderebilirsiniz. Tum veri bu instancelar uzerinde dagitik bir sekilde durur. Boyle bir veri tabani olan apche cassandrayi 3 instance dan olusacak vir cluster olacak sekilde su ana kadar ogrendigimiz obje sekilleri ile k8s e deploy etmeye calisalim. Ilk olarak bu cassandra master instance i deploy edecegiz ardindan buna baglanarak ya da baslangic komutlari ile bunu master haline donusturecek sekilde bi cluster haline getirecegiz. Sonrasinda 2. instance i olusturacagiz ve ardaindan 2. instance a baglanip 1. instance daki cassandra cluster a bu 2. instance i dahil edecek ayarlari yapacagiz. sonra 3. instance da da ayni islemleri yapacagiz. Ilk olarak singleton pod olacak sekilde olusturmaya karar verdik. 
Master olacak pod tanimini yaptik. icine pvc tanimini ekledik ki cassandra verileri tutabilecek pv ye sahip olabilsin. Pod ayaga kalkti bunu master haline getirecek komutlari ya baslangic olarak ya da sonradan baglanarak halletik. Ardindan 2. bir pod tanimi daha yaptik. Bunu da 2. instance olarak ayaga kaldirdik. Sonra 3. derken cluster ayaga kalkti. SOrun 1. her seyi manuel yaptim. sonra singleton pod olusturduk bu da fail durumu kontrol eden bir mekanizma yok. Son oalrak 4. bir instance eklersem yeni pod ve maneul ekleme olacak. Bu sorunu deployment la dener isem bu hepsi birbirinin ayni pod u olusturdu ve hepsi ayni anda olustu ve bu nedenle hangisinin once olsutugunu bilmiyorum neyse birine manuel baglandim ve birini master a cevirdim digerlerini de worker yaparak cluster a kattim. aradan zaman gecti podlardan birine ulasilamiyor sonra yeniden olustu ama o zaman bu master pod olursa o zaamn cluster coker. Bunlarin hepsi cassandra gibi statefull objelerde sorun yaratiyor. Bunun cozumu ise StatefulSet objesidir.
Deploymenta cok benzeyen bir obje temelde 3 farki var 1) statefull tarafindan olustutulan her pod statefulset taniminda belirlediginiz pvc ye gore olusturulan bir pv ye sahip olur. yani her pod un kendine ai bir pv u bulunur. Bunun yaninda stateful altinda podlar sirayla olusturulurp sirayla silinir. Podlar silinirken de bu durum aynisidir. Random olarak yapmaz ve son yaatilan pod ilk olarak silinir. Her poda statefulun adi-0 ve sirayla olaack sekilde sabit bir isim verilir. isimler ayni zamanda container hostname olarak atandigi icin her uygulama bunlara ulasabilir. Bu 3 ozellik sayesinde stateful objesi ile yaratmamiz kolaylasir. 
Senaryomu statefulset objesi iel yapmak istedigimde senaryo soyle ilerleyecektir:
3 pod olutiracak bir stateful deployment objesi deploy ettim. K8s hemen ilk podu ayaga kaldirdi. Ben bu poda bir baslangic scripti eklemistim. Bu script ortamda benim belirledigim isimde bir cluster var mi yok mu bunu konrtole ediyor. yoksa da bu isimde cluster yaratiyor. Liveness ve readiness prob tamamlandi ve hem cluster ve pod hazir. 1. pod saglikli bir sekilde calismaya basladiginda 2. pod olusturulmaya baslandi bunda da script var ancak simdi ortamda cluster mevcut. Doalyisiyla mevcut clustera dahil oldu liveness ve readiness problar tamamlandi ve bu pod da hazir. 3. pod sonrasinda cluster tamamlandi. Peki cassandra-1 podu silinirse stateful objesi ayni podu ayni isimle ayni persistent volume u atayarak ayaga kaldiracak ve sorun olmayacak. Yeni pod eklersem ne olacak yeni olusturacak ve cluster a dahil olacak. scale down oldugu zaman da en son yaratilan pod silinecek.
yaml dosyasinda volumeClaimTemplates kisminda her pod icin pvc olusturulmasini soyluyorum. Bu pvc lerde standart isimli storage class i kullanarak her bir pod icin birer pv olusturacak. Yani her podun ayni pv ye baglanmasi yerine her poda ayri bir pv yaratilacak. Bunu da yaratacak pvc leri de tempate ile yaratiyouz. Bu template i buraya yaziyoruz ki pvc ler olusturulsun o pvc lerde pv leri olsuturacak sonrasinda o podlara tek tek baglayacak. servis tanimlanmasinda ise clusterIp none diyoruz. Buna headless servis diyoruz. Bunu olusturdugumuz anda clusterIp tipi bir servis olusturulacak fakat ona bir ip atanmayacak bu bana su sekilde yardimci oluyor. Ben ne zaman bu servis ismine gotmek istersem o bana bu servis altindaki podlardan bir tanesinin ip sini donecek bunun yaninda her bir pod a da pod ismi.servis ismi seklinde erisme imkani da verecek.
Podlar 0 dan baslayarak isimlendirilir. Ready olduktan sonra diger pod olusturulur.

## job
Bir job objesi, bir veya daha fazla pod olusturur ve belirli sayida pod basariyla sonlanririlana kadar pod yurutmeyi yeniden deneeye devam eder. Job belirtilen sayida podun basariyla tamamlanma durumunu izler. Belirtilen sayida basarili tamamlamaya ulasildiginda, job (yani gorev) tamamlanir. Bir Job un silinmesi olsuturdugu podlari temizlemeyecktir. 
Hangi durumlarda kullanilir? 1. Tek seferlik calisip yapmasi gerekeni yapip kapanan uygulamalari job olarak deploy edebiliriz. ornegin maintannce scriptler. 2. Bir kuyruk veya bir bucket da islenmesi gereken pek cok isimiz oldugunda bunlari eritme adina bunlar eriyene kadar calisacak uygulamarli job seklinde deploy ederiz.
apisi batch/v1
spec kismidna 4 secenek var. paralelelism: pod olusturulma islemini kacar kacar yapacagini. completions: job altinda kac tane basarili pod calismasini istiyoruz. template kisminda container bilgilerini yaziyoruz. Bu ikisinin hemen altinda backofflimit: podlar olusturulmaya baslandikten sonra kac sefer fail ederse job u fail et demek. ActiveDeadlineSeconds ise job u fail etmeyi secons cinsinden belirtiyoruz.
Job yarattiktan sonra isi biten podlari manuel olarak silmek gerekiyor ki yer kaplamasin artik.